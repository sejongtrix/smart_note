start,end,text,end_sec,page
00:00.000,00:09.980,꼼꼼한 딥러닝 논문 리뷰와 코드 실습. 이번 시간에 리뷰할 논문은 현대 딥러닝 기반의 자연어처리 기술의 핵심 아키텍처가 되고 있는 트랜스포머입니다.,9.98,0
00:10.300,00:24.420,트랜스포머 논문의 원래 제목은 Attention is all you need 입니다. 논문의 제목에서 알 수 있듯이 트랜스포머라는 아키텍처에는 이 Attention이라고 하는 것이 가장 메인 아이디어로서 사용이 된다는 걸 알 수 있습니다.,24.42,0
00:24.420,00:36.580,실제로 트랜스포머는 Attention이라는 메커니즘을 전적으로 활용하는 아키텍처입니다. 트랜스포머가 나오게 된 계기를 이해하기 위해서 딥러닝 기반의 기계 번역 발전 과정에 대해 확인해 보겠습니다.,36.58,1
00:36.960,00:44.020,2021년 기준으로 최신 자연어처리 쪽 고성능 모델들은 이런 트랜스포머 아키텍처를 기반으로 하고 있습니다.,44.02,1
00:44.280,00:52.340,최근까지 화제가 되었던 GPT와 BERT는 모두 이러한 트랜스포머의 아키텍처를 적절히 활용하여 좋은 성능을 내고 있습니다.,52.34,1
00:52.340,01:01.020,대표적으로 GPT는 트랜스포머의 디코더 아키텍처를 활용했고 BERT는 트랜스포머의 인코더 아키텍처를 활용했다는 점이 특징입니다.,61.02,1
01:01.220,01:06.540,자연어처리 태스크 중에서 가장 대표적이면서 중요한 태스크 중 하나는 기계 번역입니다.,66.54,1
01:06.880,01:16.840,실제로 기계 번역 기술의 발전 과정을 확인해 보시면 1986년도 즈음에 RNN이 제한되었고 그로부터 약 10년 정도가 지난 뒤에 LSTM이 등장하였습니다.,76.84,1
01:17.260,01:21.240,이러한 LSTM을 활용하면 다양한 시퀀스 정보를 모델링 할 수 있는데요.,81.24,1
01:21.240,01:25.020,"대표적으로 주가 예측, 주기함수 예측 등이 가능합니다.",85.02,1
01:25.240,01:31.540,이러한 LSTM을 활용해서 2014년도에는 딥러닝 기반 기술로 시퀀스 투 시퀀스가 등장하였습니다.,91.54,1
01:31.940,01:41.880,시퀀스 투 시퀀스는 현대의 딥러닝 기술들이 다시 빠르게 나오기 시작한 시점인 2014년도에 이러한 LSTM을 활용해서 고정된 크기의 컨텍스트,101.88,1
01:41.880,01:49.980,벡터를 사용하는 방식으로 번역을 수행하는 방법을 제안하였습니다. 다만 이러한 시퀀스 투 시퀀스 모델이 나왔을 때의 시점만 하더라도,109.98,1
01:49.980,02:00.900,고정된 크기의 컨텍스트 벡터를 쓰고 있기 때문에 소스 문장을 전부 고정된 크기의 한 벡터에다가 압축을 할 필요가 있다는 점에서 성능적인 한계가 존재했습니다.,120.9,1
02:01.120,02:09.700,이후에 어텐션 메커니즘이 제한된 논문이 나오면서 이러한 시퀀스 투 시퀀스 모델에 어텐션 기법을 적용하여 성능을 더 끌어올릴 수가 있었고요.,129.7,1
02:09.820,02:19.960,"이제 그 이후에 트랜스포머 논문에서는 그냥 어뢰넨 자체를 사용할 필요가 없다는 아이디어로 오직 어텐션 기법에 의존하는 아키텍처를 설계했더니 성능이 훨씬 더 좋게 되었고,",139.96,1
02:19.960,02:21.320,성능이 훨씬 좋아지는 것을 보여주었습니다.,141.32,1
02:21.540,02:32.640,즉 이 트랜스포머를 기점으로 해서 더 이상 다양한 자연어 처리 태스크에 대해서 어뢰넨 기반의 아키텍처를 사용하지 않고 어텐션 메커니즘을 더욱 더 많이 사용하게 되었습니다.,152.64,1
02:32.860,02:42.120,그래서 어텐션 메커니즘이 등장한 이후로부터는 입력 시퀀스 전체에서 정보를 추출하는 방향으로 연구 방향이 발전되어 왔다고 할 수 있습니다.,162.12,1
02:42.280,02:47.500,"물론 이후에 나온 논문들 중에서도 어뢰넨을 활용하는 아키텍처도 많이 존재하지만,",167.5,1
02:47.500,02:56.420,전반적인 추세 자체는 어텐션 기법을 더욱 더 활용하는 이런 트랜스포머의 아키텍처를 따르는 방식으로 다양한 고성능 모델들이 제한되고 있습니다.,176.42,1
02:56.640,03:01.900,그렇다면 기존에 제한되었던 시퀀스 투 시퀀스 모델에는 어떤 한계점이 존재할까요?,181.9,2
03:02.000,03:08.940,"기존 시퀀스 투 시퀀스 모델의 한계점이라고 한다면, 이 컨텍스트 벡터 V에 소스 문장의 정보를 압축한다는 점입니다.",188.94,2
03:09.220,03:13.880,이때 병목 현상이 발생할 수 있기 때문에 성능 하락의 원인이 될 수 있는데요.,193.88,2
03:14.160,03:17.480,"현재 예시를 확인해 보시면 대표적인 시퀀스 투 시퀀스 모델은,",197.48,2
03:17.480,03:19.620,이 시퀀스 투 시퀀스 모델을 활용한 기계 번역 예시라고 할 수 있습니다.,199.62,2
03:19.760,03:25.440,"왼쪽에 있는 독일어 문장, 즉 각각의 단어들로 구성된 하나의 시퀀스가 들어왔을 때,",205.44,2
03:25.500,03:30.200,"이렇게 중간에서 하나의 고정된 크기의 컨텍스트 벡터로 바꾼 뒤에,",210.2,2
03:30.320,03:35.100,다시 이러한 컨텍스트 벡터로부터 출력 문장을 만들어내는 것을 확인할 수 있습니다.,215.1,2
03:35.300,03:42.540,"즉, 한쪽의 시퀀스에서부터 다른 한쪽의 시퀀스를 만든다는 의미에서 시퀀스 투 시퀀스 모델이라고 부를 수 있습니다.",222.54,2
03:42.780,03:46.460,결과적으로 이렇게 영어 출력 문장이 나오는 걸 확인할 수 있고요.,226.46,2
03:46.460,03:50.440,"다만 이때, 이러한 시퀀스 투 시퀀스 아키텍처를 확인해 보시면,",230.44,2
03:50.700,03:55.940,매번 단어가 입력될 때마다 히든 스테이트 값을 갱신하는 걸 확인할 수 있습니다.,235.94,2
03:56.140,03:58.160,"이런 식으로 단어가 입력될 때마다,",238.16,2
03:58.220,04:03.600,"이전까지 입력되었던 단어들에 대한 정보를 포함하고 있는 히든 스테이트 값을 받아서,",243.6,2
04:03.740,04:06.840,매번 이런 식으로 히든 스테이트 값을 새롭게 갱신합니다.,246.84,2
04:07.080,04:13.600,"즉, 이런 식으로 각각의 단어가 차례대로 순서에 맞게 입력될 때마다 히든 스테이트 값이 갱신되어,",253.6,2
04:13.620,04:16.440,"이러한 히든 스테이트 값은 이전까지 입력되었고,",256.44,2
04:16.460,04:19.160,"입력되었던 단어들에 대한 정보를 갖고 있기 때문에,",259.16,2
04:19.260,04:21.440,"이렇게 마지막 단어가 들어왔을 때,",261.44,2
04:21.540,04:28.820,그때의 히든 스테이트 값은 소스 문장 전체를 대표하는 하나의 컨텍스트 벡터로써 사용할 수가 있다는 것입니다.,268.82,2
04:28.960,04:34.600,"그렇기 때문에, 이렇게 마지막 단어가 들어왔을 때의 히든 스테이트 값을 하나의 컨텍스트 벡터로써,",274.6,2
04:34.740,04:41.880,이 컨텍스트 벡터 안에는 앞에 등장했던 소스 문장에 대한 문맥적인 정보를 담고 있다고 가정하는 것입니다.,281.88,2
04:42.100,04:45.220,"그렇기 때문에, 이러한 컨텍스트 벡터로부터 출발해서,",285.22,2
04:45.220,04:50.000,"이렇게 출력을 수행하는 디코더 파트에서는 매번 출력 단어가 들어올 때마다,",290.0,2
04:50.100,04:55.960,이러한 컨텍스트 벡터로부터 출발해서 마찬가지로 히든 스테이트를 만들어서 매번 출력을 내보냅니다.,295.96,2
04:56.080,05:00.800,"이렇게 그 다음 단계에서는 이전에 출력했던 단어가 다시 입력으로 들어와서,",300.8,2
05:00.860,05:06.900,"반복적으로 이전까지 출력했던 단어에 대한 정보를 가지고 있는 히든 스테이트와 같이 입력을 받아,",306.9,2
05:07.040,05:09.780,새롭게 히든 스테이트를 갱신하는 걸 확인할 수 있습니다.,309.78,2
05:09.980,05:14.300,"이런 식으로 디코더 파트에서는 매번 히든 스테이트 값을 갱신하면서,",314.3,2
05:14.540,05:15.020,"이렇게,",315.02,2
05:15.020,05:19.880,히든 스테이트 값으로부터 출력 값이 end of sequence가 나올 때까지 반복합니다.,319.88,2
05:20.080,05:24.080,"그래서 end of sequence가 나왔을 때 출력 문장 생성을 마치게 되고요,",324.08,2
05:24.200,05:28.060,이렇게 출력된 정보인 good evening이 나오는 걸 확인할 수 있습니다.,328.06,2
05:28.280,05:32.600,이게 가장 기본적인 형태의 시퀀스 to 시퀀스 모델의 동작 원리입니다.,332.6,2
05:32.840,05:34.100,"다만 확인해 보시면 이렇게,",334.1,2
05:34.300,05:38.420,"소스 문장을 대표하는 하나의 컨텍스트 벡터를 만들어야 한다는 점에서,",338.42,2
05:38.560,05:42.540,"이렇게 고정된 크기의 컨텍스트 벡터에 정보를 압축하려고 하면,",342.54,2
05:42.780,05:44.760,"이러한 입력 문장은 어떨 때는,",344.76,2
05:45.020,05:45.520,"짧기도 하고,",345.52,2
05:45.600,05:47.340,"어떨 때는 길기도 하기 때문에,",347.34,2
05:47.500,05:49.400,"그러한 다양한 경우의 수에 대해서,",349.4,2
05:49.680,05:53.320,"항상 소스 문장의 정보를 고정된 크기로 가지고 있는 것은,",353.32,2
05:53.440,05:56.720,전체 성능에서 병목 현상의 원인이 될 수 있습니다.,356.72,2
05:56.920,06:00.400,"그래서 이러한 문제를 조금이나마 완화하기 위한 아이디어로,",360.4,3
06:00.480,06:06.400,"이 고정된 크기의 컨텍스트 벡터를 매번 이 디코더의 RNN 셀에서 참고하도록 만들어서,",366.4,3
06:06.480,06:08.240,조금 더 성능을 개선할 수 있습니다.,368.24,3
06:08.460,06:09.360,"이렇게 하게 되면,",369.36,3
06:09.520,06:14.320,"이 컨텍스트 벡터에 대한 정보가 이 디코더 파트의 RNN 셀을 거침에 따라서,",374.32,3
06:15.020,06:17.800,"정보가 손실되는 정도를 더 줄일 수 있기 때문에,",377.8,3
06:18.420,06:20.260,"출력되는 문장이 길어진다고 하더라도,",380.26,3
06:20.820,06:25.780,"각각의 출력되는 단어에 이러한 컨텍스트 벡터에 대한 정보를 다시 한번 넣어줄 수 있어서,",385.78,3
06:25.960,06:28.740,성능이 기존보다 조금 더 향상될 순 있습니다.,388.74,3
06:29.040,06:31.040,"다만 이런 식으로 접근한다고 하더라도,",391.04,3
06:31.120,06:36.340,"여전히 이 소스 문장을 하나의 벡터에 압축해야 된다는 점은 동일하기 때문에,",396.34,3
06:36.460,06:38.240,병목 현상은 여전히 발생합니다.,398.24,3
06:38.440,06:40.040,"즉 현재의 문제 상황이라고 한다면,",400.04,4
06:40.220,06:42.680,"하나의 문맥 벡터, 즉 컨텍스트 벡터가,",402.68,4
06:42.760,06:45.000,"소스 문장의 모든 정보를 가지고 있어서,",405.0,4
06:45.000,06:47.240,성능이 저하될 수 있다는 것입니다.,407.24,4
06:47.420,06:49.540,"그렇다면 디코더 파트에서는,",409.54,4
06:49.700,06:52.840,"하나의 문맥 벡터에 대한 정보만 가지고 있는 게 아니라,",412.84,4
06:53.020,06:54.560,"출력 단어를 만들 때마다,",414.56,4
06:54.720,06:59.440,매번 소스 문장에서의 출력 값들 전부를 입력으로 받으면 어떨까요?,419.44,4
06:59.540,07:01.280,라는 아이디어가 나올 수 있는 거죠.,421.28,4
07:01.440,07:05.540,"최신 GPU는 많은 메모리와 그리고 빠른 병렬 처리를 지원하기 때문에,",425.54,4
07:05.760,07:08.520,"소스 문장의 시퀀스 길이가 길다고 하더라도,",428.52,4
07:08.660,07:12.840,"그러한 소스 문장을 구성하는 각각의 단어에 대한 출력 값들 전부를,",432.84,4
07:12.920,07:14.780,"특정 행렬에다가 기록해 놓았다가,",434.78,4
07:14.780,07:19.660,"소스 문장에 대한 전반적인 내용들을 매번 출력할 때마다 반영할 수 있기 때문에,",439.66,4
07:19.720,07:22.160,성능이 좋아질 것을 기대할 수 있습니다.,442.16,4
07:22.380,07:26.720,"다시 말해, 하나의 고정된 크기의 컨텍스트 벡터에 담지 말고,",446.72,4
07:26.880,07:31.380,"그냥 소스 문장에서 나왔던 출력 값들 전부를 매번 입력으로 받아서,",451.38,4
07:31.540,07:34.960,"일련의 처리 과정을 거쳐서 출력 단어를 만들도록 하면,",454.96,4
07:35.080,07:36.960,성능이 더 좋아질 수 있다는 겁니다.,456.96,4
07:37.200,07:38.960,"지금 보이는 아키텍처가 바로,",458.96,5
07:39.140,07:43.020,시퀀스 시퀀스에 어텐션 메커니즘을 적용한 아키텍처인데요.,463.02,5
07:43.260,07:44.760,"이렇게 어텐션 메커니즘을,",464.76,5
07:44.780,07:49.000,적용해서 인코더 파트의 모든 출력을 참고하도록 만들 수가 있습니다.,469.0,5
07:49.220,07:51.720,"실제로 파이톨치와 같은 프레임워크에서는,",471.72,5
07:51.860,07:55.280,"단순히 RNN이나 LSTM 같은 걸 사용하도록 만들면,",475.28,5
07:55.360,07:59.700,"이렇게 매번 전체 시퀀스 기기에 맞는 아웃풋 값들이 따로,",479.7,5
07:59.700,08:01.080,출력 값들이 나오게 되는데요.,481.08,5
08:01.280,08:05.780,이제 그걸 그대로 이용해서 실제로 어텐션 메커니즘을 간단하게 구현할 수도 있습니다.,485.78,5
08:06.020,08:07.460,"전반적인 내용을 확인해 보시면,",487.46,5
08:07.680,08:11.160,"이렇게 매번 단어가 출력돼서 히든스테이트가 나올 때마다,",491.16,5
08:11.200,08:13.740,"그냥 이 값들을 전부 다 출력 값으로써,",493.74,5
08:13.740,08:16.520,그냥 별도의 배열에다가 다 기록해 놓습니다.,496.52,5
08:16.720,08:19.300,"그래서 이런 식으로 각각의 단어를 거치면서,",499.3,5
08:19.340,08:22.640,갱신되는 히든스테이트 값들을 매번 다 가지고 있는 거예요.,502.64,5
08:22.740,08:23.780,"이렇게 해 줌으로써,",503.78,5
08:23.800,08:27.420,"이렇게 매 단어가 들어왔을 때의 히든스테이트 값을,",507.42,5
08:27.460,08:29.580,"전부 가지고 있을 수 있기 때문에,",509.58,5
08:29.760,08:32.380,"이러한 값들을 어떻게든 참고해서,",512.38,5
08:32.540,08:35.000,"이렇게 출력 단어가 매번 생성될 때마다,",515.0,5
08:35.160,08:39.480,이러한 소스 문장 전체를 반영하겠다라는 아이디어라고 보시면 되겠습니다.,519.48,5
08:39.800,08:42.680,"실제로는 이렇게 디코더 파트에서 매번,",522.68,5
08:42.680,08:43.720,"히든스테이트를 넣어서,",523.72,5
08:43.720,08:44.440,"히든스테이트 값을 갱신하게 되는데,",524.44,5
08:44.680,08:48.180,"이때 현재 단계에서 히든스테이트 값을 만든다고 하면,",528.18,5
08:48.380,08:50.740,"바로 이전의 히든스테이트 값을 이용해서,",530.74,5
08:50.840,08:52.740,"이 출력 단의 히든스테이트 값과,",532.74,5
08:52.800,08:56.720,"이렇게 소스 문장 단의 히든스테이트 값을 서로 묶어서,",536.72,5
08:56.860,08:58.460,"별도의 행렬 곡을 수행해서,",538.46,5
08:58.580,09:00.600,각각 에너지 값을 만들어냅니다.,540.6,5
09:00.780,09:02.600,"이제 이때 그 에너지 값은,",542.6,5
09:02.680,09:05.100,"내가 현재 어떠한 단어를 출력하기 위해서,",545.1,5
09:05.340,09:09.680,"소스 문장에서 어떤 단어에 초점을 둘 필요가 있는지를,",549.68,5
09:09.700,09:11.440,수치화해서 표현한 값입니다.,551.44,5
09:11.720,09:12.180,"이제 그래서,",552.18,5
09:12.180,09:15.040,"그러한 에너지 값에 소프트맥스를 취해서,",555.04,5
09:15.100,09:16.280,"확률 값을 구한 뒤에,",556.28,5
09:16.420,09:19.920,"그렇게 소스 문장에 각각의 히든스테이트 값에 대해서,",559.92,5
09:20.120,09:23.040,"어떤 벡터에 더 많은 가중치를 두어서,",563.04,5
09:23.160,09:24.980,"참고하면 좋을지를 반영해서,",564.98,5
09:25.440,09:28.080,"그렇게 가중치 값을 다 히든스테이트에 곱한 것을,",568.08,5
09:28.240,09:30.780,"각각의 비율에 맞게 더해준 다음에,",570.78,5
09:31.020,09:33.080,"이제 그러한 weighted sum 값을,",573.08,5
09:33.220,09:35.260,"매번 출력 단어를 만들기 위해서,",575.26,5
09:35.400,09:37.560,반영을 하겠다라고 보시면 됩니다.,577.56,5
09:37.820,09:41.360,"그래서 단순히 이렇게 컨텍스트 벡터만 참고하는 것이 아니라,",581.36,5
09:41.580,09:42.160,"여기에 더해진,",582.16,5
09:42.180,09:42.620,"더불어서,",582.62,5
09:42.680,09:44.700,"소스 문장에서 출력이 되었던,",584.7,5
09:44.820,09:47.640,"모든 히든스테이트 값들을 전부 반영해서,",587.64,5
09:47.780,09:49.820,"이러한 소스 문장의 단어들 중에서,",589.82,5
09:49.980,09:52.600,"어떤 단어에 더욱 더 주의 집중해서,",592.6,5
09:52.720,09:54.540,"출력 결과 를 만들 수 있는가를,",594.54,5
09:54.660,09:56.680,"우리 모델이 고려하도록 만들어서,",596.68,5
09:56.820,09:58.660,성능을 더욱 높일 수가 있다는 겁니다.,598.66,5
09:59.040,09:59.800,"즉 쉽게 말하면,",599.8,5
09:59.940,10:01.320,"매번 출력할 때마다,",601.32,5
10:01.520,10:04.280,"이 소스 문장에서 나왔던 모든 출력 값들을,",604.28,5
10:04.400,10:06.080,전부 참고하겠다 이겁니다.,606.08,5
10:06.180,10:09.900,"즉 이렇게 압축된 컨텍스트 벡터 하나만 보는 것이 아니라,",609.9,5
10:10.220,10:12.160,"이러한 출력 값들을 전부 고려한,",612.16,5
10:12.180,10:14.520,"하나의 weighted sum vector를 구한 다음에,",614.52,5
10:14.620,10:16.780,"개를 이렇게 같이 입력으로 넣어줘서,",616.78,5
10:16.840,10:20.080,"소스 문장에 대한 정보를 모두 고려할 수 있도록 만들기 때문에,",620.08,5
10:20.140,10:21.600,성능이 좋아질 수 있는 것입니다.,621.6,5
10:22.300,10:23.220,"자 그래서 실제로,",623.22,6
10:23.480,10:25.400,"이 attention 기법을 적용했을 때,",625.4,6
10:25.560,10:28.800,"디코더 파트에서 각각의 출력 단어를 만드는 과정을,",628.8,6
10:28.900,10:30.820,"수식적으로 표현하면 다음과 같이,",630.82,6
10:30.940,10:32.020,정리할 수가 있는데요.,632.02,6
10:32.380,10:35.580,이때 i는 현재 디코더가 처리 중인 인덱스가 되겠습니다.,635.58,6
10:35.960,10:39.080,"즉 디코더가 매번 한 번에 하나의 단어를 만드는데,",639.08,6
10:39.220,10:41.980,그 각각의 처리 중인 인덱스가 i가 되겠고요.,641.98,6
10:42.180,10:43.580,"이때 이 j 같은 경우는,",643.58,6
10:43.580,10:46.060,인코더 파트에서 출력 인덱스가 되겠습니다.,646.06,6
10:46.380,10:48.220,"즉 에너지 값이라고 하는 것은,",648.22,6
10:48.320,10:51.400,"매번 디코더가 출력 단어를 만들 때마다,",651.4,6
10:51.480,10:53.120,모든 j를 고려하는 겁니다.,653.12,6
10:53.320,10:56.480,즉 인코더의 모든 출력들을 고려하겠다 라고 보시면 돼요.,656.48,6
10:56.660,10:57.820,"이제 여기에서 s는,",657.82,6
10:57.920,11:01.520,"디코더가 이전에 출력했던 단어를 만들기 위해 사용했던,",661.52,6
11:01.660,11:02.820,히든 스테이트가 되겠고요.,662.82,6
11:02.980,11:05.980,여기 h는 인코더 파트의 각각의 히든 스테이트입니다.,665.98,6
11:06.220,11:08.140,"즉 이걸 간단하게 정리하자면,",668.14,6
11:08.280,11:11.880,"디코더 파트에서 내가 이전에 출력했던 정보는 이거인데,",671.88,6
11:11.880,11:16.200,"이 정보와 인코더의 모든 출력 값과 비교를 해서,",676.2,6
11:16.300,11:17.880,에너지 값을 구하겠다는 겁니다.,677.88,6
11:18.080,11:21.920,"즉 어떤 h 값과 가장 많은 연관성을 가지는지를,",681.92,6
11:22.020,11:23.840,"에너지 값으로 구할 수가 있는 거고,",683.84,6
11:24.140,11:26.740,"이제 이러한 에너지 값에 소프트맥스를 치유해서,",686.74,6
11:26.800,11:27.660,확률 값을 구합니다.,687.66,6
11:27.840,11:29.160,"즉 실제로 비율적으로,",689.16,6
11:29.360,11:33.380,"이 각각의 h 값들 중에서 어떤 값과 가장 연관성이 높은지를,",693.38,6
11:33.400,11:34.340,"구하도록 만들고,",694.34,6
11:34.480,11:38.500,"이제 이러한 가중치 값을 실제로 h 값과 곱하도록 만들어서,",698.5,6
11:38.640,11:41.860,"이러한 가중치가 반영된 각각의 인코더의 출력을,",701.86,6
11:41.860,11:44.720,출력 결과를 더해서 그것을 활용하는 것입니다.,704.72,6
11:44.900,11:46.460,"그래서 이 그림은 실제로,",706.46,7
11:46.640,11:49.060,"이 attention mechanism을 제안한 논문에서,",709.06,7
11:49.180,11:50.600,보여주고 있는 그림인데요.,710.6,7
11:51.020,11:54.560,자 보시면 마찬가지로 에너지랑 이 가중치의 공식은 동일합니다.,714.56,7
11:54.880,11:58.960,"보시면은 매번 디코더 파트에서 각각의 단어를 만들기 위해서는,",718.96,7
11:58.980,12:02.040,이런 식으로 h 값을 이용해서 만들 수가 있는데요.,722.04,7
12:02.340,12:05.460,"현재 h 즉 st 를 만들기 위해서,",725.46,7
12:05.620,12:08.900,"이전에 사용했던 h 값과 이 인코더 파트의,",728.9,7
12:09.000,12:11.080,"모든 각각의 h 값을,",731.08,7
12:11.080,12:13.860,"같이 묶어서 에너지 값을 구한 뒤에,",733.86,7
12:13.940,12:15.820,"이제 거기에 소프트맥스를 취해서,",735.82,7
12:15.940,12:17.740,이렇게 비율 값을 구할 수 있는 거예요.,737.74,7
12:17.880,12:20.320,즉 그러한 비율이 각각 이 a 가 되겠습니다.,740.32,7
12:20.680,12:22.960,"예를 들어서 만약에 입력 문장이,",742.96,7
12:23.040,12:26.460,I am a teacher 라고 해볼게요.,746.46,7
12:26.700,12:31.100,"이때 I am a teacher에 각각의 단어들 중에서,",751.1,7
12:31.180,12:33.400,"어떤 걸 가장 많이 참고하면 되는지를,",753.4,7
12:33.500,12:36.380,그 비율 값을 이렇게 퍼센테이지로 구해주는 겁니다.,756.38,7
12:36.700,12:40.300,"예를 들어서 i는 70%, m은 20%,,",760.3,7
12:41.080,12:43.820,"ㄴ은 5%, t도 5% 이런 식으로,",763.82,7
12:43.940,12:45.920,"다 더했을 때 100이 될 수 있도록,",765.92,7
12:45.940,12:48.420,"그 확률 값을 구해서 그 비율만큼,",768.42,7
12:48.420,12:50.720,"실제로 이 h 값을 곱한 것을,",770.72,7
12:50.800,12:53.260,이 context vector처럼 사용을 할 수 있다는 겁니다.,773.26,7
12:53.440,12:55.380,"그래서 매번 출력을 할 때마다,",775.38,7
12:55.440,12:57.600,"이렇게 소스 문장에서 출력되었던,",777.6,7
12:57.720,13:00.100,"모든 hidden state 값들을 전부 반영해서,",780.1,7
13:00.360,13:03.600,이렇게 다음에 뭘 출력할지를 만들 수 있다는 겁니다.,783.6,7
13:04.040,13:07.020,"그래서 이렇게 매번 출력 단어를 만들 때마다,",787.02,7
13:07.200,13:09.060,"이렇게 소스 문장에서 등장했던,",789.06,7
13:09.260,13:11.060,"모든 hidden state 값들을, 전부 반영해서,",791.06,7
13:11.840,13:13.200,사용할 수가 있는 겁니다.,793.2,7
13:13.380,13:15.580,"즉 다시 한 번 용어를 정리해 드리자면,",795.58,7
13:15.680,13:16.920,"이 energy 값은,",796.92,7
13:16.940,13:19.760,"소스 문장에서 나왔던 모든 출력 값들 중에서,",799.76,7
13:19.920,13:23.060,"어떤 값과 가장 연관성이 있는지를 구하기 위해서,",803.06,7
13:23.240,13:24.780,그 수치를 구한 것이고요.,804.78,7
13:24.920,13:27.200,"이제 그 값들을 softmax에 넣어서,",807.2,7
13:27.320,13:29.240,"상대적인 확률 값을 구한 것이,",809.24,7
13:29.300,13:30.640,그 가중치라고 할 수 있겠고요.,810.64,7
13:30.820,13:33.740,"그러한 가중치 값들을 실제로 각각의,",813.74,7
13:33.760,13:36.260,"그 소스 문장의 hidden state와 곱해줘서,",816.26,7
13:36.340,13:39.420,"전부 더해준 값을 실제로 디코더의 입력으로,",819.42,7
13:39.480,13:40.880,"같이 넣어주겠다라고,",820.88,7
13:41.080,13:41.660,보시면 되겠습니다.,821.66,7
13:41.940,13:44.360,"이제 이런 식으로 attention 메커니즘을 쓰게 되면,",824.36,8
13:44.460,13:45.780,"성능이 좋아질 뿐만 아니라,",825.78,8
13:45.960,13:47.500,"또 추가적인 장점으로는,",827.5,8
13:47.500,13:49.780,이러한 attention은 시각화할 수도 있다는 건데요.,829.78,8
13:49.800,13:51.480,"이런 식으로 attention 가중치,",831.48,8
13:51.620,13:53.360,"즉 구해진 확률 값을 이용해서,",833.36,8
13:53.520,13:54.900,"매번 출력이 나올 때마다,",834.9,8
13:54.940,13:58.020,"그 출력이 입력에서 어떤 정보를 참고했는지를,",838.02,8
13:58.020,13:58.780,구할 수가 있습니다.,838.78,8
13:59.120,14:00.120,"지금 보시면 이거는,",840.12,8
14:00.260,14:02.300,영어를 불어로 번역한 예시인데요.,842.3,8
14:02.720,14:06.160,여기서 the arrangement on the European 이렇게 나오죠.,846.16,8
14:06.320,14:08.200,"이 각각의 단어들이 있을 때,",848.2,8
14:08.360,14:10.920,"이제 매번 이 불어의 단어들을,",850.92,8
14:11.080,14:11.780,"출력할 때마다,",851.78,8
14:12.320,14:13.640,"이러한 입력 단어들 중에서,",853.64,8
14:13.740,14:16.480,"어떤 단어에 가장 많은 초점을 뒀는지를,",856.48,8
14:16.500,14:17.440,구할 수가 있는 겁니다.,857.44,8
14:17.760,14:19.580,"지금 이런 식으로 밝게 표시된 부분이,",859.58,8
14:19.760,14:21.540,확률 값이 높은 부분이라고 할 수 있습니다.,861.54,8
14:21.880,14:23.900,"이런 식으로 매번 출력할 때마다,",863.9,8
14:24.060,14:26.320,"이 출력하는 단어가 입력 단어 중에서,",866.32,8
14:26.440,14:29.060,"어떤 단어에 더욱 많은 가중치를 두어서,",869.06,8
14:29.300,14:31.540,attention을 수행했는지를 구할 수가 있는 겁니다.,871.54,8
14:31.800,14:33.080,"기본적으로 딥러닝은,",873.08,8
14:33.140,14:35.180,"매우 많은 파라미터를 가지고 있기 때문에,",875.18,8
14:35.340,14:37.620,"그러한 세부적인 파라미터를 일일이 분석하면서,",877.62,8
14:37.980,14:39.580,"어떤 원리로 동작을 했는지를,",879.58,8
14:39.600,14:40.860,알아내기는 쉽지 않습니다.,880.86,8
14:41.080,14:42.000,"그렇기 때문에,",882.0,8
14:42.020,14:43.760,"이렇게 attention 메커니즘은,",883.76,8
14:43.760,14:46.060,"실제로 딥러닝이 어떤 요소에,",886.06,8
14:46.080,14:48.880,"더욱 더 많은 초점을 두어서 분류를 했는가,",888.88,8
14:48.920,14:51.160,"혹은 어떠한 데이터를 만들어냈는가,",891.16,8
14:51.180,14:53.060,"같은 과정을 분석할 때,",893.06,8
14:53.100,14:54.380,용이하게 사용할 수 있습니다.,894.38,8
14:54.560,14:57.520,"그렇다면 오늘 리뷰하고 있는 트랜스포머 논문은,",897.52,9
14:57.560,14:59.320,어떤 원리로 동작할까요?,899.32,9
14:59.460,15:01.120,"트랜스포머는 말씀드렸듯이,",901.12,9
15:01.180,15:02.640,"현대의 딥러닝 기반,",902.64,9
15:02.660,15:03.900,"자연화 처리 네트워크에서,",903.9,9
15:03.960,15:05.400,핵심이 되는 논문 중 하나입니다.,905.4,9
15:05.560,15:07.540,"그래서 논문의 원래 제목은,",907.54,9
15:07.580,15:09.140,attention is all you need 이고요.,909.14,9
15:09.260,15:10.980,"말 그대로 attention 기법만,",910.98,9
15:10.980,15:11.900,"잘 활용해도,",911.9,9
15:11.980,15:13.860,"다양한 자연화 처리 태스크에서,",913.86,9
15:13.980,15:16.000,좋은 성능을 얻을 수 있다는 의미입니다.,916.0,9
15:16.200,15:16.920,"다시 말해,",916.92,9
15:16.960,15:18.520,"attention 기법만 쓰기 때문에,",918.52,9
15:18.720,15:19.280,"RNN,",919.28,9
15:19.440,15:21.420,CNN 등을 전혀 필요로 하지 않습니다.,921.42,9
15:21.700,15:22.460,"진짜 말 그대로,",922.46,9
15:22.640,15:23.800,"attention 기법만 사용해서,",923.8,9
15:23.980,15:25.240,"기계 번역부터 시작해서,",925.24,9
15:25.440,15:27.700,다양한 자연화 처리 태스크를 수행할 수 있는 겁니다.,927.7,9
15:28.020,15:29.320,"오른쪽에 보이는 그림이,",929.32,10
15:29.360,15:31.140,"원본 논문에서 보여주고 있는,",931.14,10
15:31.360,15:32.700,트랜스포머의 아키텍처인데요.,932.7,10
15:33.000,15:34.200,"여기 보이는 것과 같이,",934.2,10
15:34.440,15:37.200,실제로 RNN과 CNN을 전혀 사용하지 않습니다.,937.2,10
15:37.380,15:38.140,"물론 이런 식으로,",938.14,10
15:38.360,15:40.860,"RNN, CNN 등을 전혀 사용하지 않는다면,",940.86,10
15:40.980,15:43.660,"문장 안에 포함되어 있는 각각의 단어들의,",943.66,10
15:43.720,15:45.820,순서에 대한 정보를 주기가 어렵습니다.,945.82,10
15:46.100,15:46.760,"그렇기 때문에,",946.76,10
15:46.940,15:48.500,"트랜스포머는 문장 내,",948.5,10
15:48.600,15:49.800,"각각의 단어들에 대한,",949.8,10
15:49.880,15:51.660,"순서에 대한 정보를 알려주기 위해서,",951.66,10
15:51.860,15:54.080,"별도로 포지셔널 인코딩을 이용해서,",954.08,10
15:54.200,15:55.720,순서에 대한 정보를 줄 수 있습니다.,955.72,10
15:56.020,15:57.200,"이제 이러한 아키텍처는,",957.2,10
15:57.300,15:57.860,"향후,",957.86,10
15:57.920,15:59.720,"볼트나 GPT와 같은,",959.72,10
15:59.840,16:02.260,더욱 향상된 네트워크에서도 채택이 되었고요.,962.26,10
16:02.400,16:03.280,"또한 참고로,",963.28,10
16:03.320,16:04.800,"RNN을 사용하지 않지만,",964.8,10
16:05.020,16:08.120,마찬가지로 인코더와 디코더 파트로 구성되는 건 동일합니다.,968.12,10
16:08.360,16:10.700,"또한 attention 과정을 한 번만 쓰는 게 아니라,",970.7,10
16:10.700,16:13.660,여러 레이어를 거쳐서 반복하도록 만듭니다.,973.66,10
16:13.700,16:14.980,"즉 이러한 인코더가,",974.98,10
16:15.060,16:16.480,"여러 번 중첩되어,",976.48,10
16:16.480,16:18.460,"즉 N번만큼 중첩되어,",978.46,10
16:18.460,16:19.980,사용하도록 만든다는 건데요.,979.98,10
16:20.100,16:21.980,"참고로 지금 보이는 그림에서,",981.98,10
16:22.120,16:23.980,"이 왼쪽 파트는 인코더가 되고,",983.98,10
16:24.200,16:26.100,이 오른쪽 파트는 디코더가 되겠습니다.,986.1,10
16:26.520,16:28.800,한번 자세한 내용을 지금부터 알아볼게요.,988.8,10
16:29.000,16:29.540,"자 우리가,",989.54,10
16:29.760,16:31.080,"어떠한 단어 정보를,",991.08,11
16:31.200,16:33.220,"네트워크에 넣기 위해서는 일반적으로,",993.22,11
16:33.360,16:34.960,보통 인베딩 과정을 거칩니다.,994.96,11
16:35.180,16:36.120,"그렇게 해주는 이유는,",996.12,11
16:36.160,16:36.440,"일단,",996.44,11
16:36.520,16:38.620,"맨 처음에 입력 차원 자체는,",998.62,11
16:38.700,16:40.000,"특정 언어에서,",1000.0,11
16:40.040,16:40.680,"존재할 수 있는,",1000.68,11
16:40.700,16:42.880,"단어의 개수와 같기 때문에,",1002.88,11
16:43.000,16:45.120,"또한 그렇게 차원이 많을 뿐만 아니라,",1005.12,11
16:45.260,16:46.400,"각각의 정보들은,",1006.4,11
16:46.500,16:48.740,"원핫 인코딩 형태로 표현이 되기 때문에,",1008.74,11
16:48.920,16:50.500,"일반적으로 네트워크에 넣을 때는,",1010.5,11
16:50.640,16:52.260,"먼저 인베딩 과정을 거쳐서,",1012.26,11
16:52.340,16:53.800,"더욱 더 적은 차원의,",1013.8,11
16:53.820,16:55.520,컨티뉴어스한 값으로 표현합니다.,1015.52,11
16:55.760,16:58.420,즉 어떠한 실수 값으로 표현할 수가 있다는 건데요.,1018.42,11
16:58.460,16:59.480,"그래서 예를 들어,",1019.48,11
16:59.580,17:00.120,"이런 식으로,",1020.12,11
17:00.340,17:01.680,"I am a teacher와 같은,",1021.68,11
17:01.840,17:03.340,"하나의 문장이 들어왔을 때,",1023.34,11
17:03.540,17:04.020,"얘는 실제로,",1024.02,11
17:04.340,17:06.540,Input-Invading Matrix로 만들어집니다.,1026.54,11
17:06.660,17:07.680,"이때 일반적으로,",1027.68,11
17:07.860,17:08.900,"이 매트릭스는,",1028.9,11
17:08.920,17:10.340,"단어의 개수만큼,",1030.34,11
17:10.380,17:10.680,"단어의 개수와,",1030.68,11
17:10.700,17:12.140,행의 크기를 가지고요.,1032.14,11
17:12.140,17:12.860,"즉 이런 식으로,",1032.86,11
17:12.960,17:14.760,"I am a teacher라는 값이,",1034.76,11
17:14.780,17:16.580,"이렇게 행 형태로 들어오게 되고,",1036.58,11
17:16.800,17:18.360,"이 각각의 열 데이터는,",1038.36,11
17:18.460,17:20.360,"인베딩 차원과 같은 크기의,",1040.36,11
17:20.440,17:21.560,"데이터가 담긴,",1041.56,11
17:21.640,17:22.780,배열을 사용하게 됩니다.,1042.78,11
17:23.020,17:24.460,"현재 그림에선 이런 식으로,",1044.46,11
17:24.480,17:25.520,"총 4개의 단어가,",1045.52,11
17:25.620,17:26.660,"존재하기 때문에,",1046.66,11
17:26.980,17:28.420,"이렇게 각각의 단어들에 대해서,",1048.42,11
17:28.820,17:30.720,"그 단어에 대한 정보를 포함하고 있는,",1050.72,11
17:30.900,17:33.260,인베딩 값들을 각각 구할 수가 있다는 거죠.,1053.26,11
17:33.440,17:34.200,"이런 식으로 다,",1054.2,11
17:34.320,17:35.720,구할 수가 있는 겁니다.,1055.72,11
17:36.100,17:37.420,"이러한 인베딩 디멘전은,",1057.42,11
17:37.500,17:39.280,"모델 아키텍처를 만드는 사람이,",1059.28,11
17:39.460,17:40.640,"임의로 설정해줄 수 있는,",1060.64,11
17:40.640,17:40.840,데요.,1060.84,11
17:40.940,17:42.000,"원본 논문에서는,",1062.0,11
17:42.000,17:44.120,512 정도의 값을 사용합니다.,1064.12,11
17:44.300,17:45.260,"물론 이 값은,",1065.26,11
17:45.320,17:47.260,"모델의 아키텍처를 만드는 사람마다,",1067.26,11
17:47.280,17:48.680,다르게 설정할 수가 있는 거예요.,1068.68,11
17:48.860,17:49.480,"아무튼 그래서,",1069.48,11
17:49.640,17:51.300,"이런 식으로 전통적인 인베딩은,",1071.3,11
17:51.400,17:52.560,"네트워크에 넣기 전에,",1072.56,11
17:52.580,17:54.200,"입력 값들을 인베딩 형태로,",1074.2,11
17:54.300,17:55.060,"표현하기 위해서,",1075.06,11
17:55.220,17:57.060,사용하는 레이어라고 볼 수 있습니다.,1077.06,11
17:57.340,17:58.000,"이때 우리가,",1078.0,12
17:58.200,17:59.600,"시퀀스 투 시퀀스와 같은,",1079.6,12
17:59.760,18:00.720,"RNN 기반의,",1080.72,12
18:00.840,18:02.400,"아키텍처를 사용한다고 하면,",1082.4,12
18:02.720,18:04.260,"RNN을 사용하는 것만으로도,",1084.26,12
18:04.380,18:05.360,"각각의 단어가,",1085.36,12
18:05.480,18:06.640,"RNN에 들어갈 때,",1086.64,12
18:06.760,18:08.180,"순서에 맞게 들어가기 때문에,",1088.18,12
18:08.180,18:10.600,"자동으로 각각의 히든스텔트 값은,",1090.6,12
18:10.600,18:12.400,순서에 대한 정보를 가지게 되는데요.,1092.4,12
18:12.540,18:14.240,"만약에 트랜스포머와 같이,",1094.24,12
18:14.260,18:16.280,"RNN 자체를 사용하지 않는다면,",1096.28,12
18:16.480,18:18.120,"위치에 대한 정보를 주기 위해서,",1098.12,12
18:18.300,18:18.700,"즉,",1098.7,12
18:18.700,18:20.180,"하나의 문장에 포함되어 있는,",1100.18,12
18:20.240,18:21.380,"각각의 단어 중에서,",1101.38,12
18:21.500,18:22.960,"어떤 단어가 앞에 오는 것이고,",1102.96,12
18:23.120,18:24.660,"어떠한 단어가 뒤에 오는 것인지,",1104.66,12
18:24.820,18:26.500,"그러한 정보를 알려주기 위해서는,",1106.5,12
18:26.580,18:28.260,"위치에 대한 정보를 포함하고 있는,",1108.26,12
18:28.400,18:29.880,인베딩을 사용할 필요가 있습니다.,1109.88,12
18:30.160,18:31.060,"이제 이를 위해,",1111.06,12
18:31.180,18:32.020,"트랜스포머에서는,",1112.02,12
18:32.280,18:34.180,"위치에 대한 정보를 인코딩하고 있는,",1114.18,12
18:34.400,18:35.200,"위치 인코딩,",1115.2,12
18:35.260,18:35.600,"즉,",1115.6,12
18:35.600,18:37.160,포지셔널 인코딩을 사용합니다.,1117.16,12
18:37.300,18:38.060,"즉, 이런 식으로,",1118.06,12
18:38.180,18:39.580,"인풋 인베딩 매트릭스와,",1119.58,12
18:39.680,18:40.580,"같은 크기,",1120.58,12
18:40.580,18:42.060,"즉, 같은 디멘전을 가지는,",1122.06,12
18:42.200,18:44.440,"별도의 위치에 대한 정보를 가지고 있는,",1124.44,12
18:44.460,18:46.020,"인코딩 정보를 넣어줘서,",1126.02,12
18:46.100,18:48.380,"각각 엘레멘트 와이즈로 더해줌으로써,",1128.38,12
18:48.480,18:49.420,"각각의 단어가,",1129.42,12
18:49.540,18:51.660,"어떤 순서를 가지는지에 대한 정보를,",1131.66,12
18:51.840,18:53.760,네트워크가 알 수 있도록 만드는 것입니다.,1133.76,12
18:54.040,18:54.960,"이제 그렇게 실제로,",1134.96,12
18:55.280,18:58.120,"위치에 대한 정보까지 포함하고 있는 입력 값을,",1138.12,13
18:58.220,19:00.200,실제 어텐션에 넣어줄 수 있도록 합니다.,1140.2,13
19:00.460,19:02.300,"즉, 이렇게 어텐션이 받는 값은,",1142.3,13
19:02.420,19:04.040,"입력 문장에 대한 정보에다가,",1144.04,13
19:04.040,19:05.760,"실제 위치에 대한 정보까지,",1145.76,13
19:05.800,19:07.820,같이 포함되어 있는 입력 값입니다.,1147.82,13
19:08.060,19:08.720,"이제 그래서,",1148.72,13
19:08.840,19:09.980,"그러한 입력을 받아서,",1149.98,13
19:10.100,19:11.800,"각각의 단어들을 이용해서,",1151.8,13
19:11.960,19:13.180,어텐션을 수행하고요.,1153.18,13
19:13.260,19:15.940,"이제 이렇게 인코더 파트에서 수행하는 어텐션은,",1155.94,13
19:15.980,19:17.300,"self-attention이라고 해서,",1157.3,13
19:17.480,19:19.040,"각각의 단어가 서로에게,",1159.04,13
19:19.140,19:22.000,어떤 연관성을 가지고 있는지를 구하기 위해 사용합니다.,1162.0,13
19:22.280,19:23.160,"예를 들어 이런 식으로,",1163.16,13
19:23.460,19:25.560,"I am a teacher라고 문장이 들어오게 되면,",1165.56,13
19:25.860,19:27.780,"이 문장을 구성하는 각각의 단어인,",1167.78,13
19:27.820,19:30.300,"I am a teacher가,",1170.3,13
19:30.360,19:32.860,"각각 서로에게 어텐션 스코어를 구해서,",1172.86,13
19:33.000,19:34.020,"각각의 단어는,",1174.02,13
19:34.020,19:37.520,"다른 어떠한 단어와 높은 연관성을 가지는지에 대한 정보를,",1177.52,13
19:37.600,19:38.960,학습하도록 만들 수 있습니다.,1178.96,13
19:39.140,19:39.780,"다시 말해,",1179.78,13
19:39.820,19:40.740,"이러한 어텐션은,",1180.74,13
19:40.760,19:42.540,"이 전반적인 입력 문장에 대한,",1182.54,13
19:42.700,19:45.560,문맥에 대한 정보를 잘 학습하도록 만드는 것입니다.,1185.56,13
19:45.820,19:47.380,"또한 여기에서 추가적으로,",1187.38,14
19:47.660,19:50.060,residual learning과 같은 테크닉이 사용되는데요.,1190.06,14
19:50.300,19:52.160,"이런 residual learning 같은 경우는,",1192.16,14
19:52.260,19:54.460,"대표적인 이미지 분류 네트워크인,",1194.46,14
19:54.500,19:57.500,"레지넷과 같은 네트워크에서 사용되고 있는 기법으로,",1197.5,14
19:57.660,19:58.960,"이렇게 어떠한 값을,",1198.96,14
19:59.040,20:02.220,"레이어를 거쳐서 반복적으로 단순하게 갱신하는 것이 아니라,",1202.22,14
20:02.440,20:04.000,"특정 레이어를 건너뛰어서,",1204.0,14
20:04.020,20:07.060,복사가 된 값을 그대로 넣어주는 기법을 의미합니다.,1207.06,14
20:07.240,20:09.620,"이런 식으로 특정 레이어를 건너뛰어서,",1209.62,14
20:09.720,20:11.540,"입력할 수 있도록 만드는 것을,",1211.54,14
20:11.620,20:13.980,일반적으로 residual connection이라고 부르고요.,1213.98,14
20:14.120,20:15.200,"이렇게 해줌으로써,",1215.2,14
20:15.260,20:18.120,"전체 네트워크는 기존 정보를 입력 받으면서,",1218.12,14
20:18.440,20:21.680,"추가적으로 잔여된 부분만 학습하도록 만들기 때문에,",1221.68,14
20:21.920,20:23.480,"전반적인 학습 난이도가 낮고,",1223.48,14
20:23.660,20:26.620,"그렇기 때문에 초기의 모델 수렴 속도가 높게 되고,",1226.62,14
20:26.940,20:30.340,"그로 인해 더욱 더 글로벌 옵티마를 찾을 확률이 높아지기 때문에,",1230.34,14
20:30.540,20:32.520,"전반적으로 다양한 네트워크에 대해서,",1232.52,14
20:32.700,20:34.000,"residual learning을 사용해,",1234.0,14
20:34.020,20:36.740,성능이 좋아지는 것을 많이 목격할 수 있고요.,1236.74,14
20:36.920,20:38.480,"트랜스포머 또한 마찬가지로,",1238.48,14
20:38.480,20:40.600,"그런 아이디어를 전적으로 채택해서,",1240.6,14
20:40.700,20:42.220,성능을 높였다고 할 수 있는 겁니다.,1242.22,14
20:42.440,20:44.520,"그래서 이렇게 attention을 수행해주고,",1244.52,14
20:44.520,20:47.300,"나온 값과 residual connection을 이용해서,",1247.3,14
20:47.400,20:49.680,"바로 더해진 값을 같이 받아서,",1249.68,14
20:49.920,20:51.720,"노멀라이제이션까지 수행한 뒤에,",1251.72,14
20:51.900,20:53.880,그 결과를 내보낼 수 있도록 만듭니다.,1253.88,14
20:53.980,20:55.920,이것이 인코더의 동작 과정이고요.,1255.92,14
20:56.120,20:58.780,"그래서 실제로 이렇게 입력 값이 들어온 이후로부터,",1258.78,15
20:59.000,21:00.100,"attention을 거치고,",1260.1,15
21:00.240,21:02.360,"residual connection 이후에 노멀라이제이션,",1262.36,15
21:02.500,21:03.820,"그 다음에 다시 feed for,",1263.82,15
21:03.820,21:04.860,"레이어를 거친 다음에,",1264.86,15
21:05.020,21:06.740,"마찬가지로 residual learning,",1266.74,15
21:07.020,21:08.660,"그리고 노멀라이제이션을 추가해서,",1268.66,15
21:08.780,21:10.920,"결과적으로 하나의 인코더 레이어에서,",1270.92,15
21:11.060,21:12.760,그 결과 값을 뽑아낼 수 있습니다.,1272.76,15
21:12.960,21:16.580,"이런 식으로 attention과 정교화 과정을 반복하는 방식으로,",1276.58,15
21:16.640,21:18.800,여러 개의 레이어를 중첩해서 사용합니다.,1278.8,15
21:19.060,21:20.420,"이때 한 가지 유의할 점은,",1280.42,15
21:20.440,21:23.380,각각의 레이어는 서로 다른 파라미터를 가집니다.,1283.38,15
21:23.520,21:26.580,"예를 들어, 이 레이어 1번과 레이어 2번에 포함되어 있는,",1286.58,15
21:26.740,21:29.820,"attention 및 feed for 레이어에 사용되는 파라미터는,",1289.82,15
21:29.860,21:30.540,서로 다릅니다.,1290.54,15
21:30.680,21:32.740,"또한 이때 이렇게 레이어를 중첩해서,",1292.74,15
21:32.840,21:33.800,"사용할 수 있다는,",1293.8,15
21:33.820,21:35.320,"점에서 유추할 수 있겠지만,",1295.32,15
21:35.500,21:37.040,"이렇게 입력되는 값과,",1297.04,15
21:37.060,21:39.000,출력되는 값의 dimension은 동일합니다.,1299.0,15
21:39.260,21:41.240,"이제 그래서 실제로는 다음과 같이,",1301.24,16
21:41.300,21:44.480,전체 인코더와 디코더의 아키텍처를 그려볼 수 있는데요.,1304.48,16
21:44.740,21:46.360,"이렇게 입력 값이 들어온 다음에,",1306.36,16
21:46.520,21:48.260,"여러 개의 인코더 레이어를 반복해서,",1308.26,16
21:48.520,21:50.720,"가장 마지막에 인코더에서 나오게 된,",1310.72,16
21:50.820,21:53.520,그 출력 값은 이렇게 디코더에 들어가게 됩니다.,1313.52,16
21:53.800,21:54.780,"이렇게 해주는 이유는,",1314.78,16
21:54.820,21:57.280,"우리가 앞서 sequence to sequence 모델의,",1317.28,16
21:57.280,21:59.500,"attention 메커니즘을 활용했을 때와 마찬가지로,",1319.5,16
21:59.600,22:01.800,"디코더 파트에서는 매번 출력할 때마다,",1321.8,16
22:01.980,22:03.620,"입력 소스 문장 중에서,",1323.62,16
22:03.620,22:07.060,"어떤 단어에게 가장 많은 초점을 둬야 되는지를,",1327.06,16
22:07.140,22:08.220,알려주기 위함입니다.,1328.22,16
22:08.380,22:10.720,"다시 말해 이렇게 디코더 파트도 마찬가지로,",1330.72,16
22:10.740,22:12.280,"여러 개의 레이어로 구성이 되고,",1332.28,16
22:12.500,22:14.760,"이 마지막 레이어에서 나오게 된 출력 값이,",1334.76,16
22:14.780,22:17.220,"바로 실제로 우리가 번역을 수행한 결과,",1337.22,16
22:17.360,22:18.760,그 출력 단어가 되는 거고요.,1338.76,16
22:18.940,22:21.240,"이때 각각의 레이어는 이 인코더의,",1341.24,16
22:21.300,22:23.560,"마지막 레이어에서 나오게 된 출력 값을,",1343.56,16
22:23.580,22:24.520,입력으로 받는 것입니다.,1344.52,16
22:24.760,22:28.360,이것이 바로 트랜스포머의 가장 기본적인 동작 방식이고요.,1348.36,16
22:28.540,22:30.340,"물론 이런 식으로 인코더 파트에서,",1350.34,16
22:30.440,22:32.380,"마지막 레이어의 출력 값만 받는 게 아니라,",1352.38,16
22:32.620,22:33.600,"이렇게 각각의 레이어를,",1353.6,16
22:33.600,22:33.600,"이렇게 각각의 레이어를,",1353.6,16
22:33.620,22:36.560,레이어마다 출력 값을 받는 기법도 존재하긴 하지만요.,1356.56,16
22:36.600,22:39.280,"아무튼 기본적인 트랜스포머의 아키텍션은 이런 식으로,",1359.28,16
22:39.420,22:42.580,"인코더의 마지막 레이어에서 나오게 된 출력 값을,",1362.58,16
22:42.640,22:45.640,매번 디코더의 레이어에 넣어주는 방식으로 동작합니다.,1365.64,16
22:45.880,22:47.800,"그래서 이때 디코더 또한 마찬가지로,",1367.8,16
22:47.840,22:49.640,"각각 단어 정보를 받아서,",1369.64,16
22:49.800,22:53.700,"이어서 각 단어의 상대적인 위치에 대한 정보를 알려주기 위해,",1373.7,16
22:53.860,22:56.240,인코딩 값을 추가한 뒤에 입력을 넣게 되고요.,1376.24,16
22:56.400,22:58.300,"참고로 하나의 디코더 레이어에서는,",1378.3,16
22:58.300,22:59.880,두 개의 어텐션을 사용하는데요.,1379.88,16
23:00.160,23:02.620,"첫 번째로 보이는 어텐션은 셀프 어텐션으로,",1382.62,16
23:02.700,23:03.600,"인코더 파트와,",1383.6,16
23:03.600,23:09.300,"마찬가지로 각각의 단어들이 서로가 서로에게 어떠한 가중치를 가지는지를 구하도록 만들어서,",1389.3,16
23:09.380,23:13.680,이 출력되고 있는 문장에 대한 전반적인 표현을 학습할 수 있도록 만들고요.,1393.68,16
23:13.780,23:16.700,"이렇게 이어서 디코더 레이어의 두 번째 어텐션에서는,",1396.7,16
23:16.860,23:19.540,인코더에 대한 정보를 어텐션 할 수 있도록 만듭니다.,1399.54,16
23:19.600,23:21.620,"다시 말해 각각의 출력 단어가,",1401.62,16
23:21.740,23:24.940,인코더의 출력 정보를 받아와 사용할 수 있도록 만듭니다.,1404.94,16
23:25.000,23:27.940,"이건 다시 말해 각각의 출력되고 있는 단어가,",1407.94,16
23:28.040,23:32.240,소스 문장에서의 어떤 단어와 연관성이 있는지를 구해주는 겁니다.,1412.24,16
23:32.420,23:33.580,"그래서 여기 보이는,",1413.58,16
23:33.580,23:37.080,어텐션은 일반적으로 인코더 디코더 어텐션이라고 부르고요.,1417.08,16
23:37.200,23:39.720,"그 동작 원리를 간단한 예시로 설명드리자면,",1419.72,16
23:39.840,23:43.280,"예를 들어 입력 문장이 I am a teacher라고 한다면,",1423.28,16
23:43.460,23:46.320,이렇게 출력 값은 차례대로 나는 선생님이다.,1426.32,16
23:46.380,23:48.320,이런 식으로 출력을 내뱉게 될 건데요.,1428.32,16
23:48.360,23:50.320,"이때 출력되고 있는 단어들,",1430.32,16
23:50.400,23:54.020,"예를 들어서 선생님이라고 단어를 번역한다고 하면,",1434.02,16
23:54.160,23:57.420,"그 선생님이라는 단어는 I am a teacher 중에서,",1437.42,16
23:57.480,24:01.160,어떤 단어와 가장 높은 연관성을 가지는지를 구할 수가 있는 겁니다.,1441.16,16
24:01.380,24:03.560,"그러한 정보를 매번 어텐션을 통해서,",1443.56,16
24:03.560,24:04.800,"계산하도록 만들어서,",1444.8,16
24:04.900,24:09.040,"이렇게 인코더 파트에서 나왔던 출력 결과를 전적으로 활용하도록,",1449.04,16
24:09.080,24:10.940,네트워크를 설계할 수 있는 것입니다.,1450.94,16
24:11.440,24:13.540,"즉 그래서 디코러 또한 마찬가지로,",1453.54,16
24:13.540,24:17.860,"입력으로 들어온 입력 디멘전과 이 출력 디멘전이 같도록 만들어서,",1457.86,16
24:18.000,24:21.360,각각의 디코더 레이어는 여러 번 중첩해서 사용할 수 있습니다.,1461.36,16
24:21.580,24:23.440,"즉 다시 말해 이 트랜스포머에서는,",1463.44,17
24:23.600,24:25.520,"마지막 인코더의 레이어의 출력이,",1465.52,17
24:25.580,24:28.660,모든 디코더의 레이어에 입력되는 형식으로 동작하고요.,1468.66,17
24:28.780,24:32.380,이때 전체 레이어의 개수가 4개일 때의 예시는 다음과 같습니다.,1472.38,17
24:32.660,24:33.500,"일반적으로,",1473.5,17
24:33.560,24:37.740,이 레이어의 개수는 인코더와 디코더가 동일하도록 맞추어 주는 경우가 많고요.,1477.74,17
24:37.860,24:42.340,"즉 이렇게 인코더랑 디코더 둘 다 4개의 레이어로 구성되어 있는 걸 확인할 수 있고,",1482.34,17
24:42.460,24:45.640,"이렇게 인코더 파트에서 마지막 레이어의 출력 값이,",1485.64,17
24:45.660,24:48.460,각각의 디코더 레이어에 입력되는 걸 확인할 수 있습니다.,1488.46,17
24:48.640,24:50.060,"여기서 입력이 된다는 의미는,",1490.06,17
24:50.200,24:53.960,"방금 그림에서 확인했던 여기 디코더 파트의 두 번째 어텐션에서,",1493.96,16
24:54.080,24:55.380,"각각의 출력 단어가,",1495.38,16
24:55.480,25:00.380,"입력 단어 중에서 어떤 정보와 가장 높은 연관성을 가지는지를 계산하도록 만들어주는,",1500.38,16
25:00.480,25:02.960,바로 여기 부분에서 사용된다고 보시면 되겠습니다.,1502.96,16
25:03.180,25:03.540,"또한,",1503.54,16
25:03.540,25:06.880,말씀드렸듯이 트랜스포머에서도 인코더와 디코더의 구조를 따릅니다.,1506.88,18
25:07.000,25:07.440,"즉,",1507.44,18
25:07.440,25:09.980,"RNN을 사용하지 않는다는 점이 큰 차이점이고,",1509.98,18
25:10.060,25:13.040,인코더와 디코더를 다수 사용한다는 점이 특징입니다.,1513.04,18
25:13.320,25:14.460,"여기서 재미있는 점은,",1514.46,18
25:14.460,25:17.660,"원래 기본적으로 RNN을 사용할 때는 인코더,",1517.66,18
25:17.740,25:21.200,"즉 LSTM이나 RNN 등은 고정된 크기로 사용하고,",1521.2,18
25:21.320,25:27.580,"이 입력 단어의 개수만큼 반복적으로 인코더 레이어를 거쳐서 매번 히든 스테이트를 만들었다고 하면,",1527.58,18
25:27.820,25:31.800,"트랜스포머에서는 입력 단어 자체가 하나로 쭉 연결되어서,",1531.8,18
25:31.920,25:33.140,"한 번에 입력이 되고,",1533.14,18
25:33.540,25:35.860,한 번에 그에 대한 어텐션 값을 구한다고 할 수 있습니다.,1535.86,18
25:36.020,25:36.460,"즉,",1536.46,18
25:36.460,25:37.020,"다시 말해,",1537.02,18
25:37.020,25:40.760,"RNN을 사용했을 때와 다르게 위치에 대한 정보를 한꺼번에 넣어서,",1540.76,18
25:40.880,25:45.300,"한 번에 인코더를 거칠 때마다 병렬적으로 출력 값을 구해낼 수 있기 때문에,",1545.3,18
25:45.480,25:50.460,RNN을 사용했을 때와 비교하여 일반적으로 계산 복잡도가 더 낮게 형성됩니다.,1550.46,18
25:50.640,25:55.460,"또한 실제로 학습을 수행할 때는 이러한 입력 값들 전부를 한꺼번에 넣을 수 있기 때문에,",1555.46,18
25:55.620,25:58.900,RNN을 사용하지 않고 학습을 진행할 수 있다는 점이 장점인데요.,1558.9,18
25:59.100,26:02.020,"다만 이제 실제로 모델에서 출력 값을 내보낼 때는,",1562.02,18
26:02.080,26:03.520,"마찬가지로 이 디코더 아키더가,",1563.52,18
26:03.520,26:04.820,"아키텍처를 여러 번 사용해서,",1564.82,18
26:04.900,26:09.720,이렇게 EOS가 나올 때까지 반복하도록 만들어서 출력 값을 구하도록 만들 수 있습니다.,1569.72,18
26:09.920,26:14.740,"보시면 이렇게 중간에 컨텍스트 벡터를 압축하는 과정 등이 완전히 생략이 되어 있기 때문에,",1574.74,18
26:14.800,26:20.920,네트워크 자체에서 LSTM과 같은 RNN 구조를 아예 사용할 필요가 없다는 점이 장점이라고 할 수 있습니다.,1580.92,18
26:21.140,26:25.240,이제 그래서 실제로 이 멀티헤드 어텐션이 뭔지 한 번 알아보도록 할게요.,1585.24,16
26:25.460,26:32.140,이렇게 트랜스포머에서 사용되는 각각의 어텐션은 여러 개의 헤드를 가진다고 해서 멀티헤드 어텐션이라고 부르는데요.,1592.14,16
26:32.360,26:33.480,"그 실제 구조는,",1593.48,19
26:33.480,26:34.400,바로 다음과 같습니다.,1594.4,19
26:34.660,26:38.540,바로 오른쪽에 보이는 그림이 멀티헤드 어텐션을 보여주고 있는 그림이고요.,1598.54,19
26:38.640,26:42.120,이때 이렇게 중간에 scaled.product.attention이 사용되는데요.,1602.12,19
26:42.340,26:46.280,이러한 scaled.product.attention은 바로 왼쪽 그림과 같이 구성됩니다.,1606.28,19
26:46.440,26:48.820,"이때 이러한 어텐션 메커니즘을 이해하기 위해서는,",1608.82,19
26:48.820,26:52.460,"Query와 Key, Value가 무엇인지 알 필요가 있는데요.",1612.46,19
26:52.740,26:55.220,이때 Query는 무언가를 물어보는 주체입니다.,1615.22,19
26:55.360,26:57.640,"즉, 어텐션 메커니즘을 간단히 설명하면,",1617.64,19
26:57.760,27:02.520,어떠한 단어가 다른 단어들과 어떠한 연관성을 가지는지를 구하는 것이라 할 수 있는데요.,1622.52,19
27:02.740,27:03.240,"이때,",1623.24,19
27:03.240,27:04.820,"물어보는 주체가 Query이고,",1624.82,19
27:04.880,27:06.740,그 물어보는 대상이 Key입니다.,1626.74,19
27:06.920,27:07.420,"예를 들어,",1627.42,19
27:07.540,27:10.160,"I am a teacher라는 하나의 문장이 있을 때,",1630.16,19
27:10.260,27:13.160,"I am a teacher에 포함되어 있는 각각의 단어가,",1633.16,19
27:13.260,27:16.500,"다른 단어와 얼마나 연관성을 가지는지 측정하기 위해서,",1636.5,19
27:16.680,27:18.180,"Self-Attention을 수행할 수 있는데,",1638.18,19
27:18.440,27:18.860,"이때,",1638.86,19
27:18.960,27:24.220,"I라는 단어가 I am a teacher 각각에 대해서 얼마나 연관성이 있는지 구한다고 치면,",1644.22,19
27:24.320,27:26.240,그때 I가 Query가 되는 거고요.,1646.24,19
27:26.380,27:29.240,I am a teacher 각각 단어들은 Key가 되는 것입니다.,1649.24,19
27:29.500,27:32.500,"즉, 어떠한 단어가 다른 어떠한 단어들에 관해서,",1652.5,19
27:32.500,27:35.260,"어떠한 가중치 값을 가지는지 구하고자 한다면,",1655.26,19
27:35.360,27:40.440,이런 식으로 각각의 Key에 대해서 Attention Score를 구해오는 방식으로 동작하는 것입니다.,1660.44,19
27:40.740,27:44.340,"이때, 그렇게 Score를 구한 뒤에는 실제로 Value 값들과 곱해서,",1664.34,19
27:44.480,27:47.140,결과적인 Attention Value 값을 구할 수 있는 겁니다.,1667.14,19
27:47.400,27:48.700,"내용을 확인해 보시면 이런 식으로,",1668.7,19
27:48.920,27:51.260,"물어보는 주체, 즉 Query가 들어오고,",1671.26,19
27:51.340,27:53.380,"각각의 Attention을 수행할 단어들,",1673.38,19
27:53.460,27:54.920,그 정보가 Key로 들어가는 겁니다.,1674.92,19
27:55.180,27:58.260,"그래서 행렬급을 수행한 뒤에 간단하게 스케일링을 해주고,",1678.26,19
27:58.400,27:59.920,"필요하다면 마스크를 씌워준 다음에,",1679.92,19
28:00.080,28:02.440,"이제 Softmax를 취해서 각각의 Key 중에서,",1682.44,19
28:02.500,28:05.360,"어떤 단어와 가장 높은 연관성을 가지는지를,",1685.36,19
28:05.440,28:06.680,그 비율을 구할 수 있습니다.,1686.68,19
28:06.860,28:10.000,앞에서 공부했었던 Attention Mechanism과 같다고 할 수 있죠.,1690.0,19
28:10.200,28:13.120,"그렇게 구해진 확률 값과 실제로 Value 값을 곱해서,",1693.12,19
28:13.260,28:17.540,가중치가 적용된 결과적인 Attention Value를 구할 수가 있는 겁니다.,1697.54,19
28:17.800,28:21.600,이제 그러한 과정이 이렇게 Scaled.Paradox Attention에서 수행되는 것이고요.,1701.6,19
28:21.780,28:23.340,"또한 여기에서 참고로,",1703.34,19
28:23.380,28:25.080,"실제로 입력 값이 들어왔을 때,",1705.08,19
28:25.160,28:27.560,그러한 입력 값들은 A 체계로 구분됩니다.,1707.56,19
28:27.740,28:29.740,"즉, 어떠한 입력 문장이 들어왔을 때,",1709.74,19
28:29.840,28:32.420,"이제 그것은 Value, Key, Query로 구분되는데,",1712.42,19
28:32.500,28:38.380,"이때 A 체계의 서로 다른 Value와 Key, Query로 구분될 수 있도록 만드는 것입니다.",1718.38,19
28:38.600,28:39.580,"이렇게 해주는 이유는,",1719.58,19
28:39.580,28:43.520,"A 체계의 서로 다른 Attention Concept을 학습하도록 만들어서,",1723.52,19
28:43.640,28:48.220,더욱더 구분된 다양한 특징들을 학습할 수 있도록 유도해준다는 장점이 있습니다.,1728.22,19
28:48.480,28:52.240,"그래서 이와 같이 입력으로 들어온 값은 세 개로 복제가 되어서,",1732.24,19
28:52.360,28:55.360,"각각 Value, Key, Query로 들어가게 되고,",1735.36,19
28:55.560,28:59.980,"이러한 Value, Key, Query 값들은 Linear Layer, 즉 행렬 곱을 수행해서,",1739.98,19
29:00.100,29:02.480,"A 체계로 구분된 각각의 Query를,",1742.48,19
29:02.500,29:03.680,"A 체계의 서로 다른 Attention Concept을 만들어내게 되고,",1743.68,19
29:03.820,29:06.600,"이때 여기에서 H는 Head의 개수이기 때문에,",1746.6,19
29:06.660,29:10.660,"각각 서로 다른 Head끼리 이렇게 Value, Key, Query의 쌍을 받아서,",1750.66,19
29:10.740,29:12.740,Attention을 수행해서 결과를 내보냅니다.,1752.74,19
29:12.860,29:14.980,"이제 그 다음에 앞서 말씀 드렸듯이,",1754.98,19
29:14.980,29:16.820,"이 Attention Mechanism의 입력 값과,",1756.82,19
29:16.840,29:19.060,"이 출력 값의 Dimension은 같아야 되기 때문에,",1759.06,19
29:19.320,29:22.200,"이렇게 각각의 Head로부터 나오게 된 Attention 값들을,",1762.2,19
29:22.300,29:25.380,"다시 이렇게 Concat을 수행해서 일자로 쭉 붙인 뒤에,",1765.38,19
29:25.560,29:28.040,"마지막으로 이 Linear Layer을 거쳐서,",1768.04,19
29:28.040,29:29.500,Output 값을 내보내게 됩니다.,1769.5,19
29:29.720,29:30.640,"이때 결과적으로,",1770.64,19
29:30.640,29:33.840,"이 입력 값과 출력 값의 Dimension이 같도록 만들어서,",1773.84,19
29:33.940,29:36.120,"이러한 Multi-Head Attention Layer를 사용한 뒤에도,",1776.12,19
29:36.220,29:38.260,Dimension이 줄어들지 않도록 만듭니다.,1778.26,19
29:38.340,29:41.680,"바로 이런 식으로 각각의 Attention Mechanism이 사용되는 것이고요,",1781.68,19
29:41.820,29:43.220,"이러한 Multi-Head Attention은,",1783.22,19
29:43.240,29:46.420,이 전체 아키텍처에서 다 동일한 함수로써 동작합니다.,1786.42,16
29:46.700,29:47.840,"이때 다른 점이라고 한다면,",1787.84,16
29:48.040,29:49.300,"이렇게 사용되는 위치마다,",1789.3,16
29:49.560,29:52.980,"Query랑 Key랑 Value를 어떻게 사용할지가 달라질 수 있는 건데,",1792.98,16
29:53.200,29:54.180,"그런 점을 제외하고,",1794.18,16
29:54.280,29:57.480,기본적인 각각의 Attention Layer의 동작 방식은 같습니다.,1797.48,16
29:57.820,29:58.660,"그래서 예를 들어,",1798.66,16
29:58.740,30:00.620,"이렇게 Encoder, Decoder, Attention에서,",1800.62,16
30:00.620,30:03.280,"Decoder의 출력 단어가 Query가 되는 것이고,",1803.28,16
30:03.400,30:05.460,"각각의 출력 단어를 만들기 위해서,",1805.46,16
30:05.640,30:09.920,"Encoder 파트에서의 어떤 단어를 참고하면 좋은지를 구하기 위해서,",1809.92,16
30:10.100,30:14.180,이 Key랑 Value의 값으로는 Encoder의 출력 값을 쓰겠다는 겁니다.,1814.18,16
30:14.420,30:15.020,"다시 말해,",1815.02,16
30:15.060,30:16.700,"각각의 단어를 출력하기 위해서,",1816.7,16
30:16.920,30:18.840,"어떤 정보를 참고해야 해? 라고,",1818.84,16
30:18.920,30:20.920,"이렇게 Encoder한테 물어보는 것이기 때문에,",1820.92,16
30:21.100,30:23.560,"이 Decoder 파트에 있는 단어가 Query가 되고,",1823.56,16
30:23.760,30:27.740,Encoder 파트에 있는 각각의 값들이 Key와 Value가 된다고 할 수 있습니다.,1827.74,16
30:27.940,30:30.600,"그래서 Multi-Head Attention Layer를 더욱더 자세히,",1830.6,20
30:30.600,30:32.820,자세하게 수식으로 표현하면 다음과 같은데요.,1832.82,20
30:32.840,30:36.260,"자, 이렇게 하나의 Attention은 Query와 Key와 Value를 받고요.",1836.26,20
30:36.380,30:38.260,"이때, Query랑 Key랑 곱해서,",1838.26,20
30:38.380,30:39.400,"각 Query에 대해서,",1839.4,20
30:39.520,30:42.100,각각의 Key에 대한 에너지 값을 구할 수 있겠죠.,1842.1,20
30:42.280,30:43.620,"이제 그런 에너지 값에 대해서,",1843.62,20
30:43.760,30:45.460,"확률 값으로 표현하도록 만들어서,",1845.46,20
30:45.620,30:49.460,실제로 어떤 Key에 대해서 높은 가중치를 가지는지 계산할 수가 있고요.,1849.46,20
30:49.680,30:51.620,"이때 이렇게 Scale Factor로서,",1851.62,20
30:51.680,30:53.140,Root Decay를 사용합니다.,1853.14,20
30:53.360,30:56.020,이때 Decay는 각각의 Key Dimension이 되겠고요.,1856.02,20
30:56.200,30:58.280,"이렇게 특정한 Scale로 나눠주는 이유는,",1858.28,20
30:58.380,31:00.200,"이 Softmax 함수 자체가 가지는,",1860.2,20
31:00.200,31:01.140,"특성을 생각해 보시면,",1861.14,20
31:01.380,31:02.800,"0 근처의 위치에서는,",1862.8,20
31:02.880,31:05.220,"Gradient가 높게 형성되는 것에 반해,",1865.22,20
31:05.280,31:08.520,"값이 들쭉날쭉 조금씩 왼쪽 오른쪽으로 이동하게 되면,",1868.52,20
31:08.760,31:10.340,"교육의 값이 많이 줄어들기 때문에,",1870.34,20
31:10.580,31:13.100,"Gradient Vanishing 문제를 피하기 위한 방법으로,",1873.1,20
31:13.180,31:16.340,이러한 Scale Factor를 넣어줄 수 있다고 논문에선 말하고 있습니다.,1876.34,20
31:16.640,31:17.520,"결과적으로 이렇게,",1877.52,20
31:17.700,31:19.860,"각각의 Query가 각각의 Key에 대해서,",1879.86,20
31:20.060,31:21.860,"어떠한 가중치를 가지는지,",1881.86,20
31:21.900,31:23.240,"Score 값을 구한 뒤에,",1883.24,20
31:23.400,31:25.480,"이제 걔를 실제로 Value 값과 곱해서,",1885.48,20
31:25.740,31:27.840,Attention Value를 만들어낼 수가 있는 것입니다.,1887.84,20
31:28.180,31:29.260,"이때 말씀드렸듯이,",1889.26,20
31:29.260,31:31.220,"입력으로 들어오는 각각의 값에 대해서,",1891.22,20
31:31.380,31:34.260,"서로 다른 Linear Layer를 거치도록 만들어서,",1894.26,20
31:34.420,31:38.960,A측에 서로 다른 각각 Query Key Value 값을 만들 수 있도록 하는 것입니다.,1898.96,20
31:39.440,31:39.880,"이제 이런 식으로,",1899.88,20
31:40.120,31:42.220,"A측에 서로 다른 컨셉을,",1902.22,20
31:42.240,31:45.120,"네트워크가 구분해서 학습하도록 만들므로써,",1905.12,20
31:45.280,31:48.520,Attention을 수행하기 위한 다양한 Feature들을 학습하도록 만듭니다.,1908.52,20
31:48.640,31:51.720,"실제로 나중에 우리가 Attention Score를 시각화해 볼 때는,",1911.72,20
31:51.920,31:53.360,"이 H의 개수만큼,",1913.36,20
31:53.400,31:55.460,Attention Score의 그림이 나오게 됩니다.,1915.46,20
31:55.780,31:56.760,"이제 결과적으로 이렇게,",1916.76,20
31:56.980,31:59.080,"각 Head에 대한 출력 값들을 구할 수 있고,",1919.08,20
31:59.080,32:01.140,"이제 이것을 일자로 쭉 붙인 다음에,",1921.14,20
32:01.340,32:02.100,"마지막으로,",1922.1,20
32:02.100,32:03.540,"아웃풋 매트릭스랑 곱해서,",1923.54,20
32:03.720,32:07.320,결과적인 이 Multi-Headed Attention의 값을 구해낼 수가 있는 것입니다.,1927.32,20
32:07.540,32:09.700,"이제 이런 식으로 매번 입력 값이 들어왔을 때,",1929.7,20
32:09.780,32:11.060,"기본적으로는 이런 식으로,",1931.06,20
32:11.220,32:13.940,"Value와 Key와 Query의 값으로 각각 들어가게 되고,",1933.94,20
32:14.140,32:15.040,"이렇게 나올 때는,",1935.04,20
32:15.160,32:18.460,"입력으로 들어왔던 값과 동일한 Dimension을 가지기 때문에,",1938.46,20
32:18.680,32:20.940,"이러한 Multi-Headed Attention Layer가 포함된,",1940.94,20
32:21.040,32:24.980,하나의 Encoder 혹은 Decoder Layer는 중첩해서 사용될 수 있는 것입니다.,1944.98,20
32:25.240,32:28.500,이제 한번 자세하게 이 Transformer의 동작 원리를 알아보겠습니다.,1948.5,21
32:29.080,32:30.160,"지금은 그냥 간단하게,",1950.16,21
32:30.200,32:32.000,하나의 단어만 있다고 가정을 해볼게요.,1952.0,21
32:32.220,32:32.680,"이때,",1952.68,21
32:32.800,32:33.760,"Attention을 위해서,",1953.76,21
32:34.000,32:37.680,각각의 Head마다 Query와 Key Value 값을 만들 필요가 있습니다.,1957.68,21
32:38.040,32:38.860,"이제 그래서 이렇게,",1958.86,21
32:39.020,32:42.760,"하나의 단어가 Embedding 차원으로 표현되고 있는 상태에서,",1962.76,21
32:42.960,32:45.340,"이제 여기에서 Linear Layer를 거쳐서,",1965.34,21
32:45.360,32:48.140,각각 Query랑 Key랑 Value 값을 만들 수 있습니다.,1968.14,21
32:48.480,32:51.220,"이때 Embedding 차원을 Dim Model이라고 부를 수 있고요,",1971.22,21
32:51.520,32:55.740,"원본 논문에서는 Embedding 차원을 512 차원으로 사용한다고 언급을 했고요,",1975.74,21
32:55.900,32:58.040,"이때 만약에 Head의 개수가 8개라고 하면,",1978.04,21
32:58.240,32:59.040,"512를,",1979.04,21
32:59.040,33:03.720,"8로 나눈 64만큼 각각의 Query, Key Value의 차원이 구성되는 것입니다.",1983.72,21
33:03.960,33:07.200,"여기 보이는 그림은 그냥 간단하게 Embedding 차원이 4차원이고,",1987.2,21
33:07.280,33:09.020,Head가 2개라고 가정한 상황입니다.,1989.02,21
33:09.220,33:10.000,"즉 이제 이럴 때는,",1990.0,21
33:10.080,33:12.800,4 곱하기 2짜리 매트릭스가 만들어지겠죠.,1992.8,21
33:12.980,33:16.600,"왜냐면은 이 4차원의 데이터를 2차원의 데이터로 맵핑해야 되기 때문에,",1996.6,21
33:16.720,33:19.460,이렇게 4 곱하기 2 가중치 매트릭스가 사용되는 것입니다.,1999.46,21
33:19.640,33:23.620,"그래서 이런 식으로 Love라는 단어가 4차원으로 표현되어 있다고 하면,",2003.62,21
33:23.900,33:27.620,"이제 이것은 Query, Key, Value 각각 2차원으로 구성되어 있는,",2007.62,21
33:27.700,33:29.020,"데이터로 표현될 수 있는,",2009.02,21
33:29.040,33:29.440,것입니다.,2009.44,21
33:29.680,33:32.740,"이제 이런 식으로 Key랑 Query랑 Value를 다 구했다고 치면,",2012.74,22
33:32.820,33:36.540,"바로 다음의 공식으로 이용해서 실제로 Attention Value를 구할 수가 있는데요,",2016.54,22
33:36.840,33:39.180,"이때 이 Query는 각각의 다른 단어들,",2019.18,22
33:39.240,33:41.380,"이 Key와 행렬급을 수행해서,",2021.38,22
33:41.500,33:44.240,이렇게 하나의 Attention Energy 값을 구할 수가 있는 겁니다.,2024.24,22
33:44.440,33:44.920,"예를 들어,",2024.92,22
33:45.080,33:47.700,"I love you라고 하나의 문장이 들어왔다고 하면,",2027.7,22
33:47.980,33:53.200,"이 I라는 단어는 I에 해당하는 Key, Love에 해당하는 Key, You에 해당하는 Key 값과,",2033.2,22
33:53.220,33:54.400,"각각 곱해져서,",2034.4,22
33:54.500,33:57.120,"하나의 Attention Energy 값을 구할 수가 있는 거고요,",2037.12,22
33:57.300,33:58.740,"이제 아까 전에 말씀드렸듯이,",2038.74,22
33:59.040,34:00.980,"Softmax에 들어가는 값의 크기를,",2040.98,22
34:01.040,34:02.520,"노멀라이제이션 해주기 위해서,",2042.52,22
34:02.660,34:04.700,각각 스케일링 팩터로 나누어 줍니다.,2044.7,22
34:05.020,34:06.620,"이제 이후에 Softmax를 취해서,",2046.62,22
34:06.780,34:08.640,"실제로 각각의 Key 값에 대해서,",2048.64,22
34:08.980,34:12.080,어떠한 가중치를 가지는지를 구해낼 수가 있는 것입니다.,2052.08,22
34:12.460,34:13.380,"여기에 보이는 그림에서는,",2053.38,22
34:13.640,34:19.100,"이 I라는 단어는 I라는 단어와 72%만큼의 높은 연관성을 가지고,",2059.1,22
34:19.420,34:21.400,"이 Love라는 단어와는 15%,",2061.4,22
34:21.400,34:23.340,"그 다음에 You라는 단어와는 13%,",2063.34,22
34:23.340,34:26.720,"이렇게 각각의 가중치를 가진다고 표현할 수가 있는 거고요,",2066.72,22
34:26.920,34:28.540,"이렇게 각각의 가중치 값에다가,",2068.54,22
34:28.540,34:30.620,"이 Value 값들을 각각 곱한 뒤에,",2070.62,22
34:30.700,34:31.980,"전부 더해줘서,",2071.98,22
34:32.040,34:34.960,결과적인 Attention Value 값을 만들어낼 수가 있는 것입니다.,2074.96,22
34:35.140,34:38.060,"즉, 마찬가지로 Weighted Sum을 구할 수가 있다는 거고요,",2078.06,22
34:38.240,34:41.220,바로 이러한 과정을 통해서 실제로 Attention이 수행되는 것입니다.,2081.22,22
34:41.440,34:44.780,"그래서 한번 실제로 전체 문장이 한꺼번에 입력되는,",2084.78,23
34:44.880,34:47.720,이런 행렬과 같은 상황에서 예시를 다시 한번 확인해 보겠습니다.,2087.72,23
34:48.120,34:50.400,"실제로는 이런 식으로 행렬 곡셈 연산을 이용해서,",2090.4,23
34:50.480,34:52.060,"한꺼번에 연산이 가능하고요,",2092.06,23
34:52.180,34:54.020,"I Love You라는 하나의 문장이 있고,",2094.02,23
34:54.260,34:54.640,"그 다음에,",2094.64,23
34:54.820,34:56.780,"Embedding 차원이 4차원이라고 했을 때,",2096.78,23
34:56.840,34:58.520,"바로 이렇게 3x4짜리 메타로드,",2098.52,23
34:58.540,34:59.480,"이 메타로드가 구성되는데요,",2099.48,23
34:59.600,35:01.620,"이때 마찬가지로 하나의 헤드에 있는,",2101.62,23
35:01.700,35:05.240,이 Query Key Value를 구하기 위한 가중치 값이 이렇게 있다고 해볼게요.,2105.24,23
35:05.400,35:07.080,"현재 헤드에서는 바로 이런 식으로,",2107.08,23
35:07.240,35:09.680,"I Love You에 대한 각각의 Query 값,",2109.68,23
35:09.680,35:10.280,"Key 값,",2110.28,23
35:10.280,35:11.920,Value 값이 만들어지는 것입니다.,2111.92,23
35:12.280,35:15.400,"마찬가지로 이렇게 Query와 Key와 Value의 값이 구해졌기 때문에,",2115.4,24
35:15.540,35:17.580,"Attention Value를 구할 수 있게 되는 건데요,",2117.58,24
35:17.600,35:19.300,"이렇게 I와 Love와 You,",2119.3,24
35:19.420,35:23.000,"Query 값들을 한꺼번에 이렇게 각 Key 값과 곱해줘서,",2123.0,24
35:23.740,35:26.400,Attention Energy를 이렇게 3x3으로 만들어낼 수 있습니다.,2126.4,24
35:26.620,35:28.140,"이때 Attention Energy 값은,",2128.14,24
35:28.140,35:31.220,"말 그대로 각각의 단어가 각각의 Key 값에 대해서,",2131.22,24
35:31.400,35:35.500,얼마나 높은 그 연관성을 표현하는 수치를 부여했는지를 구할 수가 있는 겁니다.,2135.5,24
35:35.700,35:38.240,"즉 이런 식으로 Attention Energy 값은,",2138.24,24
35:38.240,35:40.900,"I Love You 각각에 대해서 구해지는 방식으로,",2140.9,24
35:40.980,35:44.740,이렇게 행과 열은 모두 단어의 개수와 동일한 크기를 가집니다.,2144.74,24
35:44.860,35:49.220,"각각의 단어가 서로에게 어떠한 연관성을 가지는지 구할 수가 있는 것이고요,",2149.22,24
35:49.420,35:50.920,"이제 여기에 Softness를 취해서,",2150.92,24
35:51.080,35:55.720,"각각의 행마다 각 Key에 대한 값들을 확률 값으로 구해낼 수 있도록 만드는 거고요,",2155.72,24
35:55.880,35:57.680,"이제 그러한 가중치 값들과,",2157.68,24
35:57.680,35:59.020,"Value 값을 곱해주어서,",2159.02,24
35:59.140,36:01.940,실제 Attention Value Matrix를 구할 수 있습니다.,2161.94,24
36:02.160,36:04.540,"보시면 이제 이렇게 Attention Value 값 자체는,",2164.54,24
36:04.580,36:08.800,입력되었던 Query와 Key와 Value와 모두 동일한 차원을 가집니다.,2168.8,24
36:09.020,36:10.840,"또한 한 가지 알아두시면 좋은 점은,",2170.84,25
36:10.900,36:12.660,"Mask 행렬을 사용할 수 있다는 점인데요,",2172.66,25
36:12.940,36:14.840,"이 Mask 행렬, 즉 Mask Matrix는,",2174.84,25
36:14.900,36:17.940,특정한 단어를 무시할 수 있도록 하기 위해 사용할 수 있습니다.,2177.94,25
36:18.200,36:19.980,"이렇게 Attention Energy 값이 있을 때에,",2179.98,25
36:20.060,36:23.520,"Attention Energy와 같은 차원의 Mask Matrix를 만들어서,",2183.52,25
36:23.780,36:27.100,"이제 얘를 Element-wise로, 즉 각각의 원소 단위로 곱해주어서,",2187.1,25
36:27.100,36:30.120,어떠한 단어는 참고하지 않도록 만들 수가 있는 것입니다.,2190.12,25
36:30.300,36:30.980,"예를 들어서 이렇게,",2190.98,25
36:31.120,36:34.940,"이 i라는 단어는 이 Love와 U에 해당하는 Key 값은 무시하도록,",2194.94,25
36:34.960,36:38.400,"즉 이 Love와 U는 그냥 Attention 하지 않도록 무시하고자 한다면,",2198.4,25
36:38.540,36:42.560,"이렇게 Attention Energy 값을 전부 다 Minus 무한이라고 할 수 있는,",2202.56,25
36:42.660,36:45.200,"가능한 최대로 작은 값을 넣어주게 되면,",2205.2,25
36:45.380,36:48.760,"실제로 Softmax를 취해서 Attention Score 값이 구해졌을 때,",2208.76,25
36:48.840,36:51.760,"고려하지 않도록 처리가 된 그런 단어들에 대해서는,",2211.76,25
36:51.780,36:54.340,모두 0%의 가중치 값을 가지게 됩니다.,2214.34,25
36:54.540,36:56.840,"즉 Mask Matrix를 그냥 씌워줌으로써,",2216.84,25
36:56.880,36:57.080,"등장하는,",2217.08,25
36:57.080,37:00.740,특정한 단어는 무시해서 Attention을 수행하지 않도록 만들 수가 있는 것입니다.,2220.74,25
37:00.920,37:03.040,"이와 같이 Mask Matrix를 이용해서,",2223.04,25
37:03.100,37:06.000,"이러한 Attention Energy 값에 Mask를 적용함으로써,",2226.0,25
37:06.080,37:10.340,특정 단어는 그냥 무시해서 Attention을 수행하지 않도록 만들 수가 있는 것입니다.,2230.34,25
37:10.600,37:13.940,"그래서 결과적으로 이렇게 각각의 Head마다 입력으로 들어온,",2233.94,26
37:13.940,37:17.320,"Query와 Key와 Value와 같은 차원의 Vector를 만들어 내기 때문에,",2237.32,26
37:17.500,37:21.960,"이렇게 각 Head마다 Query와 Key와 Value의 값들을 각각 넣어서,",2241.96,26
37:22.060,37:26.680,"Attention을 수행한 값들을 이렇게 다 Head1부터 HeadH까지라고 했을 때,",2246.68,26
37:26.680,37:29.260,"이러한 정보들을 다 일자로 쭉 연결하게 되면,",2249.26,26
37:29.360,37:34.440,다시 맨 처음에 입력이 되었던 이런 입력 Dimension과 같은 Dimension을 가지게 되는데요.,2254.44,26
37:34.680,37:39.280,"다시 말해 이런 식으로 Multi-Head Attention은 각각의 Head에 대해서 Attention을 수행한 뒤에,",2259.28,27
37:39.380,37:41.620,"그러한 결과를 다시 쭉 이어붙이기 때문에,",2261.62,27
37:41.680,37:47.320,결과적으로 만들어진 Matrix의 이 열의 개수는 원래 입력의 Embedding 차원과 동일한 값을 가집니다.,2267.32,27
37:47.460,37:50.560,"그렇기 때문에 이제 마지막에 이 W가중치 값으로,",2270.56,27
37:50.640,37:54.680,"Dmodel 곱하기 Dmodel 차원을 가지는 Matrix를 곱해 줌으로써,",2274.68,27
37:54.760,37:56.660,"결과적인 Multi-Head Attention의 값을,",2276.66,27
37:56.660,37:57.180,구할 수 있고요.,2277.18,27
37:57.420,38:01.200,"이제 이렇게 하더라도 결과 값은 입력 Dimension과 정확히 동일하기 때문에,",2281.2,27
38:01.280,38:05.760,이러한 Multi-Head Attention을 수행한 뒤에도 차원이 동일하게 유지가 된다는 점이 특징입니다.,2285.76,27
38:05.940,38:10.460,또한 앞서 간단하게 말씀드렸듯이 이 Transformer에는 세 가지 종류의 Attention이 사용되는데요.,2290.46,28
38:10.740,38:13.660,"Transformer에 쓰이는 Attention은 항상 Multi-Head Attention으로,",2293.66,28
38:13.720,38:15.780,"Head가 여러 개인 Attention이라 볼 수 있는데,",2295.78,28
38:16.000,38:18.240,"이제 그러한 Attention이 사용되는 위치에 따라서,",2298.24,28
38:18.300,38:21.700,"Encoder Self-Attention, 그리고 Masked Decoder Self-Attention,",2301.7,28
38:22.060,38:24.460,Encoder Decoder Attention 이 세 가지 종류가 존재합니다.,2304.46,28
38:24.660,38:26.640,기본적으로 Encoder에 Self-Attention을 사용하는 것입니다.,2306.64,28
38:26.660,38:30.620,"저는 말씀드렸듯이 각각의 단어가 서로에게 어떠한 연관성을 가지는지를,",2310.62,28
38:30.660,38:32.520,"Attention을 통해서 구하도록 만들고,",2312.52,28
38:32.580,38:36.960,전체 문장에 대한 Representation을 Learning할 수 있도록 만든다는 점이 특징이고요.,2316.96,28
38:37.100,38:39.640,"다만 이제 Decoder 파트에서 Self-Attention을 수행할 때는,",2319.64,28
38:39.780,38:44.280,"이렇게 각각의 출력 단어가 다른 모든 출력 단어를 전부 참고하도록 만들진 않고,",2324.28,28
38:44.460,38:47.520,앞쪽에 등장했던 단어들만 참고할 수 있도록 만듭니다.,2327.52,28
38:47.560,38:48.740,"예를 들어 출력 문장이,",2328.74,28
38:48.860,38:51.240,"나는 축구를 했다 라고 하면은,",2331.24,28
38:51.240,38:53.860,"우리가 축구를 이라는 단어를 출력할 때 있어서,",2333.86,28
38:54.080,38:56.220,"했다 라고 이렇게 뒤쪽에 나오는 단어가,",2336.22,28
38:56.220,38:58.460,"무엇인지 참고할 수 있도록 만들어버리면,",2338.46,28
38:58.560,39:01.060,"그것은 이제 일종의 치팅처럼 동작을 하기 때문에,",2341.06,28
39:01.120,39:03.740,모델이 정상적으로 학습이 되기가 어렵습니다.,2343.74,28
39:03.880,39:06.520,"그렇기 때문에 이 Decoder 파트에서 Attention을 수행할 때는,",2346.52,28
39:06.660,39:10.880,"이렇게 각각의 단어에 대해서 이 앞쪽 단어들만 참고할 수 있도록 만들므로써,",2350.88,28
39:10.940,39:14.780,치팅을 하지 않고 정상적으로 모델이 학습될 수 있도록 만드는 것입니다.,2354.78,28
39:14.980,39:16.900,"마지막으로 Encoder Decoder Attention은,",2356.9,28
39:16.920,39:18.220,"Query가 Decoder에 있고,",2358.22,28
39:18.360,39:21.280,각각의 Key와 Value는 Encoder에 있는 상황을 의미하는 것입니다.,2361.28,28
39:21.520,39:22.000,"예를 들어,",2362.0,28
39:22.180,39:23.580,"난 널 좋아해 라고,",2363.58,28
39:23.700,39:26.040,"I like you 라고 문장이 들어왔을 때,",2366.04,28
39:26.040,39:28.740,"출력 문장이 난 널 좋아해 라고 나온다고 하면,",2368.74,28
39:28.920,39:32.040,"각각의 출력 단어들이 이러한 입력 단어들 중에서,",2372.04,28
39:32.160,39:36.220,어떤 정보에 더욱 더 많은 가중치를 두는지를 구할 수 있어야 되는데요.,2376.22,28
39:36.380,39:39.160,"이제 그러한 과정에서 이 Decoder 파트에 있는 Query 값이,",2379.16,28
39:39.160,39:42.540,"이렇게 Encoder 파트에 있는 Key와 Value 값을 참조한다고 해서,",2382.54,28
39:42.720,39:44.840,Encoder Decoder Attention이라고 부르는 것입니다.,2384.84,28
39:45.040,39:47.740,또한 이어서 Self-Attention에 대해서 알아볼 건데요.,2387.74,29
39:47.820,39:49.620,"실제로 이러한 Self-Attention은,",2389.62,29
39:49.640,39:52.740,말씀드렸듯이 Encoder와 Decoder 모두에서 사용되고요.,2392.74,29
39:52.920,39:56.020,"시각화 과정을 통해서 Attention Score로 나온 값을 그려보는,",2396.02,29
39:56.040,39:56.480,어떤 단어를 구할 수 있는지 알아볼 수 있습니다.,2396.48,29
39:56.580,39:58.600,"매번 입력 문장에서 각 단어가,",2398.6,29
39:58.680,40:01.720,다른 어떤 단어와 연관성이 높은지를 구할 수가 있는 건데요.,2401.72,29
40:01.740,40:04.220,"예를 들어 이런 식으로 하나의 입력 문장이 들어왔을 때,",2404.22,29
40:04.300,40:06.900,"각각의 단어들은 다른 모든 단어에 대해서,",2406.9,29
40:07.060,40:09.180,Attention Score 값을 구할 수가 있는 겁니다.,2409.18,29
40:09.380,40:10.080,"예를 들어 이렇게,",2410.08,29
40:10.360,40:12.580,A boy who is looking at the tree is surprised.,2412.58,29
40:12.940,40:14.660,"이런 식으로 문장이 있다고 했을 때,",2414.66,29
40:14.800,40:17.460,"각각의 단어들은 다른 단어 모두에 대해서,",2417.46,29
40:17.680,40:21.700,얼마나 가중치를 부여할지를 Attention을 통해서 계산할 수가 있는 건데요.,2421.7,29
40:21.740,40:24.040,"예를 들어 이렇게 It이란 단어를 출력한다고 하면,",2424.04,29
40:24.040,40:25.720,"이러한 It이 의미하는 단어는,",2425.72,29
40:25.720,40:28.320,앞쪽에 있는 Tree와 이렇게 동일한 It이 되겠죠.,2428.32,29
40:28.460,40:32.440,"그렇기 때문에 실제로 Attention Score를 시각적으로 출력하도록 만들면,",2432.44,29
40:32.520,40:34.380,"이런 식으로 Tree와 It과 관련해서,",2434.38,29
40:34.460,40:37.980,더 높은 Score를 가지는 방식으로 학습이 될 가능성이 높습니다.,2437.98,29
40:38.120,40:42.140,"그래서 이런 식으로 각각의 단어들이 서로 어떠한 연관성을 가지는지를,",2442.14,29
40:42.220,40:45.040,Self-Attention 과정을 통해서 시각화해 볼 수가 있습니다.,2445.04,29
40:45.360,40:47.780,"자, 이제 결과적으로 우리가 앞에서 확인했던 이,",2447.78,19
40:47.880,40:51.700,Transformer의 전체 아키텍처에 포함되어 있는 내용들을 하나씩 확인해 보았는데요.,2451.7,19
40:51.900,40:54.020,"이렇게 Encoder 파트에선 입력 값이 들어오고,",2454.02,19
40:54.020,40:58.420,위치에 대한 정보를 반영해 준 입력을 실제로 첫 번째 레이어에 넣어주게 되고요.,2458.42,19
40:58.520,41:02.540,"이제 이렇게 Encoder 레이어는 N번 만큼 반복이 되어서 중첩해 사용이 되고,",2462.54,19
41:02.680,41:05.780,"이제 그렇게 나온 마지막 레이어의 Encoder의 출력 값이,",2465.78,19
41:05.820,41:08.200,각각의 Decoder 레이어에 들어간다고 보시면 됩니다.,2468.2,19
41:08.500,41:12.000,"이제 그래서 마찬가지로 Decoder 레이어도 N번만큼 중첩이 되어서,",2472.0,19
41:12.160,41:16.480,"가장 마지막에 나온 그 출력 값에 Linear 레이어와 Softmax를 취해서,",2476.48,19
41:16.580,41:18.920,각각의 출력 단어를 만들어 낼 수가 있는 것입니다.,2478.92,19
41:19.140,41:21.340,"다만 이제 우리가 한 가지 얘기 안 한 게 있다고 하면,",2481.34,19
41:21.500,41:23.640,"바로 위치 정보를 어떤 식으로 넣을지에 대한,",2483.64,19
41:24.020,41:24.620,Encoding 함수입니다.,2484.62,19
41:24.800,41:28.700,"원본 논문에서는 하나의 문장에 포함되어 있는 각각의 단어들에 대한,",2488.7,30
41:28.800,41:31.960,"상대적인 위치에 대한 정보를 모델에게 알려주기 위해서,",2491.96,30
41:32.100,41:34.240,바로 주기 함수를 활용한 공식을 사용하는데요.,2494.24,30
41:34.420,41:36.020,실제 공식은 바로 다음과 같습니다.,2496.02,30
41:36.460,41:39.060,이때 p는 포지셔널 인코딩의 약자고요.,2499.06,30
41:39.220,41:41.780,이때 이 for는 각각의 단어 번호가 되겠고요.,2501.78,30
41:41.940,41:46.060,이때 이 i는 각각의 단어에 대한 Embedded 값의 위치 하나하나를 의미합니다.,2506.06,30
41:46.320,41:49.860,"이제 그래서 이런 식으로 Sine 함수와 같은 주기 함수 값을,",2509.86,30
41:49.880,41:51.640,인코딩을 위해서 사용하는데요.,2511.64,30
41:51.860,41:53.340,"이렇게 파라미터로 들어와 있는,",2513.34,30
41:53.340,41:56.040,"만과 같은 값이나 이런 Sine과 Cosine 함수는,",2516.04,30
41:56.120,41:58.380,"이렇게 기본적인 Sine 함수와 Cosine 함수 말고,",2518.38,30
41:58.480,42:00.440,다른 주기 함수를 사용할 수도 있는 거고요.,2520.44,30
42:00.560,42:03.920,"아무튼 우리 네트워크가 각각의 입력 문장에 포함되어 있는,",2523.92,30
42:04.040,42:07.300,"각 단어들의 상대적인 위치에 대한 정보를 알 수 있도록,",2527.3,30
42:07.420,42:10.020,"이러한 주기성을 학습할 수 있도록 만들기만 한다면,",2530.02,30
42:10.160,42:11.820,어떤 함수가 들어와도 사용할 수 있습니다.,2531.82,30
42:12.020,42:15.740,"그래서 원본 논문에서도 이렇게 Sine 함수와 Cosine 함수를 이용해서,",2535.74,30
42:15.880,42:18.040,"정해진 그런 함수 값을 사용할 수도 있지만,",2538.04,30
42:18.320,42:21.300,"우리가 위치에 대한 Embedded 값을 따로 학습하도록 만들어서,",2541.3,30
42:21.380,42:23.320,네트워크에 넣을 수 있다고 말하고 있고요.,2543.32,30
42:23.340,42:24.800,"실제로 그렇게 넣었을 때도,",2544.8,30
42:24.900,42:27.200,"이렇게 Sine 함수와 Cosine 함수를 이용했을 때와,",2547.2,30
42:27.300,42:29.920,실제 성능상의 차이는 거의 없었다고 말하고 있습니다.,2549.92,30
42:30.160,42:33.420,"그래서 실제로 트랜스포머 이후에 나온 다양한 아키텍처에서는,",2553.42,30
42:33.580,42:35.140,"이러한 주기 함수를 사용하지 않고,",2555.14,30
42:35.360,42:39.360,그냥 학습이 가능한 형태로 별도의 Embedding 레이어를 사용하기도 합니다.,2559.36,30
42:39.660,42:42.600,"더욱 더 자세하게 실제로 이러한 위치 인코딩이,",2562.6,31
42:42.620,42:45.140,어떤 식으로 들어갈 수 있는지를 확인해 보시면요.,2565.14,31
42:45.200,42:48.560,예를 들어 이렇게 입력 문장이 WeRD1이라고 한번 해볼게요.,2568.56,31
42:48.840,42:52.680,이때 각각의 단어들은 딥 마더 만큼의 Embedding 차원을 가지게 됩니다.,2572.68,31
42:52.900,42:53.320,"지금 도움드리지만,",2573.32,31
42:53.340,42:55.120,이 그림에서는 이 Embedding 차원이 8이 되겠죠.,2575.12,31
42:55.300,42:58.860,"이 Sine과 Cosine 함수에 들어가는 이 포스 값과 I 값은,",2578.86,31
42:58.920,43:03.180,이러한 입력 행렬 값에서의 각각의 인덱스 값과 동일하게 들어가는 것입니다.,2583.18,31
43:03.380,43:05.080,"예를 들어 여기는 0,3이 되는데요.",2585.08,31
43:05.280,43:07.920,첫 번째 단어의 네 번째 Embedding이기 때문이죠.,2587.92,31
43:08.100,43:11.100,"그래서 이제 각각의 값들이 이러한 함수에 들어가게 돼서,",2591.1,31
43:11.180,43:14.180,"바로 입력 값과 정확히 동일한 디멘전을 가지는,",2594.18,31
43:14.200,43:15.880,위치 인코딩을 만들어낼 수 있습니다.,2595.88,31
43:16.100,43:19.200,"그래서 이제 이 값을 Element-wise로 다 더해줘서,",2599.2,31
43:19.300,43:21.240,"원소 by 원소로 다 더해준 뒤에,",2601.24,31
43:21.280,43:22.380,"그 값을 실제로,",2602.38,31
43:22.380,43:26.720,각 인코더와 디코더 레이어의 입력 값으로 사용을 한다고 보시면 되겠습니다.,2606.72,31
43:26.960,43:30.200,"여기 보이는 코드는 그냥 간단하게 N과 디멘전에 대해서,",2610.2,32
43:30.460,43:34.740,"실제로 어떤 식으로 각 단어의 위치에 대한 인코딩 정보가 들어가는지를,",2614.74,32
43:34.820,43:36.060,그림으로 표현한 것인데요.,2616.06,32
43:36.260,43:40.100,바로 이렇게 간단하게 맷플롤 라이브러리를 이용해서 그림을 그려볼 수 있습니다.,2620.1,32
43:40.420,43:45.160,"자, 마찬가지로 전체 실습 코드는 제 GitHub 저장소에 올려놓았으니까요.",2625.16,32
43:45.240,43:46.160,확인하실 수 있습니다.,2626.16,32
43:46.460,43:47.780,"이렇게 아래쪽에 내려와 보시면,",2627.78,32
43:48.100,43:49.620,Attention is all you need.,2629.62,32
43:49.780,43:51.240,Code Practice 보이시죠?,2631.24,32
43:51.320,43:52.340,"여기 들어오셔서,",2632.34,32
43:52.340,43:54.440,전체 코드를 확인해 보실 수 있습니다.,2634.44,32
43:54.560,43:57.260,"전체 코드는 여러분들의 개인 개발 환경이 아닌,",2637.26,32
43:57.300,43:59.560,"무료 딥러닝 개발 환경인 CoreApp에서,",2639.56,32
43:59.620,44:01.840,바로 실행해 볼 수 있도록 준비를 해놓았습니다.,2641.84,32
44:02.120,44:04.260,"그래서 여러분들은 구글 아이디만 있으시면,",2644.26,32
44:04.420,44:05.600,"바로 여기 링크 들어오셔서,",2645.6,32
44:05.800,44:08.240,CoreApp에서 즉시 실행해 보실 수 있습니다.,2648.24,32
44:08.520,44:11.100,다음과 같이 전체 코드를 확인해 볼 수 있는데요.,2651.1,28
44:11.400,44:12.480,"내용을 확인해 보시면,",2652.48,28
44:12.700,44:13.960,"기본적으로 본 코드는,",2653.96,28
44:14.100,44:17.660,트랜스포머의 논문 내용을 최대한 따르면서 구현된 코드입니다.,2657.66,28
44:17.880,44:22.320,실제로 트랜스포머 논문은 딥러닝 기반의 자연 처리 기법의 대표적인 코드입니다.,2662.32,28
44:22.340,44:25.220,이 코드는 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다.,2665.22,28
44:25.360,44:28.140,"그래서 최근 가장 뛰어난 번역 모델들은,",2668.14,28
44:28.200,44:31.700,이러한 트랜스포머 기반의 아키텍처를 따르는 경우가 많습니다.,2671.7,28
44:31.940,44:33.160,"코드를 실행하시기 전에,",2673.16,28
44:34.180,44:36.360,"런타임에서 런타임 유형 변경에 들어오신 뒤에,",2676.36,21
44:37.520,44:39.340,GPU로 설정이 되어 있는지 확인해 주세요.,2679.34,19
44:39.620,44:43.680,또한 본 코드에서는 독일어를 영어로 번역을 수행해 볼 건데요.,2683.68,21
44:43.720,44:47.200,"이제 번역된 영어 문장의 성능을 평가하기 위한 척도로,",2687.2,21
44:47.240,44:49.000,Blue Score를 사용할 예정입니다.,2689.0,21
44:49.160,44:51.400,"이 Blue Score는 Ngram 기반으로,",2691.4,21
44:51.400,44:55.060,"번역한 문장이 실제 정답 문장들과 비교했을 때,",2695.06,21
44:55.100,44:58.300,얼마나 유사한지를 평가해주는 평가 척도 중 하나입니다.,2698.3,21
44:58.460,45:00.480,"그래서 이걸 사용하기 위해서,",2700.48,21
45:00.580,45:04.520,Torch Text를 특정 버전으로 설치할 수 있도록 만들어 주겠습니다.,2704.52,20
45:04.820,45:07.820,이제 이어서 데이터의 전처리를 진행하게 될 건데요.,2707.82,15
45:07.840,45:09.480,문장의 토크날을 진행할 겁니다.,2709.48,15
45:09.960,45:13.600,"이때 우리는 독일어를 영어로 번역하는 테스크를 진행할 것이기 때문에,",2713.6,15
45:13.780,45:17.420,이렇게 영어와 독일어에 대해서 전처리 모듈을 설치할 수 있도록 하겠습니다.,2717.42,15
45:17.740,45:19.980,"그래서 각각 영어와 독일어에 대해서,",2719.98,15
45:20.080,45:21.380,"어떠한 문장이 있을지,",2721.38,15
45:21.380,45:23.760,"그런 문장을 토큰으로 바꿔주기 위해서,",2723.76,15
45:23.900,45:28.940,토큰으로 바꿔줄 수 있는 각각의 스페이시 라이브러리 객체를 선언해주고요.,2728.94,15
45:29.080,45:30.340,"한번 간단하게,",2730.34,15
45:30.380,45:33.640,"I am a graduate student라는 내용의 문장이 있을 때,",2733.64,15
45:33.700,45:36.360,한번 이걸 토큰으로 각각 바꾸어서 출력을 해보겠습니다.,2736.36,15
45:36.880,45:39.220,"그럼 이런 식으로 영어 문장이 정상적으로,",2739.22,15
45:39.340,45:43.920,I am a graduate student라고 잘 토크나이즈가 된 걸 확인할 수 있습니다.,2743.92,15
45:44.180,45:47.040,"이제 우리는 실제로 다수의 문장을 불러와서,",2747.04,15
45:47.080,45:50.040,"각 문장마다 전부 다 이러한 토크날을 진행해서,",2750.04,15
45:50.200,45:51.360,"우리 네트워크에 있는,",2751.36,15
45:51.380,45:52.980,번역 값으로 넣을 수 있도록 만들 겁니다.,2752.98,15
45:53.200,45:55.660,"그렇기 때문에 독일어와 영어 각각에 대해서,",2755.66,15
45:55.820,45:57.320,"어떠한 문장이 들어왔을 때,",2757.32,15
45:57.380,46:00.680,"토크날을 수행한 결과를 다시 리스트 형태로 발환할 수 있도록,",2760.68,15
46:00.720,46:01.560,함수를 정의합니다.,2761.56,15
46:01.980,46:04.460,"이렇게 마찬가지로 함수를 정의해 주시고,",2764.46,15
46:04.680,46:06.740,"또 추가적으로 필드 라이브러리를 이용해서,",2766.74,15
46:06.980,46:08.640,"어떠한 데이터셋이 있을 때,",2768.64,15
46:08.720,46:10.080,"그러한 데이터셋에 대해서,",2770.08,15
46:10.280,46:13.040,어떻게 전처리를 수행할 건지 명시할 수 있도록 합니다.,2773.04,15
46:13.320,46:14.780,"번역 모델에 입력을 넣을 때는,",2774.78,15
46:14.960,46:18.160,"각각의 문장들의 앞부분에는 SOS 토큰을 붙이고,",2778.16,15
46:18.340,46:21.260,가장 뒷부분에는 EOS 토큰을 붙이는 것이 일반적입니다.,2781.26,15
46:21.380,46:24.700,또한 각각의 단어들은 모든 소문자로 바꿔주는 것이 일반적이고요.,2784.7,15
46:24.820,46:26.800,"또한 트랜스포머에 입력을 넣을 때는,",2786.8,15
46:26.860,46:29.400,"텐서의 차원에서 시퀀스보다는 배치가,",2789.4,15
46:29.520,46:31.960,"먼저 오도록 만드는 경우가 많기 때문에 이렇게,",2791.96,15
46:32.220,46:34.880,배치 퍼스트의 값으로는 True 값을 넣어주겠습니다.,2794.88,15
46:35.220,46:37.120,"이제 이어서 약 3만 개 정도의,",2797.12,15
46:37.140,46:40.080,"영어, 독어, 쌍을 가지고 있는 번역 데이터셋인,",2800.08,15
46:40.160,46:42.020,"Multi-30K를 불러와서,",2802.02,15
46:42.080,46:44.000,데이터를 초기화할 수 있도록 해줄게요.,2804.0,15
46:44.160,46:46.600,"이때 각각 이 필드 라이브러리를 이용해서,",2806.6,15
46:46.860,46:49.040,"독일어를 영어로 바꾸는 태스크에 대해서,",2809.04,15
46:49.220,46:51.120,"각각 앞서 정의했던 전처리를,",2811.12,15
46:51.120,46:52.540,수행할 수 있도록 하는 것입니다.,2812.54,15
46:52.700,46:55.160,"이제 학습 데이터셋과 평가 데이터셋,",2815.16,15
46:55.240,46:56.400,"테스트 데이터에 대해서,",2816.4,15
46:56.540,46:58.360,"개수를 출력하도록 만들어 보시면,",2818.36,15
46:58.600,47:00.700,"이와 같이 학습 데이터가 2만 9천 개,",2820.7,15
47:00.820,47:02.840,"그리고 평가 데이터와 테스트 데이터가,",2822.84,15
47:02.880,47:04.160,"각각 약 천 개 정도,",2824.16,15
47:04.360,47:06.480,문장으로 구성되어 있는 걸 확인할 수 있고요.,2826.48,15
47:06.740,47:07.760,"이때 한번 간단하게,",2827.76,15
47:07.900,47:10.380,"인덱스 30번에 해당하는 학습 문장을,",2830.38,15
47:10.400,47:11.560,"출력하도록 만들어 보시면,",2831.56,15
47:11.840,47:14.040,"바로 이러한 독일어 문장이 들어왔을 때,",2834.04,15
47:14.160,47:16.240,"이러한 영어 문장을 출력하도록,",2836.24,15
47:16.260,47:18.820,학습 데이터가 구성되어 있는 걸 확인할 수 있고요.,2838.82,15
47:19.120,47:20.340,"자, 그래서 이제 실제로,",2840.34,15
47:20.420,47:21.100,"vocabulary 세팅을,",2841.1,15
47:21.100,47:22.000,만들 수 있습니다.,2842.0,15
47:22.140,47:24.000,"이 vocabulary를 만들어주는 이유는,",2844.0,15
47:24.000,47:25.740,"독일어를 영어로 번역할 때,",2845.74,15
47:25.820,47:28.320,"각각의 초기 인풋 디멘션의 얼마인지를,",2848.32,15
47:28.360,47:29.460,구할 수가 있기 때문입니다.,2849.46,15
47:29.620,47:31.580,"그래서 전체 단어들 중에서,",2851.58,15
47:31.680,47:32.840,"최소 2번 이상,",2852.84,15
47:32.940,47:35.340,"등장한 단어들만을 선택하도록 만들어서,",2855.34,15
47:35.500,47:37.020,"vocabulary 세트를 만든 뒤에,",2857.02,15
47:37.220,47:39.200,"각각의 length를 출력하도록 만들어 보시면,",2859.2,15
47:39.560,47:42.600,"독일어는 7,855개의 유의미한 단어가 있고,",2862.6,15
47:42.960,47:45.200,"그리고 영어는 5,893개의,",2865.2,15
47:45.200,47:47.500,각각의 단어들이 존재한다고 볼 수 있는 거예요.,2867.5,15
47:47.680,47:48.120,"이제 그래서,",2868.12,15
47:48.260,47:49.820,"이런 vocabulary 객체에서,",2869.82,15
47:50.060,47:50.920,"string to,",2870.92,15
47:50.920,47:52.240,"i 함수를 호출해 가지고,",2872.24,15
47:52.460,47:55.120,"각각의 단어가 어떠한 인덱스에 해당하는지를,",2875.12,15
47:55.140,47:56.000,구해볼 수 있습니다.,2876.0,15
47:56.260,47:56.920,"예를 들어,",2876.92,15
47:57.060,47:59.240,"영어에서 이렇게 abc, abc란 단어가,",2879.24,15
47:59.320,48:00.620,존재하는지 확인할 수 있는데요.,2880.62,15
48:00.900,48:02.280,"이때 만약에 이런 식으로,",2882.28,15
48:02.480,48:05.020,"애초에 등장하지 않았던 없는 단어라면,",2885.02,15
48:05.200,48:06.660,0이라고 뱉는 걸 확인할 수 있고요.,2886.66,15
48:06.860,48:08.700,"이렇게 아예 의미가 없는,",2888.7,15
48:08.860,48:10.680,"그런 공간에 해당하는 padding 값은,",2890.68,15
48:10.780,48:12.540,1로 들어가 있는 걸 확인할 수 있고요.,2892.54,15
48:12.840,48:15.060,"이제 이렇게 sos와 os는 기본적으로,",2895.06,15
48:15.200,48:16.720,2번과 3번에 해당합니다.,2896.72,15
48:16.940,48:19.060,"또한 이렇게 실제로 존재하는 단어 같은 경우는,",2899.06,15
48:19.180,48:20.720,"그 단어의 인덱스가 나오는 걸,",2900.72,15
48:20.720,48:21.460,확인할 수 있고요.,2901.46,15
48:21.600,48:22.400,"이제 이와 같이,",2902.4,15
48:22.460,48:24.220,"앞쪽에 붙는 4개의 토큰들이,",2904.22,15
48:24.300,48:25.720,"차례대로 unknown 토큰,",2905.72,15
48:25.760,48:26.660,"그 다음 padding 토큰,",2906.66,15
48:26.780,48:27.760,"sos 토큰,",2907.76,15
48:27.760,48:28.900,os 토큰이라고 불리고요.,2908.9,15
48:29.000,48:30.040,"이 4개의 토큰들은,",2910.04,15
48:30.140,48:31.820,"실제로 존재하는 단어는 아니지만,",2911.82,15
48:32.020,48:33.860,"네트워크가 각각의 문장들을,",2913.86,15
48:33.940,48:35.780,"적절하게 학습할 수 있도록 만들기 위해,",2915.78,15
48:35.860,48:36.740,사용하는 토큰들입니다.,2916.74,15
48:36.960,48:37.520,"이어서,",2917.52,15
48:37.580,48:39.200,"한 문장에 포함된 단어들이,",2919.2,15
48:39.300,48:39.960,"순서대로,",2919.96,15
48:40.000,48:41.920,나열된 상태로 네트워크에 입력이 되는데요.,2921.92,15
48:42.140,48:44.380,"이때 하나의 배치에 포함된 문장들이,",2924.38,15
48:44.480,48:47.180,"가지는 단어의 개수가 유사할 수 있도록 만들기 위해,",2927.18,15
48:47.460,48:49.080,bucket iterator를 사용할 수 있습니다.,2929.08,15
48:49.080,48:50.240,"이때 이런 식으로,",2930.24,15
48:50.340,48:51.720,배치 사이즈를 정해줄 수 있는데요.,2931.72,15
48:52.000,48:53.460,"하나의 배치에 포함되어 있는,",2933.46,15
48:53.500,48:54.720,"각각의 문장들의,",2934.72,15
48:54.740,48:55.820,"그 시퀀스 랭스가,",2935.82,15
48:55.960,48:57.460,"가능한 유사도록 만들어서,",2937.46,15
48:57.660,48:59.340,"길이가 짧은 문장들에 대해서,",2939.34,15
48:59.680,49:02.220,"padding 토큰이 최대한 적게 들어갈 수 있도록 하여,",2942.22,15
49:02.300,49:03.360,"실제로 학습을 위해,",2943.36,15
49:03.460,49:05.500,"네트워크에 입력으로 들어가는 데이터의,",2945.5,15
49:05.620,49:06.740,차원을 줄일 수 있습니다.,2946.74,15
49:06.960,49:08.040,"그래서 이렇게 각각,",2948.04,15
49:08.100,49:08.700,"학습용,",2948.7,15
49:08.780,49:09.300,"평가용,",2949.3,15
49:09.360,49:09.880,"테스트용,",2949.88,15
49:10.040,49:12.240,데이터셋을 iterator로 만들어주고요.,2952.24,15
49:12.460,49:13.300,"한번 간단하게,",2953.3,15
49:13.340,49:15.040,"이 train iterator에 포함되어 있는,",2955.04,15
49:15.320,49:16.900,"첫 번째 배치를 확인한 뒤에,",2956.9,15
49:17.040,49:18.300,"하나의 문장에 대한 정보를,",2958.3,15
49:18.300,49:19.200,출력해 볼 수 있습니다.,2959.2,15
49:19.340,49:20.180,"확인해 보시면,",2960.18,15
49:20.360,49:21.900,"현재 배치에 포함되어 있는,",2961.9,15
49:21.960,49:23.120,"문장들 중에서,",2963.12,15
49:23.220,49:25.260,"가장 긴 문장의 시퀀스 길이가,",2965.26,15
49:25.360,49:26.480,35가 되겠고요.,2966.48,15
49:26.620,49:27.800,"현재 출력한 문장은,",2967.8,15
49:27.840,49:28.940,"이 배치에 포함되어 있는,",2968.94,15
49:28.980,49:30.020,첫 번째 문장이고요.,2970.02,15
49:30.220,49:30.780,"이런 식으로,",2970.78,15
49:30.960,49:32.240,"어떠한 문장이 들어가 있는데,",2972.24,15
49:32.620,49:33.980,"여기 이제 2번 같은 경우는,",2973.98,15
49:34.020,49:34.800,"sos가 되고,",2974.8,15
49:35.040,49:36.640,3번은 eos라고 했죠.,2976.64,15
49:36.760,49:39.420,"그래서 이제 sos와 eos 안에 포함되어 있는,",2979.42,15
49:39.580,49:41.060,"각각의 데이터가 실제,",2981.06,15
49:41.200,49:42.000,"하나하나씩,",2982.0,15
49:42.020,49:43.140,단어를 의미하는 거고요.,2983.14,15
49:43.380,49:43.720,"이제 이렇게,",2983.72,15
49:43.900,49:45.080,"eos가 나온 뒤에는,",2985.08,15
49:45.160,49:46.920,"뒤쪽에 다 padding 토큰이 붙어가지고,",2986.92,15
49:47.120,49:47.960,"의미가 없는,",2987.96,15
49:47.960,49:48.860,"토큰이라는 것을,",2988.86,15
49:48.920,49:50.200,알려줄 수 있도록 하는 것입니다.,2990.2,15
49:50.580,49:51.120,"이제 이런 식으로,",2991.12,15
49:51.300,49:52.360,"현재 배치에 있는,",2992.36,15
49:52.420,49:53.900,"하나의 문장에 포함된 정보를,",2993.9,15
49:53.960,49:54.620,출력할 수 있습니다.,2994.62,15
49:54.860,49:55.920,"이때 이 값은,",2995.92,15
49:55.960,49:57.180,"시퀀스 length가 되기 때문에,",2997.18,15
49:57.360,49:59.060,"우리는 첫 번째 문장에 있는,",2999.06,15
49:59.260,49:59.980,"각각의 단어들을,",2999.98,15
50:00.060,50:01.580,출력하도록 만들 수가 있는 것입니다.,3001.58,15
50:02.000,50:02.640,"이제 그래서 실제로,",3002.64,15
50:03.080,50:04.300,"트레이스프머의 논문과,",3004.3,15
50:04.380,50:05.280,"최대한 유사하게,",3005.28,15
50:05.400,50:07.080,"각각의 아키텍처를 정의해서,",3007.08,15
50:07.360,50:08.640,모델을 학습할 수가 있는데요.,3008.64,15
50:08.940,50:09.600,"가장 먼저,",3009.6,15
50:09.820,50:10.840,multi-head attention입니다.,3010.84,15
50:11.080,50:11.900,"확인해 보시면 이렇게,",3011.9,15
50:12.120,50:13.900,attention을 3가지 요소로 입력으로 봤고요.,3013.9,15
50:14.140,50:15.660,Query와 Key와 Value입니다.,3015.66,15
50:15.660,50:16.520,"기본적으로,",3016.52,15
50:16.540,50:17.920,"Query, Key, Value의 차원들은,",3017.92,15
50:18.020,50:18.340,"모두,",3018.34,15
50:18.400,50:19.520,"d 모델을,",3019.52,15
50:19.540,50:20.700,"h로 나눈 값으로,",3020.7,15
50:20.780,50:22.400,"차원을 모두 같도록 만들면,",3022.4,15
50:22.520,50:23.840,간단하게 정의할 수 있습니다.,3023.84,15
50:24.080,50:25.660,"이렇게 파라미터로 3개를 봤는데,",3025.66,15
50:25.900,50:27.040,"먼저 Hidden Dimension은,",3027.04,15
50:27.060,50:27.840,"하나의 단어에 대한,",3027.84,15
50:27.900,50:28.740,인베딩이 되겠고요.,3028.74,15
50:29.020,50:29.420,"이때,",3029.42,15
50:29.520,50:30.220,"n-head는,",3030.22,15
50:30.340,50:31.640,말 그대로 h가 되겠습니다.,3031.64,15
50:31.920,50:33.100,"즉, 헤드의 개수가 되는 거고,",3033.1,15
50:33.380,50:33.680,"그 다음에,",3033.68,15
50:33.840,50:34.700,"드랍아웃 레이셔는,",3034.7,15
50:34.840,50:36.260,"별도의 정교화 테크닉으로,",3036.26,15
50:36.420,50:37.760,"드랍아웃을 적용할 때의,",3037.76,15
50:37.800,50:39.140,비율을 설정하는 것입니다.,3039.14,15
50:39.440,50:40.300,"참고로 여기에서,",3040.3,15
50:40.440,50:41.820,"우리가 앞서 공부했을 때는,",3041.82,15
50:41.820,50:44.020,"각각의 Query와 Key, Value들은,",3044.02,15
50:44.020,50:44.980,"Hidden Dimension에서,",3044.98,15
50:45.080,50:46.560,"이 Key의 차원으로,",3046.56,15
50:46.640,50:48.240,바뀌어진다고 말씀을 드렸는데요.,3048.24,15
50:48.400,50:49.280,"실제로 구현할 때는,",3049.28,15
50:49.340,50:49.980,"이와 같이 그냥,",3049.98,15
50:50.060,50:50.900,"Hidden Dimension을,",3050.9,15
50:50.920,50:51.700,"Hidden Dimension으로,",3051.7,15
50:51.800,50:53.080,"맵핑하도록 만든 다음에,",3053.08,15
50:53.300,50:54.520,"이 결과 Dimension을,",3054.52,15
50:54.560,50:55.680,"h의 개로 쪼개서,",3055.68,15
50:55.740,50:56.520,사용할 수 있는 겁니다.,3056.52,15
50:56.880,50:57.960,"자, 그래서 확인해 보시면은,",3057.96,15
50:58.000,50:59.300,"각 헤드에 포함되어 있는,",3059.3,15
50:59.560,51:00.500,"그 인베딩 차원은,",3060.5,15
51:00.540,51:01.560,"Head Dimension이라고 해서,",3061.56,15
51:01.820,51:03.540,"이 단어들의 인베딩 차원을,",3063.54,15
51:03.560,51:05.940,h로 나눈 값을 사용하도록 만듭니다.,3065.94,15
51:06.020,51:06.980,"또한, Scale 값도,",3066.98,15
51:07.020,51:08.620,"앞서 설명했던 내용과 마찬가지로,",3068.62,15
51:08.620,51:10.960,"각각의 Query와 Key와 Value에 해당하는,",3070.96,15
51:11.020,51:11.720,"그 값에,",3071.72,15
51:11.860,51:13.040,"Root를 씌운 값을,",3073.04,15
51:13.160,51:14.200,"나중에 나눠주어서,",3074.2,15
51:14.360,51:16.440,Softmax에 넣어줄 수 있도록 만듭니다.,3076.44,15
51:16.500,51:17.120,"그래서 이때,",3077.12,15
51:17.160,51:19.060,"각각 Query와 Key, Value가 들어오는데요.",3079.06,15
51:19.240,51:20.220,"이때 Query Length는,",3080.22,15
51:20.260,51:21.300,단어의 개수가 되겠죠.,3081.3,15
51:21.540,51:21.960,"그래서,",3081.96,15
51:22.040,51:22.720,"이와 같이,",3082.72,15
51:22.840,51:24.500,"각각 Query와 Key, Value로,",3084.5,15
51:24.540,51:25.200,"그대로 다,",3085.2,15
51:25.320,51:26.340,맵핑을 해주고요.,3086.34,15
51:26.520,51:27.300,"이때 여기에서,",3087.3,15
51:27.420,51:27.720,"우리는,",3087.72,15
51:27.880,51:29.720,"이러한 Query와 Key의 Value의,",3089.72,15
51:29.780,51:30.620,"결과 값들을,",3090.62,15
51:30.760,51:31.460,"h개로,",3091.46,15
51:31.540,51:32.820,나눠 사용할 수 있는 것입니다.,3092.82,15
51:33.120,51:34.620,"즉, h개 각각마다,",3094.62,15
51:34.640,51:36.400,"Head Dimension만큼의 크기로,",3096.4,15
51:36.520,51:38.060,"차원을 가지도록 만들어서,",3098.06,15
51:38.220,51:38.600,"앞서,",3098.6,15
51:38.620,51:39.580,"확인했던 그림에서,",3099.58,15
51:39.700,51:40.040,"이렇게,",3100.04,15
51:40.160,51:42.060,"Linear를 각각 거친 값을,",3102.06,15
51:42.140,51:43.200,뽑아낼 수가 있는 것입니다.,3103.2,15
51:43.600,51:43.960,"이제 그래서,",3103.96,15
51:44.140,51:45.360,"각각 h개,",3105.36,16
51:45.460,51:45.880,"Value,",3105.88,16
51:46.100,51:46.400,"Key,",3106.4,16
51:46.580,51:47.100,"Query들을,",3107.1,16
51:47.200,51:48.520,만든 것과 마찬가지입니다.,3108.52,16
51:48.740,51:49.280,"이제 그렇게,",3109.28,15
51:49.440,51:50.600,"나누어진 상태에서,",3110.6,15
51:50.840,51:52.100,"각각의 Head마다,",3112.1,15
51:52.140,51:53.180,"Query와 Key를,",3113.18,15
51:53.240,51:54.680,"서로 곱하도록 만들어주고,",3114.68,15
51:54.800,51:55.860,스케일로 나눠줍니다.,3115.86,15
51:56.060,51:56.660,"그래서 이렇게,",3116.66,15
51:56.860,51:57.780,"에너지를 구한 다음에,",3117.78,15
51:57.960,51:58.520,"필요하다면,",3118.52,15
51:58.560,52:00.020,마스크를 씌울 수 있다고 했었죠.,3120.02,15
52:00.160,52:00.520,"이때,",3120.52,15
52:00.640,52:02.360,"마스크 값이 0인 부분을 전부 다,",3122.36,15
52:02.460,52:04.660,"Minus 무한에 가까운 값으로 넣어주어서,",3124.66,15
52:04.860,52:07.120,"실제로 Softmax에 들어간 결과 값이,",3127.12,15
52:07.160,52:08.600,"거의 0%가 될 수 있도록,",3128.6,15
52:08.600,52:09.340,만드는 거고요.,3129.34,15
52:09.400,52:09.920,"이제 그래서,",3129.92,15
52:10.080,52:11.600,"Softmax를 취한 다음에,",3131.6,15
52:11.800,52:12.760,"그렇게 나오게 된,",3132.76,15
52:12.820,52:14.300,"Attention 가중치 값들을,",3134.3,15
52:14.420,52:16.160,"실제로 V값과 곱해서,",3136.16,15
52:16.280,52:17.420,"Attention Value 값들을,",3137.42,15
52:17.520,52:19.260,결과적으로 만들어줄 수가 있는 거고요.,3139.26,15
52:19.420,52:20.240,"이제 얘를 다시,",3140.24,15
52:20.400,52:21.720,"일자로 쭉 늘어뜨려서,",3141.72,15
52:21.920,52:23.080,"Concat을 수행한 것과,",3143.08,15
52:23.120,52:25.020,동일한 결과를 뽑을 수 있도록 만듭니다.,3145.02,15
52:25.100,52:26.560,"그래서 결과적으로 마지막에,",3146.56,15
52:26.620,52:28.320,"Output Linear 함수를 거쳐서,",3148.32,15
52:28.400,52:29.500,"결과를 뽑아내고,",3149.5,15
52:29.640,52:31.480,"그리고 Attention Score 값은 따로,",3151.48,15
52:31.500,52:32.760,"또 출력하도록 만들어서,",3152.76,15
52:32.900,52:33.220,"나중에,",3153.22,15
52:33.400,52:35.260,시각화를 수행할 수 있도록 만듭니다.,3155.26,15
52:35.340,52:35.760,"이어서,",3155.76,15
52:35.860,52:37.920,Pointwise Feed-For architecture 인데요.,3157.92,15
52:37.940,52:38.580,"마찬가지로,",3158.58,15
52:38.600,52:40.900,"이렇게 Hidden Dimension 만큼의 차원이 들어왔을 때,",3160.9,15
52:40.960,52:43.780,Hidden Dimension으로 그대로 내보낸다는 점이 특징이고요.,3163.78,15
52:43.880,52:45.800,"즉, 입력과 출력의 차원이 동일합니다.",3165.8,15
52:46.040,52:46.640,"이제 이어서,",3166.64,15
52:46.680,52:47.620,"앞에 정의했던,",3167.62,15
52:47.760,52:50.000,"Attention 과 Feed-For Layer에 대한,",3170.0,15
52:50.120,52:51.120,"클래스를 이용해서,",3171.12,15
52:51.240,52:54.180,실제로 하나의 인코더 레이어의 아키텍처를 정의할 수가 있는데요.,3174.18,15
52:54.480,52:56.500,마찬가지로 입력과 출력의 차원이 같고요.,3176.5,15
52:56.840,52:57.480,"그 다음에 이제,",3177.48,15
52:57.520,52:59.260,"이렇게 정의된 인코더 레이어를,",3179.26,15
52:59.400,53:01.140,"실제로는 여러 번 중첩해서,",3181.14,15
53:01.300,53:03.100,하나의 전체 인코더에 들어가게 됩니다.,3183.1,15
53:03.380,53:05.420,"자, 그래서 인코더 레이어를 확인해 보시면은,",3185.42,15
53:05.480,53:08.180,"4개의 실질적인 레이어가 들어가 있는 걸,",3188.18,15
53:08.200,53:08.580,확인할 수 있습니다.,3188.58,15
53:08.580,53:08.980,확인할 수 있는데요.,3188.98,15
53:09.260,53:10.540,"여기 그림을 확인해 보시면,",3190.54,16
53:10.740,53:11.860,"실제 인코더 레이어는,",3191.86,16
53:11.900,53:13.660,"말씀드렸듯이 Attention 이후에,",3193.66,16
53:13.680,53:15.520,"residual connection과 normalization,",3195.52,16
53:15.880,53:17.340,"그 다음에 Feed-For Layer와,",3197.34,16
53:17.420,53:19.640,"다시 한 번 residual connection과 normalization,",3199.64,16
53:19.860,53:21.320,수행을 한다고 말씀을 드렸죠.,3201.32,16
53:21.440,53:23.340,"그래서 각각의 레이어가 이렇게 4개,",3203.34,15
53:23.380,53:24.720,차례대로 들어가 있는 거고요.,3204.72,15
53:24.840,53:25.500,"가장 먼저,",3205.5,15
53:25.660,53:27.380,"하나의 입력 값이 들어왔을 때,",3207.38,15
53:27.480,53:28.240,"그 입력 값을,",3208.24,15
53:28.320,53:30.480,"Query랑 Key Value로 그대로 복제해서,",3210.48,15
53:30.560,53:32.200,같은 값들을 넣어주도록 만들고요.,3212.2,15
53:32.340,53:34.080,"우리가 특정 단어에 대해서는,",3214.08,15
53:34.120,53:35.860,"Attention을 수행하지 않도록 하기 위해서,",3215.86,15
53:36.120,53:37.660,Mask Vector를 씌울 수 있다고 했습니다.,3217.66,15
53:37.660,53:39.040,"그대로 건너온,",3219.04,15
53:39.080,53:40.680,"입력 Embedding Matrix와,",3220.68,15
53:40.720,53:42.740,Mask를 그대로 넣을 수 있도록 하는 거고요.,3222.74,15
53:42.860,53:43.980,"그래서 가장 먼저 이렇게,",3223.98,15
53:44.140,53:45.400,"Self-Attention을 수행해서,",3225.4,15
53:45.500,53:46.400,"결과을 뽑아낸 뒤에,",3226.4,15
53:46.560,53:48.040,"구해진 값을 더해줘서,",3228.04,15
53:48.180,53:49.700,"residual connection을 수행한 뒤에,",3229.7,15
53:49.980,53:52.120,"실제로 normalization을 수행한 결과가,",3232.12,15
53:52.160,53:53.100,나올 수 있도록 하고요.,3233.1,15
53:53.240,53:54.140,"이제 마찬가지로,",3234.14,15
53:54.220,53:55.840,"Feed-For Layer를 거친 다음에,",3235.84,15
53:56.100,53:58.300,"다시 한 번 residual connection을 거쳐서,",3238.3,15
53:58.460,54:00.840,만들어진 Output을 내보내는 걸 확인할 수 있습니다.,3240.84,15
54:01.080,54:03.000,"그래서 실제 인코더 아키텍처에서는,",3243.0,15
54:03.200,54:05.160,"이렇게 앞서 정의했던 인코더 레이어를,",3245.16,15
54:05.280,54:07.640,총 N Layer만큼 반복해서 써봅시다.,3247.64,15
54:07.640,54:07.640,"자,",3247.64,15
54:07.660,54:08.360,"이렇게 해서,",3248.36,15
54:08.360,54:09.940,"인코더 파트에서,",3249.94,15
54:10.140,54:12.100,"실제 단어들의 개수에 해당하는,",3252.1,15
54:12.140,54:13.380,"Input Dimension에 해당하는,",3253.38,15
54:13.480,54:14.880,"Matrix가 들어왔을 때,",3254.88,15
54:14.960,54:16.340,전부 다 Embedding 차원으로 바꿔주고요.,3256.34,15
54:17.180,54:18.740,"그 다음 또 추가적으로 여기에서,",3258.74,15
54:18.860,54:19.820,"바로 이 부분이,",3259.82,15
54:19.940,54:22.400,실제 논문과는 다르게 구현된 부분이라고 할 수 있습니다.,3262.4,15
54:22.620,54:23.460,"현재 코드에서는,",3263.46,15
54:23.600,54:24.700,"원본 논문과 다르게,",3264.7,15
54:24.820,54:27.260,위치 Embedding을 직접 학습하는 형태로 구현합니다.,3267.26,15
54:27.460,54:28.620,"즉 특정 파라미터로,",3268.62,15
54:28.640,54:29.460,"이미 정해져 있는,",3269.46,15
54:29.640,54:31.700,"Sine과 Cosine 함수를 사용하는 것이 아니라,",3271.7,15
54:31.880,54:34.480,"이런 위치 Embedding을 학습하도록 만들어도,",3274.48,15
54:34.600,54:36.380,"비슷한 결과를 내보낼 수 있기 때문에,",3276.38,15
54:36.560,54:37.520,"본 코드에서는 이렇게,",3277.52,15
54:37.640,54:37.640,"이렇게,",3277.64,15
54:37.660,54:39.780,"Embedding 값을 학습하는 레이어로서,",3279.78,15
54:39.820,54:42.100,"별도의 학습 레이어로서 사용할 수 있다고,",3282.1,15
54:42.240,54:43.140,보여주는 것입니다.,3283.14,15
54:43.340,54:45.300,"또한 이제 실제로 Mask 값은,",3285.3,15
54:45.340,54:46.440,"이 Padding 토큰에 대해서,",3286.44,15
54:46.580,54:48.480,0 값이 들어가는 형태로 만들어지는데요.,3288.48,15
54:49.120,54:49.760,"말씀드렸듯이,",3289.76,15
54:49.760,54:50.800,"하나의 배치 안에는,",3290.8,15
54:50.920,54:52.580,"여러 개의 입력 문장이 들어가 있는데,",3292.58,15
54:52.940,54:54.420,"이때 짧은 문장에 대해서는,",3294.42,15
54:54.440,54:56.200,뒤쪽에 Padding 토큰으로 채워진다고 했습니다.,3296.2,15
54:56.480,54:57.120,"그렇기 때문에,",3297.12,15
54:57.260,54:58.460,"Padding 토큰에 대해서는,",3298.46,15
54:58.520,55:00.500,"그 문장에 포함되어 있는 단어들끼리,",3300.5,15
55:00.600,55:01.700,"Attention을 수행할 때,",3301.7,15
55:01.800,55:04.000,"Padding 토큰은 무시하도록 만들어야 하기 때문에,",3304.0,15
55:04.300,55:05.580,"이렇게 Padding 토큰에 대해서,",3305.58,15
55:05.580,55:07.600,Mask를 씌워준다고 보시면 되겠습니다.,3307.6,15
55:07.880,55:08.680,"자 그래서 먼저,",3308.68,15
55:08.760,55:11.200,"맨 처음에 Input Dimension으로 입력이 들어왔을 때,",3311.2,15
55:11.280,55:13.060,"실제 Embedding 차원으로 바꿔주고,",3313.06,15
55:13.240,55:16.280,"거기다가 위치에 대한 정보 값을 더해주도록 만들기 위해서,",3316.28,15
55:16.440,55:19.020,별도의 Force Embedding 레이어를 추가해 준 거고요.,3319.02,15
55:19.140,55:21.820,"이제 Encoder 레이어는 반복적으로 중첩돼서,",3321.82,15
55:21.840,55:22.960,"사용이 되기 때문에,",3322.96,15
55:23.100,55:24.440,"Module List를 이용해서,",3324.44,15
55:24.500,55:26.760,N 레이어만큼 반복할 수 있도록 만듭니다.,3326.76,15
55:26.860,55:28.820,"그래서 처음에 입력이 들어왔을 때,",3328.82,15
55:28.940,55:31.440,이 배치 사이즈는 말 그대로 문장의 개수가 되겠고요.,3331.44,15
55:31.600,55:32.500,"이 Source Length는,",3332.5,15
55:32.500,55:33.660,"각 문장들 중에서,",3333.66,15
55:33.800,55:35.560,"단어의 개수가 가장 많은 문장,",3335.56,15
55:35.580,55:36.900,단어의 개수가 되겠습니다.,3336.9,15
55:37.200,55:37.760,"이제 그래서,",3337.76,15
55:37.940,55:39.880,"Positional Encoding은 차례대로 0부터,",3339.88,15
55:40.020,55:43.400,"가장 긴 문장에 해당하는 번호까지 들어갈 수 있도록 만들고,",3343.4,15
55:43.520,55:46.680,"이제 그것을 각각의 문장마다 적용하도록 만들기 위해서,",3346.68,15
55:46.860,55:47.680,리핏을 수행합니다.,3347.68,15
55:47.860,55:48.760,"그래서 결과적으로,",3348.76,15
55:48.940,55:50.060,"입력 Embedding 값에,",3350.06,15
55:50.140,55:52.700,"그러한 위치에 대한 정보가 포함된 데이터를,",3352.7,15
55:52.760,55:54.800,실제 입력 값으로 사용을 해주는 거고요.,3354.8,15
55:54.920,55:56.480,"그래서 그러한 입력 값이,",3356.48,15
55:56.520,55:58.720,"여러 개의 레이어를 반복적으로 거치면서,",3358.72,15
55:58.960,56:01.480,"순전파, 즉 Forward를 수행할 수 있도록 만듭니다.",3361.48,15
56:01.580,56:03.840,"마지막 Encoder 레이어에서 나오게 된,",3363.84,15
56:03.940,56:05.040,"그 출력 값을,",3365.04,15
56:05.040,56:07.360,결과적으로 사용할 수 있도록 만드는 것입니다.,3367.36,15
56:07.520,56:08.080,"이어서,",3368.08,15
56:08.100,56:10.380,Decoder 레이어의 아키텍처를 확인해 보겠습니다.,3370.38,15
56:10.680,56:11.320,"마찬가지로,",3371.32,15
56:11.320,56:12.780,입력과 출력의 차원이 같고요.,3372.78,15
56:12.940,56:14.640,"그래서 Transformer의 Decoder는,",3374.64,15
56:14.640,56:15.680,"이러한 Decoder 레이어를,",3375.68,15
56:15.760,56:17.920,여러 번 중첩해서 사용한다고 말씀드렸죠.,3377.92,15
56:18.060,56:18.720,"또한 내부적으로,",3378.72,15
56:18.940,56:22.000,두 개의 Multi-Add Attention이 사용된다고 말씀드렸습니다.,3382.0,15
56:22.300,56:23.540,하나는 Self-Attention이고요.,3383.54,15
56:23.660,56:24.440,"그리고 하나는,",3384.44,15
56:24.440,56:26.300,"Encoder에 대한 정보를 받아오기 위한,",3386.3,15
56:26.400,56:28.520,Encoder Decoder Attention이라고 말씀드렸습니다.,3388.52,15
56:28.740,56:29.900,"그래서 내용을 확인해 보시면,",3389.9,15
56:30.160,56:32.380,총 6개의 레이어가 사용되는 걸 알 수 있고요.,3392.38,15
56:32.680,56:34.320,이는 여기 그림과 마찬가지입니다.,3394.32,16
56:34.520,56:35.020,"Self-Attention이,",3395.02,16
56:35.040,56:35.840,"Self-Attention 이후에,",3395.84,16
56:35.840,56:36.940,"Residual Connection,",3396.94,16
56:37.020,56:37.480,"그 다음에,",3397.48,16
56:37.580,56:39.100,"Encoder Decoder Attention 이후에,",3399.1,16
56:39.120,56:40.680,"Residual Connection 이후에,",3400.68,16
56:40.700,56:41.740,"Feed-forward 레이어와,",3401.74,16
56:41.780,56:43.760,"Residual Connection을 한 번 더 거쳐서,",3403.76,16
56:43.820,56:45.420,결과를 뽑아낼 수 있도록 만듭니다.,3405.42,16
56:45.520,56:46.400,"이제 이러한 레이어가,",3406.4,16
56:46.400,56:48.140,N번만큼 중첩되어 쓰이고요.,3408.14,16
56:48.340,56:50.440,"그렇게 마지막 레이어에서 나오게 된 출력 값이,",3410.44,16
56:50.500,56:53.140,실제 출력 문장에 대한 정보를 가지고 있다고 했죠.,3413.14,16
56:53.380,56:53.980,"따라서,",3413.98,16
56:54.000,56:55.700,"이와 같이 6가지 레이어를 전부,",3415.7,15
56:55.700,56:57.280,초기화할 수 있도록 만들어 주었고요.,3417.28,15
56:57.440,56:58.760,"이제 이렇게 가장 먼저,",3418.76,15
56:58.940,57:00.180,Self-Attention을 수행합니다.,3420.18,15
57:00.420,57:00.980,"그렇기 때문에,",3420.98,15
57:01.120,57:02.140,"Query와 Key Value는,",3422.14,15
57:02.140,57:04.460,모두 자기 자신을 넣을 수 있도록 만들어 주고요.,3424.46,15
57:04.560,57:05.320,"그 다음 이후에,",3425.32,15
57:05.320,57:06.720,"Residual Connection을 수행한 뒤에,",3426.72,15
57:06.940,57:08.960,"그 다음 Encoder에서 정보를 가져오는,",3428.96,15
57:08.980,57:10.660,Encoder Decoder Attention을 수행하는데요.,3430.66,15
57:10.900,57:11.360,"이때,",3431.36,15
57:11.440,57:12.060,"Query는,",3432.06,15
57:12.100,57:13.680,"바로 Decoder에 포함되어 있는,",3433.68,15
57:13.740,57:15.600,출력 단어들에 대한 정보가 되겠고요.,3435.6,15
57:15.900,57:16.540,"Encoder에서,",3436.54,15
57:16.660,57:18.460,"가장 마지막 출력 값으로 나온,",3438.46,15
57:18.580,57:20.960,그 값을 Key로 사용하는 것을 알 수 있습니다.,3440.96,15
57:21.200,57:22.060,"그래서 결과적으로,",3442.06,15
57:22.200,57:23.080,"이렇게 정의된,",3443.08,15
57:23.160,57:24.720,"Decoder Layer Architecture를 이용해서,",3444.72,15
57:24.920,57:26.460,"전체 Decoder Architecture에서는,",3446.46,15
57:26.640,57:29.380,그러한 Decoder Layer를 여러 번 중첩해서 사용하고요.,3449.38,15
57:29.520,57:30.900,"이제 원본 논문과는 다르게,",3450.9,15
57:30.960,57:31.640,"마찬가지로,",3451.64,15
57:31.640,57:32.460,"위치 임베딩을,",3452.46,15
57:32.520,57:34.460,"우리가 별도로 학습하는 형태로 구현을 해서,",3454.46,15
57:34.660,57:37.480,이 Sine 함수와 Cosine 함수를 사용하지 않도록 만들 것입니다.,3457.48,15
57:37.660,57:39.180,"또한 참고로 말씀드렸듯이,",3459.18,15
57:39.180,57:40.140,"이 Target 문장에서,",3460.14,15
57:40.260,57:41.040,"각각의 단어는,",3461.04,15
57:41.080,57:42.780,"다음 단어가 무엇인지 알 수 없도록,",3462.78,15
57:42.800,57:43.100,"즉,",3463.1,15
57:43.100,57:45.180,"이전에 출력한 단어만 볼 수 있도록 하기 위해서,",3465.18,15
57:45.380,57:47.480,별도의 Mask Vector를 사용할 수 있다고 했습니다.,3467.48,15
57:47.980,57:48.360,"자, 그래서,",3468.36,15
57:48.460,57:49.280,"아까랑 동일하게,",3469.28,15
57:49.380,57:50.660,"단어의 개수와 같은,",3470.66,15
57:50.760,57:51.500,"그 Dimension을,",3471.5,15
57:51.540,57:53.200,임베딩 차원으로 바꾸어 주고요.,3473.2,15
57:53.380,57:54.220,"그 다음 마찬가지로,",3474.22,15
57:54.220,57:55.520,"위치에 대한 정보를 주기 위해서,",3475.52,15
57:55.920,57:57.680,"전체 Sequence의 Length에 해당하는,",3477.68,15
57:57.800,57:58.520,"Dimension을,",3478.52,15
58:01.640,58:03.260,"이러한 Decoder 레이어는 반복적으로,",3483.26,15
58:03.320,58:05.480,중첩해 사용할 수 있다고 말씀드렸습니다.,3485.48,15
58:05.800,58:07.660,"자, 그래서 실제로 Fold를 수행할 때,",3487.66,15
58:07.880,58:10.680,"Encoder의 마지막 레이어에서 나오게 된 출력 값과,",3490.68,15
58:10.700,58:12.920,실제로 Target 문장에 대한 정보를 받고요.,3492.92,15
58:13.240,58:14.900,"마찬가지로 Target 문장 또한,",3494.9,15
58:14.920,58:16.640,"즉, 출력하기 위한 문장 또한,",3496.64,15
58:16.660,58:18.780,"0부터 그 단어의 개수까지에 대한,",3498.78,15
58:18.920,58:20.660,"위치에 대한 정보가 담겨야 하기 때문에,",3500.66,15
58:20.900,58:22.240,"하나의 Tensor를 초기화한 뒤에,",3502.24,15
58:22.460,58:23.460,"각 문장에 대해서,",3503.46,15
58:23.580,58:25.520,모두 동일하게 적용할 수 있도록 만듭니다.,3505.52,15
58:25.600,58:26.000,"그래서,",3506.0,15
58:26.100,58:27.480,"문장이 임베딩 값에,",3507.48,15
58:27.580,58:29.060,"위치에 대한 정보를 더한 것을,",3509.06,15
58:29.280,58:30.600,실제 입력으로 사용을 하고요.,3510.6,15
58:30.660,58:31.040,"그래서,",3511.04,15
58:31.040,58:32.860,"Decoder 레이어를 여러 번 거친 다음에,",3512.86,15
58:33.020,58:34.860,"마지막에 나오게 된 Output 값에,",3514.86,15
58:35.000,58:37.820,"출력을 위한 Linear 레이어를 거치도록 만들어서,",3517.82,15
58:37.920,58:39.980,결과적인 Output 값을 뽑아낼 수 있습니다.,3519.98,15
58:40.180,58:40.640,"그래서,",3520.64,15
58:40.720,58:42.600,"결과적인 Transformer Architecture는 이렇게,",3522.6,15
58:42.880,58:44.320,"전체 Encoder Architecture와,",3524.32,15
58:44.400,58:46.080,"전체 Decoder Architecture를 받아서,",3526.08,15
58:46.340,58:48.320,"입력 문장에 따라서 Mask를 붙여서,",3528.32,15
58:48.360,58:50.400,실제 결과를 뽑아낼 수 있도록 만듭니다.,3530.4,15
58:50.640,58:52.180,"이때, Source 문장 같은 경우는,",3532.18,15
58:52.220,58:54.780,"패딩 토큰에 대해서만 Mask 값을 0으로 설정해서,",3534.78,15
58:54.840,58:55.800,학습하도록 만들고요.,3535.8,15
58:55.980,58:57.600,"이때, Target 문장 또한 마찬가지로,",3537.6,15
58:57.640,58:59.160,"우리가 학습을 진행하는 과정에서,",3539.16,15
58:59.300,59:01.020,"다음 단어가 무엇인지 알 수 없도록 하는,",3541.02,15
59:01.020,59:02.220,"학습하기 위해서 마찬가지로,",3542.22,15
59:02.220,59:03.540,Mask 행렬을 사용할 수 있습니다.,3543.54,15
59:03.720,59:05.140,"여기 보이는 패딩 Mask는,",3545.14,15
59:05.180,59:06.640,"Source 문장과 마찬가지로,",3546.64,15
59:06.700,59:08.860,"패딩 토큰에 대해서 적용하도록 만들어서,",3548.86,15
59:08.980,59:11.600,학습할 때 이러한 Mask를 사용할 수 있도록 하고요.,3551.6,15
59:11.820,59:12.380,"추가적으로,",3552.38,15
59:12.480,59:15.060,"앞쪽에 있는 단어들만 볼 수 있도록 만들기 위해서,",3555.06,15
59:15.240,59:17.020,"별도의 Mask를 하나 더 만든 다음에,",3557.02,15
59:17.200,59:19.320,"이 두 Mask에 대해서 Element YG로,",3559.32,15
59:19.340,59:20.520,"End 연산을 수행한 뒤에,",3560.52,15
59:20.660,59:22.440,"결과적으로 이렇게 두 Mask에 대해서,",3562.44,15
59:22.640,59:25.400,"둘 다 1의 값을 가지는 그런 위치에 대해서만,",3565.4,15
59:25.460,59:27.940,"실제 Attention Score를 구할 수 있도록 만들어서,",3567.94,15
59:28.160,59:30.880,이렇게 Mask 값을 생성하는 것을 알 수 있습니다.,3570.88,15
59:31.020,59:31.860,"그래서 결과적으로,",3571.86,15
59:31.940,59:33.540,"Source와 Target이 들어왔을 때,",3573.54,15
59:33.660,59:35.060,"Mask를 각각 만든 뒤에,",3575.06,15
59:35.200,59:37.480,"먼저 Encoder에 이러한 Source 문장을 넣어서,",3577.48,15
59:37.600,59:39.200,"Encoder의 출력 값들을 뽑은 뒤에,",3579.2,15
59:39.360,59:40.540,"이제 Decoder는 매번,",3580.54,15
59:40.540,59:43.320,"그러한 Encoder의 출력 값을 Attention할 수 있도록 만들어서,",3583.32,15
59:43.500,59:45.500,"결과적으로 마지막에 나온 Output 값이,",3585.5,15
59:45.500,59:47.880,우리 네트워크의 번역 결과라고 할 수 있습니다.,3587.88,15
59:48.100,59:49.960,이제 그래서 실제로 학습을 진행할 수 있는데요.,3589.96,15
59:50.260,59:51.280,"학습할 땐 이와 같이,",3591.28,15
59:51.540,59:53.480,"Input Dimension과 Output Dimension은 각각,",3593.48,15
59:53.480,59:55.780,"Source 언어에 포함되어 있는 언어의 개수,",3595.78,15
59:55.900,59:59.120,그 다음에 Target 언어에 포함되어 있는 언어의 개수가 될 수 있도록 합니다.,3599.12,15
59:59.300,01:00:00.660,"그래서 각각의 단어에 대한,",3600.66,15
01:00:00.660,01:00:02.660,"Embedding 차원은 256으로 설정하고,",3602.66,15
01:00:02.760,01:00:05.860,레이어가 총 3번씩 중첩되어 사용할 수 있도록 만들었습니다.,3605.86,15
01:00:06.160,01:00:07.280,"여기 보이는 파라미터는,",3607.28,15
01:00:07.320,01:00:09.200,"실제 논문에서 제한된 파라미터에 비하면,",3609.2,15
01:00:09.340,01:00:10.700,"크기가 많이 작은 편이지만,",3610.7,15
01:00:10.760,01:00:11.640,"그럼에도 불구하고,",3611.64,15
01:00:11.760,01:00:13.680,충분히 좋은 성능을 낼 수 있습니다.,3613.68,15
01:00:13.940,01:00:14.980,"그래서 결과적으로 이렇게,",3614.98,15
01:00:15.200,01:00:16.960,"Transformer 객체를 만들어 준 뒤에,",3616.96,15
01:00:17.020,01:00:19.300,파라미터까지 다 초기화를 진행해 주시고요.,3619.3,15
01:00:19.480,01:00:21.360,"이렇게 전체 네트워크에 포함되어 있는,",3621.36,15
01:00:21.520,01:00:23.160,모든 파라미터를 확인해 볼 수 있습니다.,3623.16,15
01:00:23.500,01:00:25.520,그래서 이제 한번 학습을 진행할 수 있는데요.,3625.52,15
01:00:25.840,01:00:28.640,"이와 같이, Running Rate는 0.0005로 하고,",3628.64,15
01:00:28.960,01:00:30.180,"Adam Optimizer를 사용해서,",3630.18,15
01:00:30.180,01:00:31.800,모델을 학습할 수 있도록 하겠습니다.,3631.8,15
01:00:32.160,01:00:34.660,"그래서 학습함수와 평가함수를,",3634.66,15
01:00:34.740,01:00:36.520,따로 정의해서 학습을 진행하면요.,3636.52,15
01:00:36.580,01:00:39.060,이렇게 학습이 진행되는 걸 확인할 수 있고요.,3639.06,15
01:00:39.220,01:00:40.480,"학습을 진행할 때마다,",3640.48,15
01:00:40.640,01:00:43.000,"Validation Loss가 더 감소하는 경우에만,",3643.0,15
01:00:43.060,01:00:44.800,"모델 파라미터를 새로운 파일로,",3644.8,15
01:00:44.820,01:00:47.020,기록하도록 만드는 것을 알 수 있습니다.,3647.02,15
01:00:47.240,01:00:48.620,"결과적으로 이와 같이,",3648.62,15
01:00:48.620,01:00:50.560,학습이 완료된 걸 확인할 수 있고요.,3650.56,15
01:00:50.840,01:00:52.120,"이렇게 학습된 결과를,",3652.12,15
01:00:52.160,01:00:54.280,"여러분들의 컴퓨터에 기록하고자 한다면,",3654.28,15
01:00:54.680,01:00:56.460,"이렇게 File Library를 이용해서,",3656.46,15
01:00:56.620,01:00:58.280,"학습이 완료된 데이터를,",3658.28,15
01:00:58.380,01:00:59.740,다운로드 받을 수 있도록 합니다.,3659.74,15
01:00:59.740,01:01:01.300,"자, 이후에 이와 같이,",3661.3,15
01:01:01.300,01:01:03.120,"학습 완료된 모델을 이용해서,",3663.12,15
01:01:03.240,01:01:04.600,"테스트 데이터에 대해서,",3664.6,15
01:01:04.780,01:01:06.120,"Evaluation을 진행하게 되면,",3666.12,15
01:01:06.420,01:01:07.060,"다음과 같이,",3667.06,15
01:01:07.200,01:01:09.060,"테스트 데이터에 대한 Loss 값을,",3669.06,15
01:01:09.100,01:01:09.880,구해볼 수 있습니다.,3669.88,15
01:01:10.140,01:01:10.920,"이제 이어서,",3670.92,15
01:01:10.980,01:01:12.440,"여러분들만의 문장을 넣어서,",3672.44,15
01:01:12.620,01:01:14.680,실제로 우리 모델을 사용해 볼 수 있는데요.,3674.68,15
01:01:14.940,01:01:16.780,"이렇게 Translate Sentence 함수 안에,",3676.78,15
01:01:16.840,01:01:18.560,내용이 정의되어 있는 걸 알 수 있습니다.,3678.56,15
01:01:18.780,01:01:20.440,"먼저 하나의 문장이 들어왔을 때,",3680.44,15
01:01:20.620,01:01:21.600,"토큰을 진행한 뒤에,",3681.6,15
01:01:21.800,01:01:22.440,"앞 뒤로,",3682.44,15
01:01:22.460,01:01:24.280,SOS와 EOS 토큰을 붙입니다.,3684.28,15
01:01:24.540,01:01:26.040,"각각의 단어들에 대한 정보를,",3686.04,15
01:01:26.200,01:01:27.640,"Index로 바꿔주어서,",3687.64,15
01:01:27.800,01:01:29.080,"우리 모델의 입력으로,",3689.08,15
01:01:29.080,01:01:30.260,넣을 수 있도록 하고요.,3690.26,15
01:01:30.280,01:01:31.800,"이어서 마스크를 만든 뒤에,",3691.8,15
01:01:31.860,01:01:33.100,"실제로 인코더에,",3693.1,15
01:01:33.140,01:01:34.460,"이러한 소스 문장을 넣어서,",3694.46,15
01:01:34.560,01:01:36.020,출력 값을 구할 수 있도록 합니다.,3696.02,15
01:01:36.280,01:01:37.000,"그 다음에 이제,",3697.0,15
01:01:37.140,01:01:38.500,"실제 출력 문장은,",3698.5,15
01:01:38.540,01:01:40.120,"SOS 토큰부터 출발해서,",3700.12,15
01:01:40.360,01:01:41.200,"Max Length까지,",3701.2,15
01:01:41.360,01:01:42.360,"한 번씩 반복적으로,",3702.36,15
01:01:42.600,01:01:43.820,"모델의 디코더에 넣어서,",3703.82,15
01:01:43.980,01:01:45.840,출력 값을 만들어 낼 수 있도록 합니다.,3705.84,15
01:01:46.080,01:01:46.760,"그래서 이때,",3706.76,15
01:01:46.820,01:01:48.360,"매번 디코더에 넣었을 때,",3708.36,15
01:01:48.480,01:01:49.540,"가장 마지막 단어가,",3709.54,15
01:01:49.640,01:01:50.740,"출력 문장으로서,",3710.74,15
01:01:50.840,01:01:52.260,하나씩 추가가 되는 거고요.,3712.26,15
01:01:52.420,01:01:53.440,"그래서 결과적으로,",3713.44,15
01:01:53.740,01:01:55.820,"최대 Max Length만큼 반복을 하다가,",3715.82,15
01:01:56.040,01:01:57.340,"EOS를 만나는 순간에,",3717.34,15
01:01:57.360,01:01:58.480,"거기에서 멈추도록 해서,",3718.48,15
01:01:58.480,01:02:00.640,"그때까지 출력된 모든 단어들이,",3720.64,15
01:02:00.700,01:02:02.200,전체 출력 문장이 되겠습니다.,3722.2,15
01:02:02.460,01:02:04.580,"그래서 실제 결과로 뽑기 위해서,",3724.58,15
01:02:04.760,01:02:05.900,"각각의 인덱스를,",3725.9,15
01:02:06.040,01:02:07.720,"다시 문자열로 바꾸어서,",3727.72,15
01:02:07.860,01:02:09.580,리턴해주는 것을 알 수 있습니다.,3729.58,15
01:02:09.880,01:02:10.900,"그래서 간단하게,",3730.9,15
01:02:11.020,01:02:12.880,"한 번 테스트 데이터 중에서,",3732.88,15
01:02:13.020,01:02:14.940,이 열 번째 데이터를 확인해 보도록 할게요.,3734.94,15
01:02:15.480,01:02:16.380,"내용 확인해 보시면,",3736.38,15
01:02:16.600,01:02:18.080,얘가 독일어 문장이 되겠습니다.,3738.08,15
01:02:18.420,01:02:20.020,"이 내용이 원래 의미하는 내용은,",3740.02,15
01:02:20.100,01:02:21.140,"한 명의 어머니와,",3741.14,15
01:02:21.280,01:02:23.480,그의 자식은 노래를 부른다.,3743.48,15
01:02:23.520,01:02:25.480,"야외에서 좋은 날을 즐기며,",3745.48,15
01:02:25.560,01:02:26.200,라는 내용이죠.,3746.2,15
01:02:26.380,01:02:28.460,"이게 실제로 출력된 결과를 확인해 보면,",3748.46,15
01:02:28.460,01:02:30.100,"우리 모델이 번역한 내용은,",3750.1,15
01:02:30.120,01:02:32.020,"어머니와 그의 어린 아들이,",3752.02,15
01:02:32.100,01:02:34.780,야외에서 작은 나날을 즐기고 있다.,3754.78,15
01:02:34.900,01:02:36.540,"이런 식으로 번역되어 있는 걸,",3756.54,15
01:02:36.580,01:02:37.320,확인할 수 있습니다.,3757.32,15
01:02:37.560,01:02:39.200,"완전히 동일한 문장은 아니더라도,",3759.2,15
01:02:39.320,01:02:40.720,"이런 전반적인 의미 자체가,",3760.72,15
01:02:40.760,01:02:41.820,"잘 전달되도록,",3761.82,15
01:02:41.840,01:02:43.680,번역이 이루어진 걸 확인할 수 있습니다.,3763.68,15
01:02:43.900,01:02:45.900,"또한 이렇게 출력된 어텐션 값은,",3765.9,15
01:02:46.000,01:02:48.000,"총 8개의 헤드로 구성된,",3768.0,15
01:02:48.120,01:02:50.520,어텐션 스코어들의 집합이라고 할 수 있는데요.,3770.52,15
01:02:50.740,01:02:51.840,"전체 그림에다가,",3771.84,15
01:02:51.940,01:02:54.100,"각각의 헤드에 대한 어텐션 스코어 값을,",3774.1,15
01:02:54.160,01:02:55.320,출력하도록 만들 수 있습니다.,3775.32,15
01:02:55.640,01:02:56.740,"자, 그래서 이와 같이,",3776.74,15
01:02:56.860,01:02:57.800,"실제 그림까지,",3777.8,15
01:02:57.900,01:02:58.440,"출력하도록,",3778.44,15
01:02:58.460,01:02:59.120,"만들어 보시면,",3779.12,8
01:02:59.380,01:02:59.940,"자, 이렇게,",3779.94,8
01:03:00.040,01:03:01.920,"각각의 단어가 출력되기 위해서,",3781.92,8
01:03:02.120,01:03:03.340,"소스 문장에서의,",3783.34,8
01:03:03.360,01:03:04.280,"어떤 정보를,",3784.28,8
01:03:04.380,01:03:06.080,많이 참고했는지를 알 수 있습니다.,3786.08,8
01:03:06.340,01:03:07.360,"이 영어 문장으로,",3787.36,8
01:03:07.480,01:03:08.080,"mother와,",3788.08,8
01:03:08.180,01:03:09.960,"이 독일어 문장에 해당 단어는,",3789.96,8
01:03:10.060,01:03:10.740,"마찬가지로,",3790.74,8
01:03:10.760,01:03:12.380,어머니라는 의미를 가지고 있습니다.,3792.38,8
01:03:12.660,01:03:13.240,"그렇기 때문에,",3793.24,8
01:03:13.420,01:03:15.540,"이렇게 mother라는 단어를 출력하기 위해서,",3795.54,8
01:03:15.780,01:03:16.660,"독일어 문장에,",3796.66,8
01:03:16.720,01:03:18.220,"이러한 단어를 참고했다는 것을,",3798.22,8
01:03:18.400,01:03:19.980,시각적으로 확인할 수 있습니다.,3799.98,8
01:03:20.240,01:03:20.820,"이와 같이,",3800.82,8
01:03:21.000,01:03:22.760,"총 8개의 헤드 각각에 대해서,",3802.76,8
01:03:23.080,01:03:24.680,"어텐션 스코어 값이 매겨지는 걸,",3804.68,8
01:03:24.700,01:03:25.340,확인할 수 있고요.,3805.34,8
01:03:25.820,01:03:26.420,"말씀드렸듯이,",3806.42,8
01:03:26.420,01:03:27.440,"각각의 헤드들은,",3807.44,8
01:03:27.440,01:03:29.640,"서로 다른 어텐션 컨셉들을,",3809.64,8
01:03:29.720,01:03:31.120,"학습하도록 만들어지기 때문에,",3811.12,8
01:03:31.320,01:03:34.160,"이렇게 각각 어텐션 스코어 값이 구해진 결과가,",3814.16,8
01:03:34.200,01:03:36.180,"조금씩 다를 수 있다는 점 또한,",3816.18,8
01:03:36.180,01:03:36.960,확인할 수 있습니다.,3816.96,8
01:03:37.180,01:03:38.080,"이제 마지막으로,",3818.08,15
01:03:38.100,01:03:39.840,"간단하게 블루 스코어를 계산해서,",3819.84,15
01:03:40.100,01:03:42.260,"학습이 완료된 트랜스포먼 모델의,",3822.26,15
01:03:42.260,01:03:43.440,스코어를 구해볼 수 있습니다.,3823.44,15
01:03:43.700,01:03:44.600,"지금 예시에서는,",3824.6,15
01:03:44.820,01:03:46.480,"각각의 입력 문장이 있을 때,",3826.48,15
01:03:46.640,01:03:48.020,"그 정답 문장 또한,",3828.02,15
01:03:48.020,01:03:49.400,"하나씩만 존재하기 때문에,",3829.4,15
01:03:49.620,01:03:50.860,"하나의 예측 값에 대해서,",3830.86,15
01:03:51.140,01:03:52.560,"그 정답 값이 1대 1로,",3832.56,15
01:03:52.600,01:03:53.680,"매칭될 수 있도록,",3833.68,15
01:03:53.740,01:03:54.940,"바로 그 정답 문장을,",3834.94,15
01:03:55.040,01:03:56.460,"하나의 리스트로 감싸서,",3836.46,15
01:03:56.560,01:03:57.420,"하나씩 나열하고,",3837.42,15
01:03:57.420,01:03:58.400,결합하는 걸 알 수 있습니다.,3838.4,15
01:03:58.600,01:04:00.120,"자 그래서 이와 같이,",3840.12,15
01:04:00.200,01:04:01.820,"문장 100개당 한 번씩,",3841.82,15
01:04:01.840,01:04:03.200,"예측과 정답 값을,",3843.2,15
01:04:03.220,01:04:04.600,출력하도록 만들어 본 거고요.,3844.6,15
01:04:04.740,01:04:06.900,"자 이렇게 실제로 문장을 보시면은,",3846.9,15
01:04:06.940,01:04:07.920,"꽤 유사하게,",3847.92,15
01:04:08.000,01:04:09.880,예측이 이루어진 걸 확인할 수 있습니다.,3849.88,15
01:04:10.140,01:04:10.740,"예를 들어,",3850.74,15
01:04:10.840,01:04:12.180,"마지막 문장 같은 경우는,",3852.18,15
01:04:12.420,01:04:13.920,"나이가 많은 한 남자는,",3853.92,15
01:04:14.060,01:04:15.060,"미디어 게임을,",3855.06,15
01:04:15.080,01:04:16.180,"플레이하고 있다라고,",3856.18,15
01:04:16.180,01:04:16.860,번역을 했는데요.,3856.86,15
01:04:17.060,01:04:19.000,"실제 정답 값과 비교했을 때,",3859.0,15
01:04:19.140,01:04:19.920,"거의 유사하게,",3859.92,15
01:04:19.940,01:04:21.600,번역을 수행한 걸 확인할 수 있습니다.,3861.6,15
01:04:21.820,01:04:23.900,"또한 여기 보이는 이 블루 4 스코어가,",3863.9,15
01:04:24.020,01:04:25.120,"일반적으로 알려져 있는,",3865.12,15
01:04:25.200,01:04:26.920,블루 스코어와 같은 값을 가지는데요.,3866.92,15
01:04:26.920,01:04:29.020,"이렇게 블루 스코어 값으로는,",3869.02,15
01:04:29.040,01:04:31.220,36을 얻은 것을 확인할 수 있습니다.,3871.22,15
01:04:31.420,01:04:31.920,"이어서,",3871.92,15
01:04:31.960,01:04:34.700,같이 한 번 논문 리딩 진행해 보도록 하겠습니다.,3874.7,15
01:04:35.080,01:04:36.060,"논문의 제목은,",3876.06,15
01:04:36.060,01:04:38.340,Attention is All You Need 이고요.,3878.34,15
01:04:38.500,01:04:39.060,"말 그대로,",3879.06,15
01:04:39.340,01:04:40.780,"Attention만 잘 활용해도,",3880.78,15
01:04:40.860,01:04:42.760,"높은 성능을 얻을 수 있다라는,",3882.76,15
01:04:42.800,01:04:43.760,의미를 가지고 있습니다.,3883.76,15
01:04:44.140,01:04:45.960,"본 논문은 2017년도,",3885.96,15
01:04:46.040,01:04:48.420,NIPS에 구글 팀이 발표한 논문이고요.,3888.42,15
01:04:48.980,01:04:49.740,"말씀드렸듯이,",3889.74,15
01:04:49.740,01:04:50.660,"이전까지에는,",3890.66,15
01:04:50.820,01:04:54.160,"RNN과 Attention 메커니즘을 같이 활용하는 기법들이,",3894.16,15
01:04:54.260,01:04:55.540,많이 사용이 되었었는데요.,3895.54,15
01:04:55.760,01:04:56.580,"본 논문에서는,",3896.58,15
01:04:56.580,01:04:59.340,"RNN 및 CNN을 전부 사용하지 않고,",3899.34,15
01:04:59.420,01:05:01.200,"오직 Attention 기법만 사용해서,",3901.2,15
01:05:01.300,01:05:03.780,기계 번역 테스트에서 좋은 성능을 얻었습니다.,3903.78,15
01:05:04.100,01:05:05.020,"바로 한 번,",3905.02,9
01:05:05.020,01:05:06.300,abstract부터 읽어볼게요.,3906.3,9
01:05:06.680,01:05:07.680,"보시는 바와 같이,",3907.68,6
01:05:07.740,01:05:08.520,"이전까지는,",3908.52,6
01:05:08.520,01:05:10.420,"인코더와 디코더를 포함한 형태로,",3910.42,6
01:05:10.520,01:05:13.300,"복잡한 RNN 혹은 CNN 기반의,",3913.3,6
01:05:13.360,01:05:15.180,"시퀀스 간 변형이 이루어지는,",3915.18,6
01:05:15.280,01:05:17.660,"RNN 및 CNN을 기반으로 하는 모델을,",3917.66,6
01:05:17.660,01:05:18.240,많이 사용했습니다.,3918.24,6
01:05:18.600,01:05:20.300,"여기에서 Transduction은,",3920.3,6
01:05:20.320,01:05:23.260,변화 혹은 형질의 변형과 같은 의미를 가지고 있는데요.,3923.26,6
01:05:23.540,01:05:24.060,"말 그대로,",3924.06,6
01:05:24.240,01:05:26.380,어떠한 시퀀스 간 변형을 의미하는 겁니다.,3926.38,6
01:05:26.580,01:05:27.560,"대표적인 예시가,",3927.56,6
01:05:27.660,01:05:28.640,기계 번역이 있겠죠.,3928.64,6
01:05:28.900,01:05:29.920,"이제 그러한 테스트에서,",3929.92,6
01:05:30.080,01:05:32.160,"이렇게 RNN 혹은 CNN을,",3932.16,6
01:05:32.180,01:05:34.520,전적으로 활용한 모델들이 많이 사용되었습니다.,3934.52,6
01:05:34.840,01:05:37.120,"또한, 그러한 인코더 디코더 아키텍처에,",3937.12,6
01:05:37.260,01:05:38.960,"Attention 메커니즘을 활용했을 때,",3938.96,6
01:05:39.040,01:05:40.760,"보다 성능이 좋아질 수 있었다는,",3940.76,6
01:05:40.880,01:05:43.600,그런 결과를 이전 논문에서 확인해 볼 수가 있었죠.,3943.6,6
01:05:43.700,01:05:45.320,"그래서, 본 논문에선 이렇게,",3945.32,6
01:05:45.580,01:05:47.840,"Transformer란 이름의 아키텍처를 제안하고,",3947.84,6
01:05:48.040,01:05:49.680,"이 아키텍처는 전적으로,",3949.68,6
01:05:49.800,01:05:52.740,Attention 메커니즘에 기반을 하고 있는 아키텍처입니다.,3952.74,6
01:05:52.980,01:05:54.080,"이때, 말 그대로,",3954.08,6
01:05:54.180,01:05:56.200,"Recurrence나 Convolution 자체를,",3956.2,6
01:05:56.200,01:05:57.400,"제거한 형태로,",3957.4,6
01:05:57.420,01:05:59.640,Attention 메커니즘만 활용을 했다는 거고요.,3959.64,6
01:05:59.720,01:06:01.580,"Attention 메커니즘만 활용함으로써,",3961.58,6
01:06:01.640,01:06:05.280,"Recurrence하게 각각의 시퀀스를 처리할 필요가 없어지기 때문에,",3965.28,6
01:06:05.440,01:06:06.900,"그냥 행렬 곡을 이용해서,",3966.9,6
01:06:07.040,01:06:08.160,"완전히 병렬적으로,",3968.16,6
01:06:08.360,01:06:10.380,"시퀀스 데이터를 처리할 수 있기 때문에,",3970.38,6
01:06:10.540,01:06:13.320,훨씬 더 빠르게 처리가 가능하다는 점이 장점입니다.,3973.32,6
01:06:13.500,01:06:14.620,"그래서 결과적으로,",3974.62,6
01:06:14.780,01:06:16.800,"굉장히 유명한 두 가지 태스크인,",3976.8,6
01:06:16.920,01:06:19.500,"WMT-14 용도 데이터셋을 이용해서,",3979.5,6
01:06:19.700,01:06:22.020,"영어를 독일어로 번역하는 태스크,",3982.02,6
01:06:22.120,01:06:24.520,"그리고 영어를 불어로 번역하는 태스크에서,",3984.52,6
01:06:24.680,01:06:26.180,"각각 훨씬 개선된 성능을,",3986.18,6
01:06:26.180,01:06:28.080,보여준 것을 확인할 수 있습니다.,3988.08,6
01:06:28.240,01:06:29.220,"그래서 이와 같이,",3989.22,6
01:06:29.280,01:06:30.500,"두 가지 태스크에 대해서,",3990.5,6
01:06:30.800,01:06:33.360,"완전히 State of the Art의 성능을 보인 것을,",3993.36,6
01:06:33.480,01:06:34.200,확인할 수 있습니다.,3994.2,6
01:06:34.420,01:06:35.400,"또한 이와 같이,",3995.4,6
01:06:35.580,01:06:37.640,"8개의 P100 GPU를 이용해서,",3997.64,6
01:06:37.840,01:06:40.000,"상대적으로 더 적은 시간을 들여서,",4000.0,6
01:06:40.100,01:06:41.720,학습을 마칠 수 있었다고 하고요.,4001.72,6
01:06:41.820,01:06:44.520,"이는 이전까지 제한되었던 모델과 비교했을 때,",4004.52,6
01:06:44.620,01:06:46.720,훨씬 더 학습 효율이 높다고 할 수 있습니다.,4006.72,6
01:06:46.980,01:06:48.080,"이러한 트랜스포머는,",4008.08,6
01:06:48.200,01:06:49.680,"비단 기계 번역뿐만 아니라,",4009.68,6
01:06:50.020,01:06:52.500,"시퀀스 데이터를 처리하는 다양한 태스크에 대해서,",4012.5,6
01:06:52.720,01:06:53.720,"일반화가 가능하고,",4013.72,6
01:06:53.860,01:06:55.880,성능이 잘 나오는 것 또한 보여주었습니다.,4015.88,6
01:06:56.180,01:06:58.580,"대표적으로 구문 분석 분야에서의,",4018.58,6
01:06:58.580,01:07:01.080,"이러한 Constituency Pulsing 태스크에 대해서도,",4021.08,6
01:07:01.160,01:07:03.360,잘 동작하는 걸 확인할 수 있었다고 합니다.,4023.36,6
01:07:03.520,01:07:04.820,"이전에 이와 같이,",4024.82,6
01:07:04.820,01:07:07.980,"RNN, 그리고 LSTM, 그리고 GRLU와 같은,",4027.98,6
01:07:08.000,01:07:09.740,다양한 모델들이 제안이 되었고요.,4029.74,6
01:07:09.880,01:07:12.360,"이러한 네트워크들은 시퀀스 모델링을 위해서,",4032.36,6
01:07:12.660,01:07:14.540,효과적으로 사용이 되고 있었습니다.,4034.54,6
01:07:14.720,01:07:15.720,"다만 기본적으로,",4035.72,6
01:07:15.800,01:07:17.300,"이러한 Recurrent 모델들은,",4037.3,6
01:07:17.400,01:07:19.480,"한 번에 한 단어씩 넣는 방식처럼,",4039.48,6
01:07:19.740,01:07:22.960,"시퀀스에 포함되어 있는 각각의 토큰들에 대한 순서 정보를,",4042.96,6
01:07:23.100,01:07:24.560,"먼저 정렬시킨 뒤에,",4044.56,6
01:07:24.760,01:07:25.940,"이것을 반복적으로,",4045.94,6
01:07:25.940,01:07:26.780,"입력으로 넣어서,",4046.78,6
01:07:26.920,01:07:30.220,이러한 Hidden State 값을 갱신시키는 방법으로 동작을 합니다.,4050.22,6
01:07:30.380,01:07:31.060,"그렇기 때문에,",4051.06,6
01:07:31.120,01:07:33.740,"이런 식으로 Recurrent하게 동작하는 모델인 경우,",4053.74,6
01:07:34.020,01:07:35.140,"시퀀스의 길이,",4055.14,6
01:07:35.140,01:07:36.620,"즉 토큰의 개수만큼,",4056.62,6
01:07:36.740,01:07:39.000,"Neural Network에 입력을 넣어야 되기 때문에,",4059.0,6
01:07:39.180,01:07:42.540,당연히 병렬적인 처리가 어렵다는 문제가 존재하고요.,4062.54,6
01:07:42.640,01:07:43.400,"다시 말해,",4063.4,6
01:07:43.440,01:07:44.740,"Layer의 Output을,",4064.74,6
01:07:44.760,01:07:46.640,"행렬급으로 바로 구할 수 있는 게 아니라,",4066.64,6
01:07:46.840,01:07:49.200,"즉 번역에서는 문장의 길이만큼,",4069.2,6
01:07:49.280,01:07:50.960,"입력을 수행할 필요가 있기 때문에,",4070.96,6
01:07:51.180,01:07:53.380,"이는 메모리 및 속도 측면에서,",4073.38,6
01:07:53.560,01:07:55.020,"비효율성을 야기할 수 있다고,",4075.02,6
01:07:55.020,01:07:56.400,지적하고 있는 것입니다.,4076.4,6
01:07:56.540,01:07:58.900,이어서 Attention Mechanism이 등장을 했었는데요.,4078.9,6
01:07:59.040,01:08:00.640,"Attention Mechanism을 활용하면서,",4080.64,6
01:08:00.740,01:08:03.280,"매번 출력 단어를 만들어낼 때마다,",4083.28,6
01:08:03.440,01:08:05.460,"소스 문장의 출력 정보 중에서,",4085.46,6
01:08:05.600,01:08:07.800,"어떤 정보가 가장 중요한지에 대해서,",4087.8,6
01:08:08.020,01:08:09.280,"가중치를 부여하도록 해서,",4089.28,6
01:08:09.640,01:08:11.860,"그러한 가중치가 적용되어 곱해진,",4091.86,6
01:08:11.940,01:08:13.600,"Hidden State 값을 이용하도록 해서,",4093.6,6
01:08:13.860,01:08:15.480,"출력 단어를 보다 효과적으로,",4095.48,6
01:08:15.620,01:08:17.840,생성할 수 있도록 만들 수 있다고 했죠.,4097.84,6
01:08:18.020,01:08:19.740,"다만 이러한 Attention Mechanism도,",4099.74,6
01:08:19.820,01:08:20.680,"기본적으로,",4100.68,6
01:08:20.760,01:08:22.960,어뢰넨과 같이 사용되는 경우가 많았고요.,4102.96,6
01:08:23.080,01:08:24.140,"그래서 본 논문에서는,",4104.14,6
01:08:24.340,01:08:25.000,"그냥 리뷰,",4105.0,6
01:08:25.000,01:08:26.200,"Recurrence한 특성 자체를,",4106.2,6
01:08:26.280,01:08:27.380,완전하게 제거해버린 겁니다.,4107.38,6
01:08:27.620,01:08:28.480,"즉, 다시 말해서,",4108.48,6
01:08:28.620,01:08:31.300,"완전히 Attention Mechanism에 전적으로 의존해서,",4111.3,6
01:08:31.440,01:08:33.340,모델의 결과를 내보낼 수 있도록 합니다.,4113.34,6
01:08:33.580,01:08:35.080,"Attention Mechanism을 활용하기 때문에,",4115.08,6
01:08:35.400,01:08:36.460,"한 번의 행렬 곡으로,",4116.46,6
01:08:36.560,01:08:38.000,"위치 정보가 포함된,",4118.0,6
01:08:38.060,01:08:40.440,"전체 시퀀스를 한 번에 처리할 수 있다는 점에서,",4120.44,6
01:08:40.680,01:08:41.300,"다시 말해,",4121.3,6
01:08:41.380,01:08:44.060,"순차적으로 입력을 넣지 않아도 되기 때문에,",4124.06,6
01:08:44.340,01:08:46.340,병렬 처리가 가능하다고 볼 수 있는 것입니다.,4126.34,6
01:08:46.560,01:08:48.140,"그래서 이러한 특징을 활용했더니,",4128.14,6
01:08:48.360,01:08:50.480,"성능이 훨씬 좋아진 걸 확인할 수가 있고,",4130.48,6
01:08:50.840,01:08:52.660,"8개의 P100 GPU를 이용해서,",4132.66,6
01:08:52.780,01:08:53.920,"학습을 해본 결과,",4133.92,6
01:08:53.920,01:08:55.200,"현실적인 시간,",4135.2,6
01:08:55.280,01:08:56.540,"즉, 12시간 만에,",4136.54,6
01:08:56.560,01:08:57.500,"상당히 좋은,",4137.5,6
01:08:57.580,01:09:00.500,베이스 모델의 성능을 얻을 수 있었다고 말하고 있습니다.,4140.5,6
01:09:00.760,01:09:01.400,"자, 그래서,",4141.4,6
01:09:01.600,01:09:02.660,"기반이 되고 있는,",4142.66,6
01:09:02.700,01:09:04.660,다양한 백그라운드 논문을 소개하고 있고요.,4144.66,6
01:09:04.960,01:09:06.820,"이 중에서 Self-Attention이라고 하는 것은,",4146.82,6
01:09:06.980,01:09:08.480,"타겟 문장을 만들기 위해서,",4148.48,6
01:09:08.740,01:09:11.320,"소스 문장에서의 히든 정보를 참고하는 것이 아니라,",4151.32,6
01:09:11.680,01:09:12.960,"어떠한 문장이 있을 때,",4152.96,6
01:09:13.180,01:09:14.940,"자기 자신의 문장 스스로에게,",4154.94,6
01:09:15.040,01:09:16.060,"Attention을 수행해서,",4156.06,6
01:09:16.720,01:09:18.840,"Representation을 러닝할 수 있도록 만들어주는 게,",4158.84,6
01:09:18.920,01:09:20.440,Self-Attention Mechanism입니다.,4160.44,6
01:09:20.600,01:09:21.340,"다시 말해,",4161.34,6
01:09:21.420,01:09:22.820,"하나의 시퀀스가 있을 때,",4162.82,6
01:09:22.900,01:09:23.900,"그 시퀀스에 포함해서,",4163.9,6
01:09:23.900,01:09:26.200,"포함되어 있는 서로 다른 위치에 대한 정보가,",4166.2,6
01:09:26.240,01:09:28.940,"서로가 서로에게 가중치를 부여하도록 만들어서,",4168.94,6
01:09:29.100,01:09:31.400,"하나의 시퀀스에 대한 Representation을,",4171.4,6
01:09:31.420,01:09:32.620,"효과적으로 학습하고,",4172.62,6
01:09:32.720,01:09:34.580,표현할 수 있도록 만들어주는 것입니다.,4174.58,6
01:09:34.860,01:09:35.280,"예를 들어,",4175.28,6
01:09:35.460,01:09:37.800,"I am a teacher라는 하나의 문장이 있을 때,",4177.8,6
01:09:38.100,01:09:39.840,"4개의 단어들은 서로가 서로에게,",4179.84,6
01:09:39.940,01:09:40.860,"Attention을 수행해서,",4180.86,6
01:09:41.040,01:09:42.660,가중치를 부여하도록 할 수 있다는 거죠.,4182.66,6
01:09:42.860,01:09:43.420,"자, 그래서,",4183.42,6
01:09:43.540,01:09:44.800,"본 논문에서는 이와 같이,",4184.8,6
01:09:45.080,01:09:48.620,"Transformer는 전적으로 이러한 Attention Mechanism에 기반하는,",4188.62,6
01:09:48.800,01:09:50.060,"사실상 최초의,",4190.06,6
01:09:50.160,01:09:53.600,"시퀀스 간 변형이 가능하도록 만들어준 네트워크라고 할 수 있고,",4193.6,6
01:09:53.600,01:09:55.740,"본 논문은 이러한 아이디어를 통해서,",4195.74,6
01:09:55.800,01:09:58.080,매우 좋은 성능을 이끌어낼 수가 있었고요.,4198.08,6
01:09:58.180,01:09:59.140,"그래서 결과적으로,",4199.14,6
01:09:59.320,01:10:02.200,"비교적 최근에 나온 GPT나 BERT와 같은,",4202.2,6
01:10:02.240,01:10:03.460,"다양한 아키텍처들은,",4203.46,6
01:10:03.600,01:10:07.140,이러한 Transformer에서 제한되었던 아키텍처를 많이 따르고 있습니다.,4207.14,6
01:10:07.480,01:10:07.980,"자, 그래서,",4207.98,6
01:10:08.160,01:10:10.000,"시퀀스 간 변형 모델에 대해서,",4210.0,6
01:10:10.420,01:10:13.280,많은 아키텍처는 Encoder-Decoder 구조를 따르고 있고요.,4213.28,6
01:10:13.500,01:10:15.560,"여기에서 35번 논문 같은 경우는,",4215.56,6
01:10:15.600,01:10:18.020,원본 Sequence-to-Sequence 논문을 의미하고요.,4218.02,6
01:10:18.100,01:10:18.720,"이런 식으로,",4218.72,6
01:10:18.980,01:10:20.680,"X1부터 Xn까지,",4220.68,6
01:10:20.840,01:10:22.660,"총 N개의 토큰으로 구성된,",4222.66,6
01:10:22.780,01:10:23.580,"입력 시퀀스는,",4223.58,6
01:10:23.580,01:10:25.780,"이것을 Continuous한,",4225.78,6
01:10:25.800,01:10:27.340,"어떠한 Embedding Vector로써,",4227.34,6
01:10:27.400,01:10:28.220,"바꾸어주고,",4228.22,6
01:10:28.280,01:10:30.740,"이러한 Embedding Vector인 Z가 들어왔을 때,",4230.74,6
01:10:30.860,01:10:31.840,"이 디코더는,",4231.84,6
01:10:31.840,01:10:33.640,"Y1부터 Ym까지,",4233.64,6
01:10:33.820,01:10:35.600,"총 N개의 토큰으로 구성된,",4235.6,6
01:10:35.640,01:10:37.820,출력 문장을 만드는 방식으로 동작합니다.,4237.82,6
01:10:38.040,01:10:38.940,"이때 기본적으로,",4238.94,6
01:10:39.080,01:10:41.000,"RNN 구조를 따르고 있는 모델들은,",4241.0,6
01:10:41.120,01:10:42.260,"Auto-Regressive하게,",4242.26,6
01:10:42.400,01:10:43.780,"시퀀스의 길이만큼,",4243.78,6
01:10:43.840,01:10:46.160,네트워크의 입력에 주어지는 방식으로 동작합니다.,4246.16,6
01:10:46.340,01:10:47.000,"다시말해,",4247.0,6
01:10:47.020,01:10:49.480,"이전 단계에서 생성되었던 Symbol을 이용해서,",4249.48,6
01:10:49.760,01:10:53.000,다음번에 나올 출력 값을 만드는 방식으로 동작한다는 거죠.,4253.0,6
01:10:53.000,01:10:54.000,"엄밀히 말하면,",4254.0,6
01:10:54.100,01:10:55.600,"Transformer 또한 마찬가지로,",4255.6,6
01:10:55.600,01:10:58.080,"인코더와 디코더 파트로 구성이 되어 있으며,",4258.08,6
01:10:58.180,01:11:00.700,이러한 기본적인 아키텍처는 활용하고 있습니다.,4260.7,6
01:11:00.940,01:11:02.400,"단지 다른 점이라고 한다면,",4262.4,6
01:11:02.600,01:11:04.400,"모델을 Recurrent하게 이용하진 않고,",4264.4,6
01:11:04.780,01:11:06.320,"Attention Mechanism만 활용해서,",4266.32,6
01:11:06.500,01:11:08.360,"시퀀스에 대한 정보를 한 번에,",4268.36,6
01:11:08.380,01:11:10.960,입력으로 준다는 점이 그 특징이라고 할 수 있습니다.,4270.96,6
01:11:11.240,01:11:13.160,"자, 그래서 여기 보이는 그림이,",4273.16,10
01:11:13.160,01:11:15.380,전체 Transformer의 아키텍처라고 할 수 있는데요.,4275.38,10
01:11:15.740,01:11:16.560,"자, 이와 같이,",4276.56,10
01:11:16.700,01:11:18.460,"RNN을 사용하지 않는 대신에,",4278.46,10
01:11:18.540,01:11:21.580,"문장 내에 포함되어 있는 각각의 단어들의 위치 정보를,",4281.58,10
01:11:21.660,01:11:22.360,"인코딩해서,",4282.36,10
01:11:22.480,01:11:22.980,입력할게요.,4282.98,10
01:11:22.980,01:11:23.180,"이렇게 입력을 하기 위해,",4283.18,10
01:11:23.360,01:11:25.000,Positional Encoding을 사용하고요.,4285.0,10
01:11:25.060,01:11:27.380,"이렇게 입력 임베딩과 같은 디멘션으로,",4287.38,10
01:11:27.440,01:11:28.560,"합치기를 수행해서,",4288.56,10
01:11:28.660,01:11:32.000,이렇게 만들어진 결과를 실제 임베딩 Vector로써 사용을 하고요.,4292.0,10
01:11:32.080,01:11:32.940,"이제 얘가 이렇게,",4292.94,10
01:11:33.120,01:11:35.600,"Query, Key, Value 값으로 각각 복제가 되어,",4295.6,10
01:11:35.740,01:11:36.480,입력됩니다.,4296.48,10
01:11:36.720,01:11:38.360,"여기 보이는 Multihead Attention은,",4298.36,10
01:11:38.380,01:11:39.960,"Self Attention으로 동작을 하며,",4299.96,10
01:11:40.080,01:11:40.880,"보시는 바와 같이,",4300.88,10
01:11:40.960,01:11:42.860,Query와 Key와 Value 값이 모두 동일합니다.,4302.86,10
01:11:43.080,01:11:45.960,또한 Attention은 입력과 출력이 차원에 같다고 말씀드렸죠.,4305.96,10
01:11:46.100,01:11:46.940,"그래서 결과적으로,",4306.94,10
01:11:47.080,01:11:48.980,"이렇게 들어갈 때의 차원과,",4308.98,10
01:11:49.060,01:11:51.120,이 Attention을 나왔을 때의 차원은 동일하구요.,4311.12,10
01:11:51.340,01:11:52.020,"마찬가지로,",4312.02,10
01:11:52.020,01:11:53.600,"residual connection을 수행한 뒤에,",4313.6,10
01:11:53.640,01:11:55.180,"정규화를 수행해주고,",4315.18,10
01:11:55.220,01:11:57.000,"이렇게 Feedforward Layer를 거치고,",4317.0,10
01:11:57.120,01:11:58.800,"다시 한 번 정규화를 수행할 때까지,",4318.8,10
01:11:58.880,01:12:01.360,"전부 다 각각의 Layer에 대한 입력과 출력이,",4321.36,10
01:12:01.380,01:12:02.760,디멘션은 같다고 보시면 됩니다.,4322.76,10
01:12:02.980,01:12:04.480,"그래서 이제 이러한 과정 자체를,",4324.48,10
01:12:04.540,01:12:05.980,"총 N번만큼 반복해서,",4325.98,10
01:12:06.200,01:12:08.060,"총 N개의 Encoder Layer가,",4328.06,10
01:12:08.080,01:12:10.300,"차곡차곡 반복적으로 수행이 돼서,",4330.3,10
01:12:10.440,01:12:12.220,"마지막에 나온 그 출력 값을,",4332.22,10
01:12:12.300,01:12:12.900,"이와 같이,",4332.9,10
01:12:13.000,01:12:14.740,"매 Encoder, Decoder, Attention에서,",4334.74,10
01:12:14.820,01:12:16.960,사용할 수 있도록 만든다고 보시면 되겠습니다.,4336.96,10
01:12:17.200,01:12:18.600,"또한 이렇게 Decoder 파트에서는,",4338.6,10
01:12:18.880,01:12:20.400,"지금까지 출력된 단어만,",4340.4,10
01:12:20.400,01:12:21.860,"Attention 할 수 있도록 하기 위해서,",4341.86,10
01:12:21.860,01:12:23.180,"학습을 수행할 때,",4343.18,10
01:12:23.220,01:12:24.420,"이렇게 마스크를 씌워서,",4344.42,10
01:12:24.560,01:12:27.760,"뒤쪽에 있는 단어는 미리 알지 못하도록 막는 방식으로,",4347.76,10
01:12:27.780,01:12:30.980,모델이 정상적인 데이터만을 학습할 수 있도록 만들어주구요.,4350.98,10
01:12:31.260,01:12:32.400,"마찬가지로 여기 보이는,",4352.4,10
01:12:32.500,01:12:34.060,"이 Decoder의 첫 번째 Attention에서는,",4354.06,10
01:12:34.240,01:12:36.000,"Query와 Key, Value의 값이 같기 때문에,",4356.0,10
01:12:36.120,01:12:37.780,Self-Attention이 수행된다고 할 수 있습니다.,4357.78,10
01:12:37.980,01:12:39.340,"이렇게 두 번째 Attention에서는,",4359.34,10
01:12:39.580,01:12:41.640,"이 Query의 값이 Decoder에 있기 때문에,",4361.64,10
01:12:41.800,01:12:43.740,"각각의 출력 단어를 만들기 위해서,",4363.74,10
01:12:43.960,01:12:46.940,"이 Encoder 파트에서 어떠한 정보를 참고하면 좋은지를,",4366.94,10
01:12:47.040,01:12:48.620,Attention이 수행한다고 할 수 있는 겁니다.,4368.62,10
01:12:48.840,01:12:49.580,"즉, 다시 말해,",4369.58,10
01:12:49.620,01:12:51.080,"이 Key와 Value의 값들은,",4371.08,10
01:12:51.100,01:12:51.840,"Encoder 파트에서,",4371.84,10
01:12:51.840,01:12:52.480,받습니다.,4372.48,10
01:12:52.680,01:12:55.000,"그래서 마찬가지로 Feed-forward Layer를 거친 뒤에,",4375.0,10
01:12:55.100,01:12:56.880,"마지막에 Linear Layer를 거치고,",4376.88,10
01:12:56.920,01:12:57.980,"Softmax를 취해서,",4377.98,10
01:12:58.140,01:13:00.880,"실제로 각각의 출력 문장에 포함된 단어들이,",4380.88,10
01:13:00.980,01:13:02.860,"실제로 어떤 단어에 해당하는지,",4382.86,10
01:13:02.880,01:13:04.120,구할 수 있도록 할 수 있습니다.,4384.12,10
01:13:04.320,01:13:05.620,"또 이제 여기에서 추가적으로,",4385.62,10
01:13:05.740,01:13:07.140,"Label Smoothing을 적용해서,",4387.14,10
01:13:07.400,01:13:08.540,"정규화 효과를 더해서,",4388.54,10
01:13:08.680,01:13:10.120,성능을 더 높일 수 있습니다.,4390.12,10
01:13:10.340,01:13:12.340,"그래서 이러한 Transformer의 아키텍처는,",4392.34,10
01:13:12.400,01:13:14.640,향후 많은 논문에 영향을 미치게 되었습니다.,4394.64,10
01:13:15.160,01:13:17.020,"자, 그래서 설명했던 내용과 동일하게,",4397.02,6
01:13:17.160,01:13:18.500,"Encoder 파트는 기본적으로,",4398.5,6
01:13:18.620,01:13:20.040,"여러 번 Encoder Layer가,",4400.04,6
01:13:20.040,01:13:21.460,중첩이 되어 사용이 되구요.,4401.46,6
01:13:21.460,01:13:22.500,"보시는 바와 같이,",4402.5,6
01:13:22.540,01:13:24.080,"본 논문에서는 총 6번,",4404.08,6
01:13:24.140,01:13:25.600,"Encoder Layer를 중첩해서,",4405.6,6
01:13:25.680,01:13:26.940,사용할 수 있다고 말했구요.,4406.94,6
01:13:27.080,01:13:27.660,"이와 같이,",4407.66,6
01:13:27.680,01:13:28.980,"residual connection을 이용해서,",4408.98,6
01:13:29.180,01:13:30.680,"normalization을 거치기 전에,",4410.68,6
01:13:30.820,01:13:31.700,"그 입력 값으로,",4411.7,6
01:13:31.820,01:13:33.600,"identity mapping을 수행할 수 있도록,",4413.6,6
01:13:33.680,01:13:34.400,만들어 주었구요.,4414.4,6
01:13:34.620,01:13:35.420,"또한 이와 같이,",4415.42,6
01:13:35.560,01:13:36.780,"embedding vector의 차원은,",4416.78,6
01:13:36.800,01:13:39.060,512차원으로 설정했다고 말하고 있습니다.,4419.06,6
01:13:39.460,01:13:40.060,"마찬가지로,",4420.06,6
01:13:40.060,01:13:41.740,"Decoder에서도 총 6개의,",4421.74,6
01:13:41.740,01:13:43.660,"Decoder Layer를 쌓을 수 있도록 만들었고,",4423.66,6
01:13:43.840,01:13:45.440,"실제 구현상에서 이와 같이,",4425.44,6
01:13:45.580,01:13:47.240,"Encoder와 같은 Layer의 개수를,",4427.24,6
01:13:47.360,01:13:48.800,가지도록 만드는 경우가 많습니다.,4428.8,6
01:13:49.000,01:13:50.960,"그래서 Multi-Add Attention을 수행할 때,",4430.96,6
01:13:50.960,01:13:51.820,"Encoder의,",4431.82,6
01:13:51.840,01:13:52.860,"Output 값에 대해서,",4432.86,6
01:13:53.000,01:13:54.960,Attention을 수행할 수 있도록 만든다고 했구요.,4434.96,6
01:13:55.240,01:13:56.480,"마찬가지로 동일하게,",4436.48,6
01:13:56.560,01:13:57.800,"residual connection을 사용해서,",4437.8,6
01:13:57.980,01:13:59.260,"학습 난이도를 낮추어서,",4439.26,6
01:13:59.440,01:14:02.140,"보다 좋은 글로벌 Optima를 찾을 수 있도록,",4442.14,6
01:14:02.260,01:14:03.200,모델을 설계했습니다.,4443.2,6
01:14:03.500,01:14:04.960,"또한 Decoder 파트에서 쓰이는,",4444.96,6
01:14:05.060,01:14:06.020,"Self-Attention에서는,",4446.02,6
01:14:06.380,01:14:08.760,"이전에 등장한 단어들만 참고할 수 있는 형태로,",4448.76,6
01:14:08.860,01:14:09.620,"마스크를 씌워서,",4449.62,6
01:14:09.840,01:14:11.160,"마스크가 붙은 형태의,",4451.16,6
01:14:11.160,01:14:12.900,"Multi-Add Attention을 사용할 수 있도록,",4452.9,6
01:14:12.980,01:14:13.780,만들었다고 합니다.,4453.78,6
01:14:14.060,01:14:15.020,"자 그래서 여기에서는,",4455.02,6
01:14:15.180,01:14:17.060,Attention Mechanism에 대해 설명하고 있는데요.,4457.06,6
01:14:17.440,01:14:18.680,"이때 하나의 Query는 말 그대로,",4458.68,6
01:14:18.840,01:14:20.020,어떠한 질문을 날리는 겁니다.,4460.02,6
01:14:20.220,01:14:20.940,"특정 키의,",4460.94,6
01:14:20.940,01:14:22.420,에게 물어보는 것과 같습니다.,4462.42,6
01:14:22.660,01:14:24.860,"즉 이때 Query라고 하는 것은 말씀드렸듯이,",4464.86,6
01:14:24.860,01:14:26.540,질문을 하는 주체라고 할 수 있구요.,4466.54,6
01:14:26.680,01:14:27.460,"이때 이 키는,",4467.46,6
01:14:27.500,01:14:29.360,Attention을 수행할 대상이라고 할 수 있습니다.,4469.36,6
01:14:29.620,01:14:30.380,"예를 들어서,",4470.38,6
01:14:30.540,01:14:31.420,"사랑해라는,",4471.42,6
01:14:31.480,01:14:33.220,"하나의 단어가 생성되기 위해서,",4473.22,6
01:14:33.480,01:14:34.560,"I love you라는,",4474.56,6
01:14:34.680,01:14:36.480,"문장에 포함된 단어들 중에,",4476.48,6
01:14:36.680,01:14:39.820,"어떤 단어가 가장 중요했는지를 물어보는 방식으로,",4479.82,6
01:14:39.920,01:14:41.020,"각각의 Query가,",4481.02,6
01:14:41.100,01:14:41.860,"키에 대해서,",4481.86,6
01:14:42.020,01:14:44.360,Attention을 수행하는 Mechanism으로 이해할 수 있습니다.,4484.36,6
01:14:44.580,01:14:46.160,"그래서 여기 보이는 그림이,",4486.16,10
01:14:46.180,01:14:47.740,"실제로 Multi-Add Attention을,",4487.74,10
01:14:47.760,01:14:49.080,잘 설명하고 있는 그림이구요.,4489.08,10
01:14:49.260,01:14:50.580,"이러한 Multi-Add Attention은,",4490.58,10
01:14:50.580,01:14:52.800,"각각의 인코더와 디코더 레이어에서,",4492.8,10
01:14:52.940,01:14:53.680,사용된다고 했습니다.,4493.68,10
01:14:54.000,01:14:55.360,"이때 Multi-Add Attention은,",4495.36,10
01:14:55.380,01:14:55.980,"내부적으로,",4495.98,10
01:14:56.120,01:14:58.060,Scaled.Product Attention을 가지고 있는데요.,4498.06,10
01:14:58.380,01:15:00.500,얘는 바로 왼쪽에 있는 그림과 같이 생겼습니다.,4500.5,10
01:15:00.840,01:15:01.300,"자 이때 이렇게,",4501.3,10
01:15:01.580,01:15:03.200,"Query와 키와 Value가 들어오게 되면,",4503.2,10
01:15:03.440,01:15:04.480,"각각의 Query가,",4504.48,10
01:15:04.500,01:15:05.140,"이 키에 대해서,",4505.14,10
01:15:05.420,01:15:07.040,"질문을 하는 내용이 바로 이렇게,",4507.04,10
01:15:07.300,01:15:08.800,행렬 고부로 이루어지구요.,4508.8,10
01:15:08.980,01:15:09.340,"또한,",4509.34,10
01:15:09.340,01:15:10.920,"Softmax에 들어가는 값에 대해서,",4510.92,10
01:15:11.120,01:15:12.100,"스케일링을 하기 위해,",4512.1,10
01:15:12.240,01:15:13.440,Scale Layer를 포함합니다.,4513.44,10
01:15:13.720,01:15:14.800,"이때 Scale Layer는,",4514.8,10
01:15:14.900,01:15:16.540,"여기 들어오는 키의 차원에,",4516.54,10
01:15:16.600,01:15:17.800,"루트를 씌운 값을,",4517.8,10
01:15:17.840,01:15:19.280,나눠줄 수 있는 형태로 사용합니다.,4519.28,10
01:15:19.500,01:15:20.560,"또한 이렇게 MaskValue,",4520.56,10
01:15:20.580,01:15:21.400,"Vector 같은 경우는,",4521.4,10
01:15:21.400,01:15:22.240,"필요할 때,",4522.24,10
01:15:22.300,01:15:23.100,사용을 하고요.,4523.1,10
01:15:23.240,01:15:24.160,"그래서 이와 같이,",4524.16,10
01:15:24.240,01:15:25.280,"Softmax를 취해서,",4525.28,10
01:15:25.400,01:15:26.380,"각각의 키에 대해서,",4526.38,10
01:15:26.660,01:15:28.300,"얼마나 중요한지에 대한 값을,",4528.3,10
01:15:28.420,01:15:30.280,"확률 형태로 표현할 수 있도록 한 뒤에,",4530.28,10
01:15:30.560,01:15:32.100,"이제 그러한 확률 값을 각각,",4532.1,10
01:15:32.100,01:15:33.300,"실제 Value와 곱해서,",4533.3,10
01:15:33.600,01:15:34.760,"Attention Value 값을,",4534.76,10
01:15:34.820,01:15:35.760,만들어 낼 수 있는 겁니다.,4535.76,10
01:15:36.040,01:15:37.080,"이제 이러한 과정들이,",4537.08,10
01:15:37.160,01:15:38.240,"각각의 Head마다,",4538.24,10
01:15:38.240,01:15:39.880,"서로 다르게 이루어진 뒤에,",4539.88,10
01:15:40.080,01:15:41.520,"다시 이렇게 결과를 합쳐서,",4541.52,10
01:15:41.700,01:15:43.060,"Linear Layer를 거친 뒤에,",4543.06,10
01:15:43.300,01:15:44.860,Output 값을 내보낸다고 할 수 있습니다.,4544.86,10
01:15:45.280,01:15:46.700,"자 이때 실제 구현상으로는,",4546.7,10
01:15:46.780,01:15:48.080,"이렇게 입력 값이 들어왔을 때,",4548.08,10
01:15:48.200,01:15:49.020,"입력 값은,",4549.02,10
01:15:49.080,01:15:49.800,"이 V와,",4549.8,10
01:15:49.800,01:15:50.460,"K와,",4550.46,10
01:15:50.460,01:15:51.280,"Key에 대해서,",4551.28,10
01:15:51.360,01:15:52.420,"각각 복제가 되어서,",4552.42,10
01:15:52.500,01:15:53.460,"들어가도록 할 수 있고,",4553.46,10
01:15:53.760,01:15:54.120,"이때,",4554.12,10
01:15:54.300,01:15:55.780,"각각의 Linear Layer는,",4555.78,10
01:15:55.840,01:15:56.760,"Embedding 차원을,",4556.76,10
01:15:56.800,01:15:59.140,"Key, Query, Value의 차원으로 바꿔줍니다.",4559.14,10
01:15:59.360,01:16:00.980,"이제 그렇게 나온 값들을 각각,",4560.98,10
01:16:00.980,01:16:02.140,"Attention을 수행한 뒤에,",4562.14,10
01:16:02.360,01:16:03.560,"동일한 차원이 나오게 되면,",4563.56,10
01:16:03.680,01:16:05.000,"다시 얘네들을 묶어주어서,",4565.0,10
01:16:05.200,01:16:06.840,"기존의 Embedding 차원과 결과적으로,",4566.84,10
01:16:07.000,01:16:08.040,"같은 차원이 될 수 있도록,",4568.04,10
01:16:08.080,01:16:08.820,만들어 준다는 거죠.,4568.82,10
01:16:09.000,01:16:10.680,"또한 여기에서 실제로 구현할 때는,",4570.68,10
01:16:10.980,01:16:11.580,"예를 들어서,",4571.58,10
01:16:11.960,01:16:13.660,"이렇게 입력으로 들어오는 Embedding 차원이,",4573.66,10
01:16:13.760,01:16:14.800,"512라고 하고,",4574.8,10
01:16:15.100,01:16:16.600,"이 H가 8이라고 한다면,",4576.6,10
01:16:16.900,01:16:18.100,"각각의 Linear Layer는,",4578.1,10
01:16:18.140,01:16:19.780,64차원으로 Mapping을 해준다는 거죠.,4579.78,10
01:16:19.780,01:16:22.240,"다만 여기에서 실제로 구현할 때는,",4582.24,10
01:16:22.320,01:16:24.200,"그냥 512 곱하기 512로,",4584.2,10
01:16:24.240,01:16:26.760,"병렬적으로 그냥 한 번에 행렬급을 구한 뒤에,",4586.76,10
01:16:26.820,01:16:27.880,"그 결과 값을,",4587.88,10
01:16:27.940,01:16:29.900,8개로 쪼개서 사용할 수도 있습니다.,4589.9,10
01:16:30.200,01:16:32.440,"자 그래서 실제로 Attention Mechanism에서,",4592.44,6
01:16:32.500,01:16:33.120,"핵심이 되는,",4593.12,6
01:16:33.240,01:16:34.820,"Scaled.ProductAttention에 대해서,",4594.82,6
01:16:35.000,01:16:35.620,설명하고 있는데요.,4595.62,6
01:16:36.020,01:16:37.040,"자 확인해 보시면 이렇게,",4597.04,6
01:16:37.180,01:16:38.520,"실제로는 행렬 형태로,",4598.52,6
01:16:38.640,01:16:40.580,"한 번에 Query와 Key들을 묶어서,",4600.58,6
01:16:40.760,01:16:42.520,병렬적으로 계산할 수가 있고요.,4602.52,6
01:16:42.900,01:16:44.380,"말씀드린 내용과 마찬가지로,",4604.38,6
01:16:44.460,01:16:46.120,먼저 Query랑 Key랑 곱하고요.,4606.12,6
01:16:46.220,01:16:49.140,"이때 Query와 Key는 기본적으로 차원이 같도록 만들어서,",4609.14,6
01:16:49.240,01:16:49.760,"이와 같이,",4609.76,6
01:16:49.760,01:16:51.380,곱셈이 수행될 수 있도록 만듭니다.,4611.38,6
01:16:51.400,01:16:52.080,"이제 그래서,",4612.08,6
01:16:52.220,01:16:53.960,"Scale Factor만큼 나눠준 뒤에,",4613.96,6
01:16:54.020,01:16:54.980,"확률 값을 구하고,",4614.98,6
01:16:55.100,01:16:55.800,"얘를 실제로,",4615.8,6
01:16:56.000,01:16:57.000,"이 Value 값과,",4617.0,6
01:16:57.080,01:16:58.760,행렬급을 수행할 수 있다고 했죠.,4618.76,6
01:16:58.960,01:17:00.540,"자 그래서 이러한 방식은,",4620.54,6
01:17:00.620,01:17:03.100,Additive Attention과는 약간 구별되는 방식인데요.,4623.1,6
01:17:03.460,01:17:04.540,"바로 여기 확인해 보시면 이렇게,",4624.54,6
01:17:04.880,01:17:06.600,"Key랑 Query를 그냥 곱해가지고,",4626.6,6
01:17:06.780,01:17:08.160,한 번에 Attention을 구할 수가 있습니다.,4628.16,6
01:17:08.500,01:17:09.920,"사실 우리가 앞서 확인했던,",4629.92,6
01:17:10.180,01:17:11.460,"Sequence to Sequence에,",4631.46,6
01:17:11.520,01:17:13.080,"Attention Mechanism을 활용했을 때는,",4633.08,6
01:17:13.260,01:17:14.300,"이 Query와 Key가,",4634.3,6
01:17:14.380,01:17:15.680,"특정한 행렬급에,",4635.68,6
01:17:15.720,01:17:17.900,함께 입력되는 형태로 동작을 했었는데요.,4637.9,6
01:17:18.100,01:17:18.680,"여기서는 그냥,",4638.68,6
01:17:18.680,01:17:20.080,"Query와 Key를 바로,",4640.08,6
01:17:20.160,01:17:21.520,"서로 곱하도록 만들어서,",4641.52,6
01:17:21.660,01:17:24.460,Not Product Attention 형태로 사용했다고 볼 수 있고요.,4644.46,6
01:17:24.680,01:17:25.700,"논문에선 이와 같이,",4645.7,6
01:17:25.800,01:17:27.620,"Not Product Attention을 사용했을 때,",4647.62,6
01:17:27.720,01:17:30.160,"단순히 내적을 이용해서 Attention을 수행하는 것이,",4650.16,6
01:17:30.280,01:17:31.700,"Practically 빠르고,",4651.7,6
01:17:31.780,01:17:33.440,공간 효율적이었다고 말하고 있습니다.,4653.44,6
01:17:33.660,01:17:34.180,"또한 이렇게,",4654.18,6
01:17:34.460,01:17:36.060,"내적을 이용하는 방식인 경우,",4656.06,6
01:17:36.320,01:17:37.360,"스케일링을 하지 않으면,",4657.36,6
01:17:37.500,01:17:38.960,결과가 많이 안 좋았다고 하는데요.,4658.96,6
01:17:39.260,01:17:40.840,"본 논문에서는 그러한 이유를,",4660.84,6
01:17:40.920,01:17:42.520,다음과 같이 추측하고 있습니다.,4662.52,6
01:17:42.860,01:17:44.480,"바로 Softmax 같은 경우는,",4664.48,6
01:17:44.640,01:17:45.740,"이 중간 부분이,",4665.74,6
01:17:45.940,01:17:47.560,"Gradient가 상대적으로 크고,",4667.56,6
01:17:47.760,01:17:48.660,"Side로 가면,",4668.66,6
01:17:48.660,01:17:50.340,"갈수록 Gradient가 작아지는,",4670.34,6
01:17:50.400,01:17:51.520,특징을 가지고 있는데요.,4671.52,6
01:17:51.800,01:17:52.600,"그렇기 때문에,",4672.6,6
01:17:52.640,01:17:53.900,"값이 너무 커지거나 하면,",4673.9,6
01:17:54.140,01:17:55.960,"너무 Gradient가 작아질 수 있어서,",4675.96,6
01:17:56.100,01:17:57.260,학습이 잘 안 될 수가 있겠죠.,4677.26,6
01:17:57.460,01:17:58.160,"그렇기 때문에,",4678.16,6
01:17:58.300,01:18:00.420,"특정 Scale Factor만큼 곱해주어서,",4680.42,6
01:18:00.600,01:18:01.780,"값을 작게 만들어,",4681.78,6
01:18:01.940,01:18:03.980,학습이 잘 이루어질 수 있도록 만드는 것입니다.,4683.98,6
01:18:04.200,01:18:05.280,"또한 말씀드렸듯이,",4685.28,6
01:18:05.300,01:18:06.640,"기존의 Embedding 차원을,",4686.64,6
01:18:06.660,01:18:07.900,"D-Model이라고 했을 때,",4687.9,6
01:18:08.040,01:18:09.180,"얘를 H만큼,",4689.18,6
01:18:09.260,01:18:10.720,"즉, Head의 개수만큼 나누어서,",4690.72,6
01:18:10.880,01:18:12.100,"각각 Key나 Value,",4692.1,6
01:18:12.320,01:18:13.000,"Query와 같은,",4693.0,6
01:18:13.120,01:18:14.740,차원들을 결정할 수 있다고 했고요.,4694.74,6
01:18:14.920,01:18:16.420,"이러한 Vector들은 나중에 다시,",4696.42,6
01:18:16.660,01:18:17.760,"이어붙여지기 때문에,",4697.76,6
01:18:17.760,01:18:18.760,"결과적으로는,",4698.76,6
01:18:18.760,01:18:20.180,"입력과 출력의 Dimension이,",4700.18,6
01:18:20.220,01:18:21.320,같도록 만들 수 있습니다.,4701.32,6
01:18:21.560,01:18:22.320,"자, 그래서,",4702.32,6
01:18:22.480,01:18:23.560,"Multi-Head Attention은,",4703.56,6
01:18:23.580,01:18:25.360,"말 그대로 Head가 여러 개란 의미라서,",4705.36,6
01:18:25.640,01:18:27.700,이렇게 Multi-Head라고 이름이 붙은 거고요.,4707.7,6
01:18:27.840,01:18:28.420,"이와 같이,",4708.42,6
01:18:28.520,01:18:29.800,"각각의 Head에 대해서,",4709.8,6
01:18:29.960,01:18:31.840,"전부 다 Attention을 수행한 값을,",4711.84,6
01:18:31.940,01:18:33.520,"다시 이렇게 이어붙인 뒤에,",4713.52,6
01:18:33.840,01:18:35.280,"Output 값을 내보내기 위해서,",4715.28,6
01:18:35.460,01:18:36.240,행렬급을 수행합니다.,4716.24,6
01:18:36.560,01:18:38.040,"이제 여기에서 이 i 값은,",4718.04,6
01:18:38.140,01:18:40.020,각각의 Head에 대한 Index라고 할 수 있고요.,4720.02,6
01:18:40.220,01:18:41.060,"그래서 이와 같이,",4721.06,6
01:18:41.160,01:18:43.140,"실제로 Query나 Key를 만들기 위한,",4723.14,6
01:18:43.220,01:18:43.920,"행렬의 크기는,",4723.92,6
01:18:44.080,01:18:46.400,D-Model 곱하기 DK가 사용이 되고요.,4726.4,6
01:18:46.560,01:18:47.360,"다시 말해서,",4727.36,6
01:18:47.360,01:18:49.720,"D-Model 차원의 Embedding Vector를,",4729.72,6
01:18:49.760,01:18:52.760,"DK 차원의 Query와 Key Vector의 차원으로,",4732.76,6
01:18:52.840,01:18:54.140,만들 수 있도록 하는 것입니다.,4734.14,6
01:18:54.380,01:18:55.440,"물론 말씀드렸듯이,",4735.44,6
01:18:55.440,01:18:56.680,"실제로 구현할 때는 그냥,",4736.68,6
01:18:56.840,01:18:58.940,"D-Model 곱하기 D-Model만큼의,",4738.94,6
01:18:58.940,01:18:59.880,"행렬을 곱한 뒤에,",4739.88,6
01:19:00.040,01:19:01.660,"그냥 결과 값 자체를 나누어서,",4741.66,6
01:19:01.760,01:19:02.560,사용할 수도 있는 거예요.,4742.56,6
01:19:02.760,01:19:03.800,"그래서 본 논문에서는,",4743.8,6
01:19:04.060,01:19:05.040,"D-Model을 512,",4745.04,6
01:19:05.380,01:19:06.960,"그리고 H를 8로 설정해서,",4746.96,6
01:19:07.220,01:19:09.800,Query Vector의 차원을 64라고 설정을 했고요.,4749.8,6
01:19:09.920,01:19:10.800,"또한 엄밀히 말하면,",4750.8,6
01:19:10.920,01:19:12.160,"이 Value 값 같은 경우는,",4752.16,6
01:19:12.260,01:19:13.860,"차원을 Query 혹은 Key와,",4753.86,6
01:19:13.880,01:19:15.080,"똑같이 맞출 필요는 없지만,",4755.08,6
01:19:15.340,01:19:15.740,"여기선 이렇게,",4755.74,6
01:19:16.000,01:19:17.340,"Query, Key, Value 전부 다,",4757.34,6
01:19:17.340,01:19:19.040,"같은 차원인 64 차원으로,",4759.04,6
01:19:19.100,01:19:20.600,사용할 수 있다고 말하고 있습니다.,4760.6,6
01:19:20.820,01:19:22.720,"자 그래서 이제 이러한 Attention이,",4762.72,6
01:19:22.740,01:19:24.520,"실제로 어디에 쓰이는지 확인해 보시면은,",4764.52,6
01:19:24.560,01:19:25.640,"앞서 말씀드렸듯이,",4765.64,6
01:19:25.680,01:19:27.740,총 3가지 위치에서 사용이 되는데요.,4767.74,6
01:19:27.980,01:19:29.360,"모든 Attention은 기본적으로,",4769.36,6
01:19:29.440,01:19:30.620,"다 Head가 여러개인,",4770.62,6
01:19:30.660,01:19:31.860,"Multi-Head Attention이며,",4771.86,6
01:19:32.000,01:19:33.260,"사용되는 위치에 따라서,",4773.26,6
01:19:33.280,01:19:34.740,3가지로 구분되는 것입니다.,4774.74,6
01:19:34.980,01:19:36.520,"먼저 Encoder Decoder Attention은,",4776.52,6
01:19:36.600,01:19:38.560,Decoder 파트에서 사용이 되는 거고요.,4778.56,6
01:19:38.740,01:19:41.060,"이때 이 Query는 이 Decoder에서 오는 것이고,",4781.06,6
01:19:41.320,01:19:42.900,"이때 이 Key와 Value 값은,",4782.9,6
01:19:43.000,01:19:45.420,Encoder의 출력 파트에서 가져온다고 했어요.,4785.42,6
01:19:45.660,01:19:46.820,"이 내용은 다시 말하면,",4786.82,6
01:19:46.820,01:19:48.680,"우리가 출력 단어를 만들기 위해서,",4788.68,6
01:19:48.820,01:19:51.120,"소스 문장에 포함되어 있는 단어들 중에서,",4791.12,6
01:19:51.300,01:19:52.240,"어떤 정보에,",4792.24,6
01:19:52.320,01:19:54.140,"보다 초점을 맞추면 되는지를,",4794.14,6
01:19:54.220,01:19:55.580,"계산하는 과정이라고,",4795.58,6
01:19:55.720,01:19:56.740,비유할 수 있다고 했죠.,4796.74,6
01:19:56.920,01:19:59.080,"이어서 Self-Attention은 기본적으로,",4799.08,6
01:19:59.320,01:20:01.200,"Query와 Key와 Value가 모두 같은,",4801.2,6
01:20:01.260,01:20:02.380,형태를 의미하고요.,4802.38,6
01:20:02.500,01:20:04.620,바로 Encoder 파트에서 그대로 사용이 됩니다.,4804.62,6
01:20:04.860,01:20:06.640,"그리고 Decoder 파트 또한 마찬가지로,",4806.64,6
01:20:06.640,01:20:08.740,"맨 처음에 입력 인베딩이 들어왔을 때,",4808.74,6
01:20:08.880,01:20:10.660,Self-Attention을 수행할 수 있다고 했는데요.,4810.66,6
01:20:10.900,01:20:12.200,"이제 여기서는 마스크를 씌워서,",4812.2,6
01:20:12.420,01:20:13.100,"다시 말해,",4813.1,6
01:20:13.160,01:20:14.820,"Softmax에 들어가는 값이,",4814.82,6
01:20:14.900,01:20:16.460,"Minus 무한이 될 수 있도록 해서,",4816.46,6
01:20:16.460,01:20:18.760,"0%가 부여될 수 있도록 하여,",4818.76,6
01:20:18.820,01:20:19.800,"각각의 단어가,",4819.8,6
01:20:19.900,01:20:21.720,"앞부분에 있는 단어에 대한 정보만,",4821.72,6
01:20:21.740,01:20:23.180,"참고할 수 있도록 만들었다고,",4823.18,6
01:20:23.280,01:20:24.040,보시면 되겠습니다.,4824.04,6
01:20:24.340,01:20:25.140,"또한 이와 같이,",4825.14,6
01:20:25.320,01:20:27.080,"Position-Wise Feed-For-All 네트워크를,",4827.08,6
01:20:27.140,01:20:27.980,사용할 수 있다고 했는데요.,4827.98,6
01:20:28.300,01:20:30.200,"여기 보이시는 이 Max-Function 텀은,",4830.2,6
01:20:30.360,01:20:31.880,"Relo Activation에 대한 내용을,",4831.88,6
01:20:31.900,01:20:32.720,보여주고 있는 것입니다.,4832.72,6
01:20:33.020,01:20:33.920,"그래서 이와 같이,",4833.92,6
01:20:34.080,01:20:35.520,"입력 값과 출력 값은,",4835.52,6
01:20:35.540,01:20:37.400,모두 같은 차원을 가지게 되고요.,4837.4,6
01:20:37.560,01:20:38.540,"이렇게 중간에,",4838.54,6
01:20:38.560,01:20:39.360,"Hidden Dimension으로,",4839.36,6
01:20:39.480,01:20:40.880,"약간 고차원 공간에,",4840.88,6
01:20:40.900,01:20:41.700,"맵핑이 되었다가,",4841.7,6
01:20:41.940,01:20:43.040,"출력 레이어를 통해서,",4843.04,6
01:20:43.260,01:20:44.760,"Feed-For-All가 수행될 수 있도록,",4844.76,6
01:20:44.760,01:20:46.120,만든다고 보시면 되겠습니다.,4846.12,6
01:20:46.360,01:20:47.320,"또한 마찬가지로,",4847.32,6
01:20:47.320,01:20:49.280,Embedding과 Softmax가 사용이 되었는데요.,4849.28,6
01:20:49.480,01:20:50.660,"이건 이제 기본적으로,",4850.66,6
01:20:50.740,01:20:52.260,"Sequence-to-Sequence 모델에서,",4852.26,6
01:20:52.320,01:20:53.920,사용되는 내용과 같다고 할 수 있습니다.,4853.92,6
01:20:54.200,01:20:55.200,"그냥 Input Dimension,",4855.2,6
01:20:55.380,01:20:57.460,"즉, 특정한 언어에 포함되어 있는,",4857.46,6
01:20:57.520,01:20:59.080,"단어의 개수와 비례하는,",4859.08,6
01:20:59.240,01:21:00.820,"그러한 Input Dimension이 들어왔을 때,",4860.82,6
01:21:00.940,01:21:02.240,"Embedding 레이어로 거쳐서,",4862.24,6
01:21:02.340,01:21:03.040,"Embedding 차원,",4863.04,6
01:21:03.060,01:21:04.400,"즉, D-Model 차원으로,",4864.4,6
01:21:04.460,01:21:05.220,맵핑해 주는 겁니다.,4865.22,6
01:21:05.480,01:21:06.180,"또한 이어서,",4866.18,6
01:21:06.280,01:21:07.100,"Transformer에서는,",4867.1,6
01:21:07.260,01:21:08.220,"Attention Mechanism만,",4868.22,6
01:21:08.240,01:21:09.220,"전적으로 활용하기 때문에,",4869.22,6
01:21:09.540,01:21:11.340,"위치에 대한 정보 값을 같이 주기 위해서,",4871.34,6
01:21:11.740,01:21:12.600,"인코딩 정보를,",4872.6,6
01:21:12.600,01:21:14.480,같이 더해서 넣어줄 수 있다고 했습니다.,4874.48,6
01:21:14.780,01:21:15.460,"다시 말해,",4875.46,6
01:21:15.480,01:21:18.480,"Recurrence 및 Convolution 둘 다 사용하지 않기 때문에,",4878.48,6
01:21:18.540,01:21:20.820,위치에 대한 정보를 같이 넣어줄 필요가 있는 겁니다.,4880.82,6
01:21:21.060,01:21:23.040,"그래서, 본 논문에서는 다음과 같이,",4883.04,6
01:21:23.180,01:21:25.660,"Sine과 Cosine과 같은 주기 함수를 사용해서,",4885.66,6
01:21:25.780,01:21:27.040,입력을 넣었다고 하고요.,4887.04,6
01:21:27.540,01:21:28.160,"말씀드렸듯이,",4888.16,6
01:21:28.160,01:21:29.960,"우리가 위치에 대한 정보를 넣어줄 때,",4889.96,6
01:21:30.080,01:21:33.280,꼭 Sine이나 Cosine 함수를 이러한 형태로 사용할 필요는 없고요.,4893.28,6
01:21:33.480,01:21:35.140,"이런 함수를 사용하지 않고,",4895.14,6
01:21:35.460,01:21:36.900,"이러한 Embedding 레이어 또한,",4896.9,6
01:21:36.920,01:21:38.560,"우리가 별도로 학습하도록 만들어서,",4898.56,6
01:21:38.720,01:21:40.120,네트워크를 구성할 수도 있습니다.,4900.12,6
01:21:40.380,01:21:41.540,"실제로 본 논문에서는,",4901.54,6
01:21:41.720,01:21:42.000,"이렇게,",4902.0,6
01:21:42.000,01:21:43.480,"학습이 가능한 형태로,",4903.48,6
01:21:43.560,01:21:45.280,Embedding 레이어를 사용해봤다고 하는데요.,4905.28,6
01:21:45.480,01:21:47.700,실제로 성능상에는 차이가 없었다고 합니다.,4907.7,6
01:21:47.920,01:21:48.860,"사실 그렇기 때문에,",4908.86,6
01:21:49.020,01:21:51.360,"우리가 PyTorch와 같은 프레임웍을 사용할 때,",4911.36,6
01:21:51.460,01:21:52.860,"그냥 학습하도록 하는 게,",4912.86,6
01:21:52.880,01:21:54.340,"구현이 더 쉬울 수도 있기 때문에,",4914.34,6
01:21:54.480,01:21:56.880,"실제로 우리가 아까 실습을 진행했을 때,",4916.88,6
01:21:57.000,01:21:59.800,별도의 학습 가능한 Embedding 레이어를 사용을 했던 겁니다.,4919.8,6
01:22:00.040,01:22:01.180,"다만, 본 논문에서는,",4921.18,6
01:22:01.400,01:22:03.220,"이러한 정연파 함수를 사용했을 때,",4923.22,6
01:22:03.500,01:22:05.360,"보다 긴 시퀀스가 들어왔을 때,",4925.36,6
01:22:05.400,01:22:08.000,성능이 더 잘 나올 수 있다고 언급하고 있습니다.,4928.0,6
01:22:08.240,01:22:09.840,"이어서, 섹션 4에서는,",4929.84,6
01:22:10.080,01:22:11.360,"이러한 Self-Attention이,",4931.36,6
01:22:11.380,01:22:11.980,왜 더 유용한가?,4931.98,6
01:22:11.980,01:22:13.540,유리한가에 대해서 설명하고 있는데요.,4933.54,6
01:22:13.760,01:22:15.080,"본 논문의 저자들은,",4935.08,6
01:22:15.140,01:22:19.080,"3가지의 어떠한 열망하는 그러한 장점들을 목표로 두고,",4939.08,6
01:22:19.200,01:22:21.800,이러한 Self-Attention을 고안했다고 말하고 있는데요.,4941.8,6
01:22:22.200,01:22:23.000,"첫 번째로는,",4943.0,6
01:22:23.000,01:22:24.940,"각각의 레이어마다 계산 복잡도가,",4944.94,6
01:22:24.960,01:22:26.640,줄어든다는 장점이 있습니다.,4946.64,6
01:22:26.920,01:22:28.580,"또한, Recurrence를 없앰으로써,",4948.58,6
01:22:28.640,01:22:30.180,병렬적인 처리가 가능하고요.,4950.18,6
01:22:30.220,01:22:30.880,"마지막으로,",4950.88,6
01:22:30.980,01:22:32.660,"Long Range Dependency에 대해서도,",4952.66,6
01:22:32.720,01:22:34.400,잘 처리할 수 있다고 말하고 있습니다.,4954.4,6
01:22:34.880,01:22:37.160,"자, 그래서 이렇게 위쪽에 보이는 테이블이,",4957.16,6
01:22:37.320,01:22:38.600,"RNN을 사용했을 때와,",4958.6,6
01:22:38.680,01:22:39.920,"Convolution을 사용했을 때,",4959.92,6
01:22:40.000,01:22:41.600,"그리고 이렇게 Attention 메커니지만,",4961.6,6
01:22:41.600,01:22:42.580,"활용했을 때에 대한,",4962.58,6
01:22:42.700,01:22:44.500,효율성을 분석하고 있는 건데요.,4964.5,6
01:22:44.520,01:22:46.120,"이때 N은 Sequence의 길이,",4966.12,6
01:22:46.140,01:22:47.500,즉 단어의 개수라고 할 수 있고요.,4967.5,6
01:22:47.700,01:22:48.780,"이때 확인해 보시면은,",4968.78,6
01:22:48.780,01:22:50.320,"이러한 Attention 기법을 사용할 때,",4970.32,6
01:22:50.460,01:22:52.000,"Sequential Data를 처리할 때,",4972.0,6
01:22:52.060,01:22:54.200,"단 한 번에 병렬적으로 구할 수 있기 때문에,",4974.2,6
01:22:54.520,01:22:55.680,"RNN과 비교했을 때,",4975.68,6
01:22:55.820,01:22:57.180,"훨씬 네트워크에 들어가는,",4977.18,6
01:22:57.200,01:22:59.160,입력의 횟수가 적다는 걸 확인할 수 있고요.,4979.16,6
01:22:59.400,01:22:59.980,"이와 같이,",4979.98,6
01:23:00.040,01:23:01.600,"복잡도를 비교했을 때에도,",4981.6,6
01:23:01.820,01:23:03.620,"이 N은 단어의 개수이기 때문에,",4983.62,6
01:23:03.820,01:23:05.300,"일반적으로 이 D보다는,",4985.3,6
01:23:05.340,01:23:07.120,조금 더 작게 형성되는 경우가 많습니다.,4987.12,6
01:23:07.460,01:23:08.740,"N 제곱 곱하기 D가,",4988.74,6
01:23:08.800,01:23:10.280,"N 곱하기 D 제곱보다는,",4990.28,6
01:23:10.280,01:23:11.920,"더 낮을 확률이 높기 때문에,",4991.92,6
01:23:11.940,01:23:14.300,보다 유리한 복잡도를 가진다고 할 수 있습니다.,4994.3,6
01:23:14.480,01:23:15.880,"실제로 이렇게 본문에서도,",4995.88,6
01:23:16.040,01:23:17.180,내용이 설명되고 있는데요.,4997.18,6
01:23:17.460,01:23:18.440,"보통 이 N,",4998.44,6
01:23:18.560,01:23:20.480,"즉 Sequence의 길이가 이 D보다는,",5000.48,6
01:23:20.520,01:23:22.000,"짧은 경우가 많기 때문에,",5002.0,6
01:23:22.220,01:23:24.020,훨씬 효율적일 수 있다고 말하고 있습니다.,5004.02,6
01:23:24.280,01:23:25.780,"실제로 다양한 번역 모델에서는,",5005.78,6
01:23:26.000,01:23:27.620,"이 N, 즉 Sequence 길이는,",5007.62,6
01:23:27.740,01:23:30.760,이렇게 단어 단위의 토큰의 개수와 같다고 할 수 있죠.,5010.76,6
01:23:30.960,01:23:32.460,"그런 경우를 생각해 보았을 때,",5012.46,6
01:23:32.500,01:23:33.640,"확실히 이 N이,",5013.64,6
01:23:33.640,01:23:36.160,일반적으로 D보다는 작게 형성되는 경우가 많습니다.,5016.16,6
01:23:36.420,01:23:38.760,"실제로 다양한 번역 데이터셋을 확인해 보시면,",5018.76,6
01:23:39.060,01:23:40.260,"한 문장에 포함되어도,",5020.26,6
01:23:40.260,01:23:42.660,"있는 단어의 개수가 그렇게 많지 않기 때문에,",5022.66,6
01:23:42.700,01:23:43.900,"그런 측면을 보았을 때,",5023.9,6
01:23:43.940,01:23:45.820,"단어의 개수가 이러한 N이 되기 때문에,",5025.82,6
01:23:45.900,01:23:47.960,그러한 측면에서 효과적이라고 할 수 있습니다.,5027.96,6
01:23:48.120,01:23:49.040,"또한 추가적으로,",5029.04,6
01:23:49.120,01:23:50.520,"Attention Mechanism 자체가,",5030.52,6
01:23:50.620,01:23:52.120,"우리 Neural Network를,",5032.12,6
01:23:52.160,01:23:53.760,"보다 설명 가능한 형태로,",5033.76,6
01:23:53.860,01:23:55.940,만들어 준다는 점이 장점이라고 할 수 있습니다.,5035.94,6
01:23:56.140,01:23:58.180,"실제로 우리는 각 단어를 출력할 때,",5038.18,6
01:23:58.320,01:24:00.100,"소스 문장에서 어떤 단어를,",5040.1,6
01:24:00.180,01:24:02.080,"가장 많이 참고해서 만들었는지를,",5042.08,6
01:24:02.140,01:24:03.780,시각적으로 출력해 볼 수 있습니다.,5043.78,6
01:24:04.020,01:24:06.220,"단순히 그냥 각각의 헤드에 포함되어 있는,",5046.22,6
01:24:06.460,01:24:08.100,"그 Self-Attention Mechanism의,",5048.1,6
01:24:08.160,01:24:10.120,"Softmax 값을 출력해 보면,",5050.12,6
01:24:10.260,01:24:10.520,되겠죠.,5050.52,6
01:24:10.700,01:24:12.220,"또한 트레이닝을 진행할 때,",5052.22,6
01:24:12.260,01:24:14.340,설정했던 내용들은 다음과 같은데요.,5054.34,6
01:24:14.540,01:24:16.100,"말씀드렸듯이 영어 독일어,",5056.1,6
01:24:16.140,01:24:17.800,"그리고 영어 프랑스어,",5057.8,6
01:24:17.900,01:24:20.180,"이 두 가지 대표적인 데이터셋에 대해서,",5060.18,6
01:24:20.320,01:24:21.260,실험했다고 하고요.,5061.26,6
01:24:21.340,01:24:24.460,"WMT 2014년도 영어 독어 데이터셋은,",5064.46,6
01:24:24.460,01:24:27.460,약 450만 개 정도의 문장 쌍이 존재하고요.,5067.46,6
01:24:27.500,01:24:29.400,"그리고 영어 프랑스어 같은 경우는,",5069.4,6
01:24:29.400,01:24:32.320,"3600만 개 정도의 문장 쌍이 존재하는,",5072.32,6
01:24:32.380,01:24:33.580,데이터셋을 이용했고요.,5073.58,6
01:24:33.740,01:24:35.380,"학습을 위한 하드웨어로는,",5075.38,6
01:24:35.380,01:24:38.600,8개의 NVIDIA P100 GPU를 사용을 했고요.,5078.6,6
01:24:38.760,01:24:39.960,"베이스 모델만으로도,",5079.96,6
01:24:40.260,01:24:42.100,"State of the Art의 성능이 나오고,",5082.1,6
01:24:42.120,01:24:43.080,"학습 시간 또한,",5083.08,6
01:24:43.080,01:24:45.660,매우 빠른 12시간 밖에 걸리지 않았다고 말하고 있습니다.,5085.66,6
01:24:45.880,01:24:46.780,"또한 이와 같이,",5086.78,6
01:24:46.840,01:24:48.160,"Adam Optimizer를 사용하고,",5088.16,6
01:24:48.360,01:24:50.220,세부적인 파라미터는 다음과 같습니다.,5090.22,6
01:24:50.500,01:24:51.740,"또한 정규화 효과를 위해서,",5091.74,6
01:24:52.120,01:24:53.860,"residual learning을 수행할 때,",5093.86,6
01:24:53.920,01:24:56.280,이 드랍아웃도 같이 사용할 수 있도록 만들었고요.,5096.28,6
01:24:56.400,01:24:56.900,"이어서,",5096.9,6
01:24:56.980,01:24:58.380,"label 값을 넣어줄 때,",5098.38,6
01:24:58.480,01:25:00.640,"이 스무신 기법까지 적용하도록 만들어서,",5100.64,6
01:25:00.780,01:25:02.640,"우리 모델이 특정 출력 값에 대해서,",5102.64,6
01:25:02.840,01:25:04.560,"확신을 가지지 않도록 함으로써,",5104.56,6
01:25:04.640,01:25:06.380,정규화 효과를 더할 수 있다고 했습니다.,5106.38,6
01:25:06.720,01:25:09.440,"사실 이것도 굉장히 잘 알려진 정규화 기법이고,",5109.44,6
01:25:09.440,01:25:10.800,"이미지 분류 등에서도,",5110.8,6
01:25:10.940,01:25:13.140,이미 많이 사용되고 있는 기법 중 하나죠.,5113.14,6
01:25:13.260,01:25:14.180,"그래서 이와 같이,",5114.18,6
01:25:14.200,01:25:17.520,Accuracy와 Blue Score를 높일 수 있었다고 말하고 있습니다.,5117.52,6
01:25:17.740,01:25:19.440,"그래서 여기 보이는 표가 실제,",5119.44,6
01:25:19.460,01:25:21.940,트랜스포머 아키텍처의 성능을 잘 보여주고 있는데요.,5121.94,6
01:25:22.240,01:25:22.780,"이와 같이,",5122.78,6
01:25:22.820,01:25:24.680,"기본적인 베이스 모델만 가지고도,",5124.68,6
01:25:24.780,01:25:27.240,"기존 State of the Art 네트워크와 필적하는,",5127.24,6
01:25:27.320,01:25:29.060,좋은 성능을 내는 걸 확인할 수 있었고요.,5129.06,6
01:25:29.200,01:25:32.020,"이때, 학습 시간은 훨씬 짧았다는 걸 확인할 수 있습니다.",5132.02,6
01:25:32.220,01:25:35.240,"또한, 이러한 트랜스포머의 파라미터 수를 훨씬 늘려서,",5135.24,6
01:25:35.400,01:25:36.980,"큰 모델을 사용했을 때에도,",5136.98,6
01:25:37.200,01:25:38.600,"이전 연구와 비교했을 때,",5138.6,6
01:25:38.660,01:25:39.420,"학습 효율이 더 높은,",5139.42,6
01:25:39.420,01:25:39.940,"성능이 높았으며,",5139.94,6
01:25:39.980,01:25:42.560,성능은 훨씬 더 개선된 걸 확인할 수 있습니다.,5142.56,6
01:25:42.740,01:25:44.740,"또한, 이런 트랜스포머 아키텍처에서,",5144.74,6
01:25:44.840,01:25:48.580,"어떤 컴포넌트가 상대적으로 중요한지에 대한 내용을 확인하기 위해서,",5148.58,6
01:25:48.780,01:25:51.040,모델 Variation 실험 또한 진행을 했는데요.,5151.04,6
01:25:51.300,01:25:52.980,"간단하게 Head의 수를 줄여보거나,",5152.98,6
01:25:53.120,01:25:54.860,"특정 파라미터의 수를 늘려보거나,",5154.86,6
01:25:54.900,01:25:56.480,"줄여보거나, 이런 실험들을 해본 겁니다.",5156.48,6
01:25:56.720,01:26:00.100,"첫 번째로, Head의 수를 바꿔가면서 실험을 해본 건데요.",5160.1,6
01:26:00.180,01:26:02.080,"이렇게 여기 보이는 이 베이스 모델이,",5162.08,7
01:26:02.140,01:26:04.860,가장 기본적으로 사용한 Hyper-Parameter라고 할 수 있고요.,5164.86,7
01:26:05.080,01:26:06.300,"여기서 A 같은 경우는,",5166.3,7
01:26:06.300,01:26:07.860,"이 Head의 디멘저를 바꿔가지고,",5167.86,7
01:26:07.960,01:26:08.780,"그에 따라서,",5168.78,7
01:26:08.780,01:26:11.660,이 Key와 Value의 디멘저를 또한 바뀔 수 있도록 한 겁니다.,5171.66,7
01:26:12.080,01:26:14.960,"말씀드렸듯이, D 모델을 H로 나눈 값이,",5174.96,7
01:26:15.000,01:26:17.420,Key와 Value의 디멘저로 사용될 수 있다고 했죠.,5177.42,7
01:26:17.520,01:26:18.460,"그래서 확인해 보시면,",5178.46,7
01:26:18.680,01:26:20.720,"이렇게 Head를 8개 사용했을 때,",5180.72,7
01:26:20.760,01:26:22.540,가장 성능이 좋은 걸 확인할 수 있고요.,5182.54,7
01:26:22.740,01:26:25.720,"또한, B 같은 경우는 별도로 Head와 상관없이,",5185.72,7
01:26:25.860,01:26:28.460,이 Key Value의 디멘저를 더 줄여본 겁니다.,5188.46,7
01:26:28.780,01:26:31.380,"확인 결과, 당연히 파라미터의 수가 줄어서,",5191.38,7
01:26:31.500,01:26:33.600,모델의 Capacity 또한 감소하게 되겠죠.,5193.6,7
01:26:33.780,01:26:36.620,실제로 결과 또한 더 안 좋아지는 걸 확인할 수가 있었고요.,5196.62,7
01:26:36.720,01:26:38.760,"그래서 이렇게 64를 사용했을 때,",5198.76,7
01:26:38.760,01:26:42.480,16이나 32를 사용했을 때보다 더 좋은 걸 확인할 수 있습니다.,5202.48,7
01:26:42.620,01:26:44.880,"또한 이렇게 모델의 크기를 더 키웠을 때,",5204.88,7
01:26:44.920,01:26:46.860,더 성능이 좋아진 것 또한 확인할 수 있습니다.,5206.86,7
01:26:47.040,01:26:47.980,"보시면 이런 식으로,",5207.98,7
01:26:48.140,01:26:49.480,"Embedding 차원을 높이거나,",5209.48,7
01:26:49.640,01:26:52.460,"이 Feed Forward Layer에 포함되어 있는 차원을 높였을 때,",5212.46,7
01:26:52.520,01:26:54.540,더 성능이 좋아지는 걸 확인할 수 있고요.,5214.54,7
01:26:54.720,01:26:56.700,"또한, Dropout 기법은 이처럼,",5216.7,1
01:26:56.720,01:26:59.420,Overfitting 방지에 매우 효과적인 걸 확인할 수 있습니다.,5219.42,1
01:26:59.600,01:27:01.620,"그래서 이렇게 Dropout을 썼을 때,",5221.62,7
01:27:01.680,01:27:03.820,더 성능이 많이 좋아진 걸 확인할 수 있습니다.,5223.82,7
01:27:04.020,01:27:05.820,"또한, 위치에 대한 정보를 주기 위해,",5225.82,7
01:27:06.040,01:27:08.480,"Sine과 Cosine 함수를 이용한 Encoding 대신에,",5228.48,7
01:27:08.480,01:27:10.340,"별도의 Embedding Layer를 사용했을 때,",5230.34,7
01:27:10.400,01:27:12.220,"이때는 Base Model과 비교했을 때,",5232.22,7
01:27:12.300,01:27:14.880,성능의 차이는 거의 없는 걸 확인할 수 있습니다.,5234.88,7
01:27:15.040,01:27:16.620,"또한 이러한 Transformer는,",5236.62,6
01:27:16.640,01:27:17.960,"비단 기계 번역뿐만 아니라,",5237.96,6
01:27:18.100,01:27:20.760,다양한 자연어 처리 태스크에서 사용이 가능한데요.,5240.76,6
01:27:20.920,01:27:22.860,"대표적으로, 구문 분석 분야에 대해서,",5242.86,6
01:27:23.080,01:27:24.920,실험한 결과 또한 보여주고 있습니다.,5244.92,6
01:27:25.240,01:27:28.440,여기 Table 4번이 그 내용을 간단하게 보여주고 있고요.,5248.44,10
01:27:28.740,01:27:29.640,"여기 확인해 보시면,",5249.64,10
01:27:29.840,01:27:31.020,"이 경우를 제외하고,",5251.02,10
01:27:31.220,01:27:34.040,"나머지는 다른 State of the Art 네트워크와 비교했을 때,",5254.04,10
01:27:34.120,01:27:35.820,더 성능이 좋은 걸 확인할 수 있습니다.,5255.82,10
01:27:36.060,01:27:38.000,"Semi-supervised 케이스에 대해서도,",5258.0,10
01:27:38.000,01:27:41.020,마찬가지로 더 좋은 성능이 나오는 걸 확인할 수 있습니다.,5261.02,10
01:27:41.240,01:27:42.480,"자, 그래서 결과적으로,",5262.48,6
01:27:42.580,01:27:44.120,"이와 같이 본 논문에서는,",5264.12,6
01:27:44.200,01:27:46.100,Transformer 아키텍처를 제안했고요.,5266.1,6
01:27:46.220,01:27:48.220,"본 논문은 기존까지 논문과 다르게,",5268.22,6
01:27:48.340,01:27:50.620,"전적으로 Attention Mechanism만 활용을 해서,",5270.62,6
01:27:50.900,01:27:54.380,Recurrent한 네트워크 자체를 전부 다 아키텍처에서 빼버렸고요.,5274.38,6
01:27:54.540,01:27:56.840,"이로 인해, 보다 높은 병렬성을 얻게 되고,",5276.84,6
01:27:57.020,01:27:59.200,성능 또한 많이 개선될 수 있었습니다.,5279.2,6
01:27:59.340,01:28:01.460,"그래서 실제로 기계 번역 태스크에 대해서,",5281.46,6
01:28:01.700,01:28:04.460,"기존까지 존재했던 다른 아키텍처에 비해서,",5284.46,6
01:28:04.520,01:28:06.380,"더 좋은 성능을 보여줄 수가 있었고,",5286.38,6
01:28:06.500,01:28:07.840,"비단 기계 번역뿐만 아니라,",5287.84,6
01:28:07.840,01:28:09.300,"다양한 태스크에 대해서도,",5289.3,6
01:28:09.340,01:28:12.100,적용 가능성이 높다는 것까지 잘 보여주었습니다.,5292.1,6
01:28:12.320,01:28:16.100,"이상으로, 이번 시간에는 Transformer에 대해서 알아보았습니다.",5296.1,6
