start,end,text,end_sec,page
00:00.0,00:10.0,꼼꼼한 딥러닝 논문 리뷰와 코드 실습. 이번 시간에 리뷰할 논문은 현대 딥러닝 기반의 자연어처리 기술의 핵심 아키텍처가 되고 있는 트랜스포머입니다.,9.98,1
00:10.3,00:24.4,트랜스포머 논문의 원래 제목은 Attention is all you need 입니다. 논문의 제목에서 알 수 있듯이 트랜스포머라는 아키텍처에는 이 Attention이라고 하는 것이 가장 메인 아이디어로서 사용이 된다는 걸 알 수 있습니다.,24.42,1
00:24.4,00:36.6,실제로 트랜스포머는 Attention이라는 메커니즘을 전적으로 활용하는 아키텍처입니다. 트랜스포머가 나오게 된 계기를 이해하기 위해서 딥러닝 기반의 기계 번역 발전 과정에 대해 확인해 보겠습니다.,36.58,1
00:37.0,00:44.0,2021년 기준으로 최신 자연어처리 쪽 고성능 모델들은 이런 트랜스포머 아키텍처를 기반으로 하고 있습니다.,44.02,1
00:44.3,00:52.3,최근까지 화제가 되었던 GPT와 BERT는 모두 이러한 트랜스포머의 아키텍처를 적절히 활용하여 좋은 성능을 내고 있습니다.,52.34,1
00:52.3,01:01.0,대표적으로 GPT는 트랜스포머의 디코더 아키텍처를 활용했고 BERT는 트랜스포머의 인코더 아키텍처를 활용했다는 점이 특징입니다.,61.02,1
01:01.2,01:06.5,자연어처리 태스크 중에서 가장 대표적이면서 중요한 태스크 중 하나는 기계 번역입니다.,66.54,1
01:06.9,01:16.8,실제로 기계 번역 기술의 발전 과정을 확인해 보시면 1986년도 즈음에 RNN이 제한되었고 그로부터 약 10년 정도가 지난 뒤에 LSTM이 등장하였습니다.,76.84,1
01:17.3,01:21.2,이러한 LSTM을 활용하면 다양한 시퀀스 정보를 모델링 할 수 있는데요.,81.24,1
01:21.2,01:25.0,"대표적으로 주가 예측, 주기함수 예측 등이 가능합니다.",85.02,1
01:25.2,01:31.5,이러한 LSTM을 활용해서 2014년도에는 딥러닝 기반 기술로 시퀀스 투 시퀀스가 등장하였습니다.,91.54,1
01:31.9,01:41.9,시퀀스 투 시퀀스는 현대의 딥러닝 기술들이 다시 빠르게 나오기 시작한 시점인 2014년도에 이러한 LSTM을 활용해서 고정된 크기의 컨텍스트,101.88,1
01:41.9,01:50.0,벡터를 사용하는 방식으로 번역을 수행하는 방법을 제안하였습니다. 다만 이러한 시퀀스 투 시퀀스 모델이 나왔을 때의 시점만 하더라도,109.98,1
01:50.0,02:00.9,고정된 크기의 컨텍스트 벡터를 쓰고 있기 때문에 소스 문장을 전부 고정된 크기의 한 벡터에다가 압축을 할 필요가 있다는 점에서 성능적인 한계가 존재했습니다.,120.9,1
02:01.1,02:09.7,이후에 어텐션 메커니즘이 제한된 논문이 나오면서 이러한 시퀀스 투 시퀀스 모델에 어텐션 기법을 적용하여 성능을 더 끌어올릴 수가 있었고요.,129.7,1
02:09.8,02:20.0,"이제 그 이후에 트랜스포머 논문에서는 그냥 어뢰넨 자체를 사용할 필요가 없다는 아이디어로 오직 어텐션 기법에 의존하는 아키텍처를 설계했더니 성능이 훨씬 더 좋게 되었고,",139.96,1
02:20.0,02:21.3,성능이 훨씬 좋아지는 것을 보여주었습니다.,141.32,1
02:21.5,02:32.6,즉 이 트랜스포머를 기점으로 해서 더 이상 다양한 자연어 처리 태스크에 대해서 어뢰넨 기반의 아키텍처를 사용하지 않고 어텐션 메커니즘을 더욱 더 많이 사용하게 되었습니다.,152.64,1
02:32.9,02:42.1,그래서 어텐션 메커니즘이 등장한 이후로부터는 입력 시퀀스 전체에서 정보를 추출하는 방향으로 연구 방향이 발전되어 왔다고 할 수 있습니다.,162.12,1
02:42.3,02:47.5,"물론 이후에 나온 논문들 중에서도 어뢰넨을 활용하는 아키텍처도 많이 존재하지만,",167.5,1
02:47.5,02:56.4,전반적인 추세 자체는 어텐션 기법을 더욱 더 활용하는 이런 트랜스포머의 아키텍처를 따르는 방식으로 다양한 고성능 모델들이 제한되고 있습니다.,176.42,1
02:56.6,03:01.9,그렇다면 기존에 제한되었던 시퀀스 투 시퀀스 모델에는 어떤 한계점이 존재할까요?,181.9,2
03:02.0,03:08.9,"기존 시퀀스 투 시퀀스 모델의 한계점이라고 한다면, 이 컨텍스트 벡터 V에 소스 문장의 정보를 압축한다는 점입니다.",188.94,2
03:09.2,03:13.9,이때 병목 현상이 발생할 수 있기 때문에 성능 하락의 원인이 될 수 있는데요.,193.88,2
03:14.2,03:17.5,"현재 예시를 확인해 보시면 대표적인 시퀀스 투 시퀀스 모델은,",197.48,2
03:17.5,03:19.6,이 시퀀스 투 시퀀스 모델을 활용한 기계 번역 예시라고 할 수 있습니다.,199.62,2
03:19.8,03:25.4,"왼쪽에 있는 독일어 문장, 즉 각각의 단어들로 구성된 하나의 시퀀스가 들어왔을 때,",205.44,2
03:25.5,03:30.2,"이렇게 중간에서 하나의 고정된 크기의 컨텍스트 벡터로 바꾼 뒤에,",210.2,2
03:30.3,03:35.1,다시 이러한 컨텍스트 벡터로부터 출력 문장을 만들어내는 것을 확인할 수 있습니다.,215.1,2
03:35.3,03:42.5,"즉, 한쪽의 시퀀스에서부터 다른 한쪽의 시퀀스를 만든다는 의미에서 시퀀스 투 시퀀스 모델이라고 부를 수 있습니다.",222.54,2
03:42.8,03:46.5,결과적으로 이렇게 영어 출력 문장이 나오는 걸 확인할 수 있고요.,226.46,2
03:46.5,03:50.4,"다만 이때, 이러한 시퀀스 투 시퀀스 아키텍처를 확인해 보시면,",230.44,2
03:50.7,03:55.9,매번 단어가 입력될 때마다 히든 스테이트 값을 갱신하는 걸 확인할 수 있습니다.,235.94,2
03:56.1,03:58.2,"이런 식으로 단어가 입력될 때마다,",238.16,2
03:58.2,04:03.6,"이전까지 입력되었던 단어들에 대한 정보를 포함하고 있는 히든 스테이트 값을 받아서,",243.6,2
04:03.7,04:06.8,매번 이런 식으로 히든 스테이트 값을 새롭게 갱신합니다.,246.84,2
04:07.1,04:13.6,"즉, 이런 식으로 각각의 단어가 차례대로 순서에 맞게 입력될 때마다 히든 스테이트 값이 갱신되어,",253.6,2
04:13.6,04:16.4,"이러한 히든 스테이트 값은 이전까지 입력되었고,",256.44,2
04:16.5,04:19.2,"입력되었던 단어들에 대한 정보를 갖고 있기 때문에,",259.16,2
04:19.3,04:21.4,"이렇게 마지막 단어가 들어왔을 때,",261.44,2
04:21.5,04:28.8,그때의 히든 스테이트 값은 소스 문장 전체를 대표하는 하나의 컨텍스트 벡터로써 사용할 수가 있다는 것입니다.,268.82,2
04:29.0,04:34.6,"그렇기 때문에, 이렇게 마지막 단어가 들어왔을 때의 히든 스테이트 값을 하나의 컨텍스트 벡터로써,",274.6,2
04:34.7,04:41.9,이 컨텍스트 벡터 안에는 앞에 등장했던 소스 문장에 대한 문맥적인 정보를 담고 있다고 가정하는 것입니다.,281.88,2
04:42.1,04:45.2,"그렇기 때문에, 이러한 컨텍스트 벡터로부터 출발해서,",285.22,2
04:45.2,04:50.0,"이렇게 출력을 수행하는 디코더 파트에서는 매번 출력 단어가 들어올 때마다,",290,2
04:50.1,04:56.0,이러한 컨텍스트 벡터로부터 출발해서 마찬가지로 히든 스테이트를 만들어서 매번 출력을 내보냅니다.,295.96,2
04:56.1,05:00.8,"이렇게 그 다음 단계에서는 이전에 출력했던 단어가 다시 입력으로 들어와서,",300.8,2
05:00.9,05:06.9,"반복적으로 이전까지 출력했던 단어에 대한 정보를 가지고 있는 히든 스테이트와 같이 입력을 받아,",306.9,2
05:07.0,05:09.8,새롭게 히든 스테이트를 갱신하는 걸 확인할 수 있습니다.,309.78,2
05:10.0,05:14.3,"이런 식으로 디코더 파트에서는 매번 히든 스테이트 값을 갱신하면서,",314.3,2
05:14.5,05:15.0,"이렇게,",315.02,2
05:15.0,05:19.9,히든 스테이트 값으로부터 출력 값이 end of sequence가 나올 때까지 반복합니다.,319.88,2
05:20.1,05:24.1,"그래서 end of sequence가 나왔을 때 출력 문장 생성을 마치게 되고요,",324.08,2
05:24.2,05:28.1,이렇게 출력된 정보인 good evening이 나오는 걸 확인할 수 있습니다.,328.06,2
05:28.3,05:32.6,이게 가장 기본적인 형태의 시퀀스 to 시퀀스 모델의 동작 원리입니다.,332.6,2
05:32.8,05:34.1,"다만 확인해 보시면 이렇게,",334.1,2
05:34.3,05:38.4,"소스 문장을 대표하는 하나의 컨텍스트 벡터를 만들어야 한다는 점에서,",338.42,2
05:38.6,05:42.5,"이렇게 고정된 크기의 컨텍스트 벡터에 정보를 압축하려고 하면,",342.54,2
05:42.8,05:44.8,"이러한 입력 문장은 어떨 때는,",344.76,2
05:45.0,05:45.5,"짧기도 하고,",345.52,2
05:45.6,05:47.3,"어떨 때는 길기도 하기 때문에,",347.34,2
05:47.5,05:49.4,"그러한 다양한 경우의 수에 대해서,",349.4,2
05:49.7,05:53.3,"항상 소스 문장의 정보를 고정된 크기로 가지고 있는 것은,",353.32,2
05:53.4,05:56.7,전체 성능에서 병목 현상의 원인이 될 수 있습니다.,356.72,2
05:56.9,06:00.4,"그래서 이러한 문제를 조금이나마 완화하기 위한 아이디어로,",360.4,3
06:00.5,06:06.4,"이 고정된 크기의 컨텍스트 벡터를 매번 이 디코더의 RNN 셀에서 참고하도록 만들어서,",366.4,3
06:06.5,06:08.2,조금 더 성능을 개선할 수 있습니다.,368.24,3
06:08.5,06:09.4,"이렇게 하게 되면,",369.36,3
06:09.5,06:14.3,"이 컨텍스트 벡터에 대한 정보가 이 디코더 파트의 RNN 셀을 거침에 따라서,",374.32,3
06:15.0,06:17.8,"정보가 손실되는 정도를 더 줄일 수 있기 때문에,",377.8,3
06:18.4,06:20.3,"출력되는 문장이 길어진다고 하더라도,",380.26,3
06:20.8,06:25.8,"각각의 출력되는 단어에 이러한 컨텍스트 벡터에 대한 정보를 다시 한번 넣어줄 수 있어서,",385.78,3
06:26.0,06:28.7,성능이 기존보다 조금 더 향상될 순 있습니다.,388.74,3
06:29.0,06:31.0,"다만 이런 식으로 접근한다고 하더라도,",391.04,3
06:31.1,06:36.3,"여전히 이 소스 문장을 하나의 벡터에 압축해야 된다는 점은 동일하기 때문에,",396.34,3
06:36.5,06:38.2,병목 현상은 여전히 발생합니다.,398.24,3
06:38.4,06:40.0,"즉 현재의 문제 상황이라고 한다면,",400.04,4
06:40.2,06:42.7,"하나의 문맥 벡터, 즉 컨텍스트 벡터가,",402.68,4
06:42.8,06:45.0,"소스 문장의 모든 정보를 가지고 있어서,",405,4
06:45.0,06:47.2,성능이 저하될 수 있다는 것입니다.,407.24,4
06:47.4,06:49.5,"그렇다면 디코더 파트에서는,",409.54,4
06:49.7,06:52.8,"하나의 문맥 벡터에 대한 정보만 가지고 있는 게 아니라,",412.84,4
06:53.0,06:54.6,"출력 단어를 만들 때마다,",414.56,4
06:54.7,06:59.4,매번 소스 문장에서의 출력 값들 전부를 입력으로 받으면 어떨까요?,419.44,4
06:59.5,07:01.3,라는 아이디어가 나올 수 있는 거죠.,421.28,4
07:01.4,07:05.5,"최신 GPU는 많은 메모리와 그리고 빠른 병렬 처리를 지원하기 때문에,",425.54,4
07:05.8,07:08.5,"소스 문장의 시퀀스 길이가 길다고 하더라도,",428.52,4
07:08.7,07:12.8,"그러한 소스 문장을 구성하는 각각의 단어에 대한 출력 값들 전부를,",432.84,4
07:12.9,07:14.8,"특정 행렬에다가 기록해 놓았다가,",434.78,4
07:14.8,07:19.7,"소스 문장에 대한 전반적인 내용들을 매번 출력할 때마다 반영할 수 있기 때문에,",439.66,4
07:19.7,07:22.2,성능이 좋아질 것을 기대할 수 있습니다.,442.16,4
07:22.4,07:26.7,"다시 말해, 하나의 고정된 크기의 컨텍스트 벡터에 담지 말고,",446.72,4
07:26.9,07:31.4,"그냥 소스 문장에서 나왔던 출력 값들 전부를 매번 입력으로 받아서,",451.38,4
07:31.5,07:35.0,"일련의 처리 과정을 거쳐서 출력 단어를 만들도록 하면,",454.96,4
07:35.1,07:37.0,성능이 더 좋아질 수 있다는 겁니다.,456.96,4
07:37.2,07:39.0,"지금 보이는 아키텍처가 바로,",458.96,5
07:39.1,07:43.0,시퀀스 시퀀스에 어텐션 메커니즘을 적용한 아키텍처인데요.,463.02,5
07:43.3,07:44.8,"이렇게 어텐션 메커니즘을,",464.76,5
07:44.8,07:49.0,적용해서 인코더 파트의 모든 출력을 참고하도록 만들 수가 있습니다.,469,5
07:49.2,07:51.7,"실제로 파이톨치와 같은 프레임워크에서는,",471.72,5
07:51.9,07:55.3,"단순히 RNN이나 LSTM 같은 걸 사용하도록 만들면,",475.28,5
07:55.4,07:59.7,"이렇게 매번 전체 시퀀스 기기에 맞는 아웃풋 값들이 따로,",479.7,5
07:59.7,08:01.1,출력 값들이 나오게 되는데요.,481.08,5
08:01.3,08:05.8,이제 그걸 그대로 이용해서 실제로 어텐션 메커니즘을 간단하게 구현할 수도 있습니다.,485.78,5
08:06.0,08:07.5,"전반적인 내용을 확인해 보시면,",487.46,5
08:07.7,08:11.2,"이렇게 매번 단어가 출력돼서 히든스테이트가 나올 때마다,",491.16,5
08:11.2,08:13.7,"그냥 이 값들을 전부 다 출력 값으로써,",493.74,5
08:13.7,08:16.5,그냥 별도의 배열에다가 다 기록해 놓습니다.,496.52,5
08:16.7,08:19.3,"그래서 이런 식으로 각각의 단어를 거치면서,",499.3,5
08:19.3,08:22.6,갱신되는 히든스테이트 값들을 매번 다 가지고 있는 거예요.,502.64,5
08:22.7,08:23.8,"이렇게 해 줌으로써,",503.78,5
08:23.8,08:27.4,"이렇게 매 단어가 들어왔을 때의 히든스테이트 값을,",507.42,5
08:27.5,08:29.6,"전부 가지고 있을 수 있기 때문에,",509.58,5
08:29.8,08:32.4,"이러한 값들을 어떻게든 참고해서,",512.38,5
08:32.5,08:35.0,"이렇게 출력 단어가 매번 생성될 때마다,",515,5
08:35.2,08:39.5,이러한 소스 문장 전체를 반영하겠다라는 아이디어라고 보시면 되겠습니다.,519.48,5
08:39.8,08:42.7,"실제로는 이렇게 디코더 파트에서 매번,",522.68,5
08:42.7,08:43.7,"히든스테이트를 넣어서,",523.72,5
08:43.7,08:44.4,"히든스테이트 값을 갱신하게 되는데,",524.44,5
08:44.7,08:48.2,"이때 현재 단계에서 히든스테이트 값을 만든다고 하면,",528.18,5
08:48.4,08:50.7,"바로 이전의 히든스테이트 값을 이용해서,",530.74,5
08:50.8,08:52.7,"이 출력 단의 히든스테이트 값과,",532.74,5
08:52.8,08:56.7,"이렇게 소스 문장 단의 히든스테이트 값을 서로 묶어서,",536.72,5
08:56.9,08:58.5,"별도의 행렬 곡을 수행해서,",538.46,5
08:58.6,09:00.6,각각 에너지 값을 만들어냅니다.,540.6,5
09:00.8,09:02.6,"이제 이때 그 에너지 값은,",542.6,5
09:02.7,09:05.1,"내가 현재 어떠한 단어를 출력하기 위해서,",545.1,5
09:05.3,09:09.7,"소스 문장에서 어떤 단어에 초점을 둘 필요가 있는지를,",549.68,5
09:09.7,09:11.4,수치화해서 표현한 값입니다.,551.44,5
09:11.7,09:12.2,"이제 그래서,",552.18,5
09:12.2,09:15.0,"그러한 에너지 값에 소프트맥스를 취해서,",555.04,5
09:15.1,09:16.3,"확률 값을 구한 뒤에,",556.28,5
09:16.4,09:19.9,"그렇게 소스 문장에 각각의 히든스테이트 값에 대해서,",559.92,5
09:20.1,09:23.0,"어떤 벡터에 더 많은 가중치를 두어서,",563.04,5
09:23.2,09:25.0,"참고하면 좋을지를 반영해서,",564.98,5
09:25.4,09:28.1,"그렇게 가중치 값을 다 히든스테이트에 곱한 것을,",568.08,5
09:28.2,09:30.8,"각각의 비율에 맞게 더해준 다음에,",570.78,5
09:31.0,09:33.1,"이제 그러한 weighted sum 값을,",573.08,5
09:33.2,09:35.3,"매번 출력 단어를 만들기 위해서,",575.26,5
09:35.4,09:37.6,반영을 하겠다라고 보시면 됩니다.,577.56,5
09:37.8,09:41.4,"그래서 단순히 이렇게 컨텍스트 벡터만 참고하는 것이 아니라,",581.36,5
09:41.6,09:42.2,"여기에 더해진,",582.16,5
09:42.2,09:42.6,"더불어서,",582.62,5
09:42.7,09:44.7,"소스 문장에서 출력이 되었던,",584.7,5
09:44.8,09:47.6,"모든 히든스테이트 값들을 전부 반영해서,",587.64,5
09:47.8,09:49.8,"이러한 소스 문장의 단어들 중에서,",589.82,5
09:50.0,09:52.6,"어떤 단어에 더욱 더 주의 집중해서,",592.6,5
09:52.7,09:54.5,"출력 결과 를 만들 수 있는가를,",594.54,5
09:54.7,09:56.7,"우리 모델이 고려하도록 만들어서,",596.68,5
09:56.8,09:58.7,성능을 더욱 높일 수가 있다는 겁니다.,598.66,5
09:59.0,09:59.8,"즉 쉽게 말하면,",599.8,5
09:59.9,10:01.3,"매번 출력할 때마다,",601.32,5
10:01.5,10:04.3,"이 소스 문장에서 나왔던 모든 출력 값들을,",604.28,5
10:04.4,10:06.1,전부 참고하겠다 이겁니다.,606.08,5
10:06.2,10:09.9,"즉 이렇게 압축된 컨텍스트 벡터 하나만 보는 것이 아니라,",609.9,5
10:10.2,10:12.2,"이러한 출력 값들을 전부 고려한,",612.16,5
10:12.2,10:14.5,"하나의 weighted sum vector를 구한 다음에,",614.52,5
10:14.6,10:16.8,"개를 이렇게 같이 입력으로 넣어줘서,",616.78,5
10:16.8,10:20.1,"소스 문장에 대한 정보를 모두 고려할 수 있도록 만들기 때문에,",620.08,5
10:20.1,10:21.6,성능이 좋아질 수 있는 것입니다.,621.6,5
10:22.3,10:23.2,"자 그래서 실제로,",623.22,6
10:23.5,10:25.4,"이 attention 기법을 적용했을 때,",625.4,6
10:25.6,10:28.8,"디코더 파트에서 각각의 출력 단어를 만드는 과정을,",628.8,6
10:28.9,10:30.8,"수식적으로 표현하면 다음과 같이,",630.82,6
10:30.9,10:32.0,정리할 수가 있는데요.,632.02,6
10:32.4,10:35.6,이때 i는 현재 디코더가 처리 중인 인덱스가 되겠습니다.,635.58,6
10:36.0,10:39.1,"즉 디코더가 매번 한 번에 하나의 단어를 만드는데,",639.08,6
10:39.2,10:42.0,그 각각의 처리 중인 인덱스가 i가 되겠고요.,641.98,6
10:42.2,10:43.6,"이때 이 j 같은 경우는,",643.58,6
10:43.6,10:46.1,인코더 파트에서 출력 인덱스가 되겠습니다.,646.06,6
10:46.4,10:48.2,"즉 에너지 값이라고 하는 것은,",648.22,6
10:48.3,10:51.4,"매번 디코더가 출력 단어를 만들 때마다,",651.4,6
10:51.5,10:53.1,모든 j를 고려하는 겁니다.,653.12,6
10:53.3,10:56.5,즉 인코더의 모든 출력들을 고려하겠다 라고 보시면 돼요.,656.48,6
10:56.7,10:57.8,"이제 여기에서 s는,",657.82,6
10:57.9,11:01.5,"디코더가 이전에 출력했던 단어를 만들기 위해 사용했던,",661.52,6
11:01.7,11:02.8,히든 스테이트가 되겠고요.,662.82,6
11:03.0,11:06.0,여기 h는 인코더 파트의 각각의 히든 스테이트입니다.,665.98,6
11:06.2,11:08.1,"즉 이걸 간단하게 정리하자면,",668.14,6
11:08.3,11:11.9,"디코더 파트에서 내가 이전에 출력했던 정보는 이거인데,",671.88,6
11:11.9,11:16.2,"이 정보와 인코더의 모든 출력 값과 비교를 해서,",676.2,6
11:16.3,11:17.9,에너지 값을 구하겠다는 겁니다.,677.88,6
11:18.1,11:21.9,"즉 어떤 h 값과 가장 많은 연관성을 가지는지를,",681.92,6
11:22.0,11:23.8,"에너지 값으로 구할 수가 있는 거고,",683.84,6
11:24.1,11:26.7,"이제 이러한 에너지 값에 소프트맥스를 치유해서,",686.74,6
11:26.8,11:27.7,확률 값을 구합니다.,687.66,6
11:27.8,11:29.2,"즉 실제로 비율적으로,",689.16,6
11:29.4,11:33.4,"이 각각의 h 값들 중에서 어떤 값과 가장 연관성이 높은지를,",693.38,6
11:33.4,11:34.3,"구하도록 만들고,",694.34,6
11:34.5,11:38.5,"이제 이러한 가중치 값을 실제로 h 값과 곱하도록 만들어서,",698.5,6
11:38.6,11:41.9,"이러한 가중치가 반영된 각각의 인코더의 출력을,",701.86,6
11:41.9,11:44.7,출력 결과를 더해서 그것을 활용하는 것입니다.,704.72,6
11:44.9,11:46.5,"그래서 이 그림은 실제로,",706.46,7
11:46.6,11:49.1,"이 attention mechanism을 제안한 논문에서,",709.06,7
11:49.2,11:50.6,보여주고 있는 그림인데요.,710.6,7
11:51.0,11:54.6,자 보시면 마찬가지로 에너지랑 이 가중치의 공식은 동일합니다.,714.56,7
11:54.9,11:59.0,"보시면은 매번 디코더 파트에서 각각의 단어를 만들기 위해서는,",718.96,7
11:59.0,12:02.0,이런 식으로 h 값을 이용해서 만들 수가 있는데요.,722.04,7
12:02.3,12:05.5,"현재 h 즉 st 를 만들기 위해서,",725.46,7
12:05.6,12:08.9,"이전에 사용했던 h 값과 이 인코더 파트의,",728.9,7
12:09.0,12:11.1,"모든 각각의 h 값을,",731.08,7
12:11.1,12:13.9,"같이 묶어서 에너지 값을 구한 뒤에,",733.86,7
12:13.9,12:15.8,"이제 거기에 소프트맥스를 취해서,",735.82,7
12:15.9,12:17.7,이렇게 비율 값을 구할 수 있는 거예요.,737.74,7
12:17.9,12:20.3,즉 그러한 비율이 각각 이 a 가 되겠습니다.,740.32,7
12:20.7,12:23.0,"예를 들어서 만약에 입력 문장이,",742.96,7
12:23.0,12:26.5,I am a teacher 라고 해볼게요.,746.46,7
12:26.7,12:31.1,"이때 I am a teacher에 각각의 단어들 중에서,",751.1,7
12:31.2,12:33.4,"어떤 걸 가장 많이 참고하면 되는지를,",753.4,7
12:33.5,12:36.4,그 비율 값을 이렇게 퍼센테이지로 구해주는 겁니다.,756.38,7
12:36.7,12:40.3,"예를 들어서 i는 70%, m은 20%,,",760.3,7
12:41.1,12:43.8,"ㄴ은 5%, t도 5% 이런 식으로,",763.82,7
12:43.9,12:45.9,"다 더했을 때 100이 될 수 있도록,",765.92,7
12:45.9,12:48.4,"그 확률 값을 구해서 그 비율만큼,",768.42,7
12:48.4,12:50.7,"실제로 이 h 값을 곱한 것을,",770.72,7
12:50.8,12:53.3,이 context vector처럼 사용을 할 수 있다는 겁니다.,773.26,7
12:53.4,12:55.4,"그래서 매번 출력을 할 때마다,",775.38,7
12:55.4,12:57.6,"이렇게 소스 문장에서 출력되었던,",777.6,7
12:57.7,13:00.1,"모든 hidden state 값들을 전부 반영해서,",780.1,7
13:00.4,13:03.6,이렇게 다음에 뭘 출력할지를 만들 수 있다는 겁니다.,783.6,7
13:04.0,13:07.0,"그래서 이렇게 매번 출력 단어를 만들 때마다,",787.02,7
13:07.2,13:09.1,"이렇게 소스 문장에서 등장했던,",789.06,7
13:09.3,13:11.1,"모든 hidden state 값들을, 전부 반영해서,",791.06,7
13:11.8,13:13.2,사용할 수가 있는 겁니다.,793.2,7
13:13.4,13:15.6,"즉 다시 한 번 용어를 정리해 드리자면,",795.58,7
13:15.7,13:16.9,"이 energy 값은,",796.92,7
13:16.9,13:19.8,"소스 문장에서 나왔던 모든 출력 값들 중에서,",799.76,7
13:19.9,13:23.1,"어떤 값과 가장 연관성이 있는지를 구하기 위해서,",803.06,7
13:23.2,13:24.8,그 수치를 구한 것이고요.,804.78,7
13:24.9,13:27.2,"이제 그 값들을 softmax에 넣어서,",807.2,7
13:27.3,13:29.2,"상대적인 확률 값을 구한 것이,",809.24,7
13:29.3,13:30.6,그 가중치라고 할 수 있겠고요.,810.64,7
13:30.8,13:33.7,"그러한 가중치 값들을 실제로 각각의,",813.74,7
13:33.8,13:36.3,"그 소스 문장의 hidden state와 곱해줘서,",816.26,7
13:36.3,13:39.4,"전부 더해준 값을 실제로 디코더의 입력으로,",819.42,7
13:39.5,13:40.9,"같이 넣어주겠다라고,",820.88,7
13:41.1,13:41.7,보시면 되겠습니다.,821.66,7
13:41.9,13:44.4,"이제 이런 식으로 attention 메커니즘을 쓰게 되면,",824.36,8
13:44.5,13:45.8,"성능이 좋아질 뿐만 아니라,",825.78,8
13:46.0,13:47.5,"또 추가적인 장점으로는,",827.5,8
13:47.5,13:49.8,이러한 attention은 시각화할 수도 있다는 건데요.,829.78,8
13:49.8,13:51.5,"이런 식으로 attention 가중치,",831.48,8
13:51.6,13:53.4,"즉 구해진 확률 값을 이용해서,",833.36,8
13:53.5,13:54.9,"매번 출력이 나올 때마다,",834.9,8
13:54.9,13:58.0,"그 출력이 입력에서 어떤 정보를 참고했는지를,",838.02,8
13:58.0,13:58.8,구할 수가 있습니다.,838.78,8
13:59.1,14:00.1,"지금 보시면 이거는,",840.12,8
14:00.3,14:02.3,영어를 불어로 번역한 예시인데요.,842.3,8
14:02.7,14:06.2,여기서 the arrangement on the European 이렇게 나오죠.,846.16,8
14:06.3,14:08.2,"이 각각의 단어들이 있을 때,",848.2,8
14:08.4,14:10.9,"이제 매번 이 불어의 단어들을,",850.92,8
14:11.1,14:11.8,"출력할 때마다,",851.78,8
14:12.3,14:13.6,"이러한 입력 단어들 중에서,",853.64,8
14:13.7,14:16.5,"어떤 단어에 가장 많은 초점을 뒀는지를,",856.48,8
14:16.5,14:17.4,구할 수가 있는 겁니다.,857.44,8
14:17.8,14:19.6,"지금 이런 식으로 밝게 표시된 부분이,",859.58,8
14:19.8,14:21.5,확률 값이 높은 부분이라고 할 수 있습니다.,861.54,8
14:21.9,14:23.9,"이런 식으로 매번 출력할 때마다,",863.9,8
14:24.1,14:26.3,"이 출력하는 단어가 입력 단어 중에서,",866.32,8
14:26.4,14:29.1,"어떤 단어에 더욱 많은 가중치를 두어서,",869.06,8
14:29.3,14:31.5,attention을 수행했는지를 구할 수가 있는 겁니다.,871.54,8
14:31.8,14:33.1,"기본적으로 딥러닝은,",873.08,8
14:33.1,14:35.2,"매우 많은 파라미터를 가지고 있기 때문에,",875.18,8
14:35.3,14:37.6,"그러한 세부적인 파라미터를 일일이 분석하면서,",877.62,8
14:38.0,14:39.6,"어떤 원리로 동작을 했는지를,",879.58,8
14:39.6,14:40.9,알아내기는 쉽지 않습니다.,880.86,8
14:41.1,14:42.0,"그렇기 때문에,",882,8
14:42.0,14:43.8,"이렇게 attention 메커니즘은,",883.76,8
14:43.8,14:46.1,"실제로 딥러닝이 어떤 요소에,",886.06,8
14:46.1,14:48.9,"더욱 더 많은 초점을 두어서 분류를 했는가,",888.88,8
14:48.9,14:51.2,"혹은 어떠한 데이터를 만들어냈는가,",891.16,8
14:51.2,14:53.1,"같은 과정을 분석할 때,",893.06,8
14:53.1,14:54.4,용이하게 사용할 수 있습니다.,894.38,8
14:54.6,14:57.5,"그렇다면 오늘 리뷰하고 있는 트랜스포머 논문은,",897.52,9
14:57.6,14:59.3,어떤 원리로 동작할까요?,899.32,9
14:59.5,15:01.1,"트랜스포머는 말씀드렸듯이,",901.12,9
15:01.2,15:02.6,"현대의 딥러닝 기반,",902.64,9
15:02.7,15:03.9,"자연화 처리 네트워크에서,",903.9,9
15:04.0,15:05.4,핵심이 되는 논문 중 하나입니다.,905.4,9
15:05.6,15:07.5,"그래서 논문의 원래 제목은,",907.54,9
15:07.6,15:09.1,attention is all you need 이고요.,909.14,9
15:09.3,15:11.0,"말 그대로 attention 기법만,",910.98,9
15:11.0,15:11.9,"잘 활용해도,",911.9,9
15:12.0,15:13.9,"다양한 자연화 처리 태스크에서,",913.86,9
15:14.0,15:16.0,좋은 성능을 얻을 수 있다는 의미입니다.,916,9
15:16.2,15:16.9,"다시 말해,",916.92,9
15:17.0,15:18.5,"attention 기법만 쓰기 때문에,",918.52,9
15:18.7,15:19.3,"RNN,",919.28,9
15:19.4,15:21.4,CNN 등을 전혀 필요로 하지 않습니다.,921.42,9
15:21.7,15:22.5,"진짜 말 그대로,",922.46,9
15:22.6,15:23.8,"attention 기법만 사용해서,",923.8,9
15:24.0,15:25.2,"기계 번역부터 시작해서,",925.24,9
15:25.4,15:27.7,다양한 자연화 처리 태스크를 수행할 수 있는 겁니다.,927.7,9
15:28.0,15:29.3,"오른쪽에 보이는 그림이,",929.32,10
15:29.4,15:31.1,"원본 논문에서 보여주고 있는,",931.14,10
15:31.4,15:32.7,트랜스포머의 아키텍처인데요.,932.7,10
15:33.0,15:34.2,"여기 보이는 것과 같이,",934.2,10
15:34.4,15:37.2,실제로 RNN과 CNN을 전혀 사용하지 않습니다.,937.2,10
15:37.4,15:38.1,"물론 이런 식으로,",938.14,10
15:38.4,15:40.9,"RNN, CNN 등을 전혀 사용하지 않는다면,",940.86,10
15:41.0,15:43.7,"문장 안에 포함되어 있는 각각의 단어들의,",943.66,10
15:43.7,15:45.8,순서에 대한 정보를 주기가 어렵습니다.,945.82,10
15:46.1,15:46.8,"그렇기 때문에,",946.76,10
15:46.9,15:48.5,"트랜스포머는 문장 내,",948.5,10
15:48.6,15:49.8,"각각의 단어들에 대한,",949.8,10
15:49.9,15:51.7,"순서에 대한 정보를 알려주기 위해서,",951.66,10
15:51.9,15:54.1,"별도로 포지셔널 인코딩을 이용해서,",954.08,10
15:54.2,15:55.7,순서에 대한 정보를 줄 수 있습니다.,955.72,10
15:56.0,15:57.2,"이제 이러한 아키텍처는,",957.2,10
15:57.3,15:57.9,"향후,",957.86,10
15:57.9,15:59.7,"볼트나 GPT와 같은,",959.72,10
15:59.8,16:02.3,더욱 향상된 네트워크에서도 채택이 되었고요.,962.26,10
16:02.4,16:03.3,"또한 참고로,",963.28,10
16:03.3,16:04.8,"RNN을 사용하지 않지만,",964.8,10
16:05.0,16:08.1,마찬가지로 인코더와 디코더 파트로 구성되는 건 동일합니다.,968.12,10
16:08.4,16:10.7,"또한 attention 과정을 한 번만 쓰는 게 아니라,",970.7,10
16:10.7,16:13.7,여러 레이어를 거쳐서 반복하도록 만듭니다.,973.66,10
16:13.7,16:15.0,"즉 이러한 인코더가,",974.98,10
16:15.1,16:16.5,"여러 번 중첩되어,",976.48,10
16:16.5,16:18.5,"즉 N번만큼 중첩되어,",978.46,10
16:18.5,16:20.0,사용하도록 만든다는 건데요.,979.98,10
16:20.1,16:22.0,"참고로 지금 보이는 그림에서,",981.98,10
16:22.1,16:24.0,"이 왼쪽 파트는 인코더가 되고,",983.98,10
16:24.2,16:26.1,이 오른쪽 파트는 디코더가 되겠습니다.,986.1,10
16:26.5,16:28.8,한번 자세한 내용을 지금부터 알아볼게요.,988.8,10
16:29.0,16:29.5,"자 우리가,",989.54,10
16:29.8,16:31.1,"어떠한 단어 정보를,",991.08,11
16:31.2,16:33.2,"네트워크에 넣기 위해서는 일반적으로,",993.22,11
16:33.4,16:35.0,보통 인베딩 과정을 거칩니다.,994.96,11
16:35.2,16:36.1,"그렇게 해주는 이유는,",996.12,11
16:36.2,16:36.4,"일단,",996.44,11
16:36.5,16:38.6,"맨 처음에 입력 차원 자체는,",998.62,11
16:38.7,16:40.0,"특정 언어에서,",1000,11
16:40.0,16:40.7,"존재할 수 있는,",1000.68,11
16:40.7,16:42.9,"단어의 개수와 같기 때문에,",1002.88,11
16:43.0,16:45.1,"또한 그렇게 차원이 많을 뿐만 아니라,",1005.12,11
16:45.3,16:46.4,"각각의 정보들은,",1006.4,11
16:46.5,16:48.7,"원핫 인코딩 형태로 표현이 되기 때문에,",1008.74,11
16:48.9,16:50.5,"일반적으로 네트워크에 넣을 때는,",1010.5,11
16:50.6,16:52.3,"먼저 인베딩 과정을 거쳐서,",1012.26,11
16:52.3,16:53.8,"더욱 더 적은 차원의,",1013.8,11
16:53.8,16:55.5,컨티뉴어스한 값으로 표현합니다.,1015.52,11
16:55.8,16:58.4,즉 어떠한 실수 값으로 표현할 수가 있다는 건데요.,1018.42,11
16:58.5,16:59.5,"그래서 예를 들어,",1019.48,11
16:59.6,17:00.1,"이런 식으로,",1020.12,11
17:00.3,17:01.7,"I am a teacher와 같은,",1021.68,11
17:01.8,17:03.3,"하나의 문장이 들어왔을 때,",1023.34,11
17:03.5,17:04.0,"얘는 실제로,",1024.02,11
17:04.3,17:06.5,Input-Invading Matrix로 만들어집니다.,1026.54,11
17:06.7,17:07.7,"이때 일반적으로,",1027.68,11
17:07.9,17:08.9,"이 매트릭스는,",1028.9,11
17:08.9,17:10.3,"단어의 개수만큼,",1030.34,11
17:10.4,17:10.7,"단어의 개수와,",1030.68,11
17:10.7,17:12.1,행의 크기를 가지고요.,1032.14,11
17:12.1,17:12.9,"즉 이런 식으로,",1032.86,11
17:13.0,17:14.8,"I am a teacher라는 값이,",1034.76,11
17:14.8,17:16.6,"이렇게 행 형태로 들어오게 되고,",1036.58,11
17:16.8,17:18.4,"이 각각의 열 데이터는,",1038.36,11
17:18.5,17:20.4,"인베딩 차원과 같은 크기의,",1040.36,11
17:20.4,17:21.6,"데이터가 담긴,",1041.56,11
17:21.6,17:22.8,배열을 사용하게 됩니다.,1042.78,11
17:23.0,17:24.5,"현재 그림에선 이런 식으로,",1044.46,11
17:24.5,17:25.5,"총 4개의 단어가,",1045.52,11
17:25.6,17:26.7,"존재하기 때문에,",1046.66,11
17:27.0,17:28.4,"이렇게 각각의 단어들에 대해서,",1048.42,11
17:28.8,17:30.7,"그 단어에 대한 정보를 포함하고 있는,",1050.72,11
17:30.9,17:33.3,인베딩 값들을 각각 구할 수가 있다는 거죠.,1053.26,11
17:33.4,17:34.2,"이런 식으로 다,",1054.2,11
17:34.3,17:35.7,구할 수가 있는 겁니다.,1055.72,11
17:36.1,17:37.4,"이러한 인베딩 디멘전은,",1057.42,11
17:37.5,17:39.3,"모델 아키텍처를 만드는 사람이,",1059.28,11
17:39.5,17:40.6,"임의로 설정해줄 수 있는,",1060.64,11
17:40.6,17:40.8,데요.,1060.84,11
17:40.9,17:42.0,"원본 논문에서는,",1062,11
17:42.0,17:44.1,512 정도의 값을 사용합니다.,1064.12,11
17:44.3,17:45.3,"물론 이 값은,",1065.26,11
17:45.3,17:47.3,"모델의 아키텍처를 만드는 사람마다,",1067.26,11
17:47.3,17:48.7,다르게 설정할 수가 있는 거예요.,1068.68,11
17:48.9,17:49.5,"아무튼 그래서,",1069.48,11
17:49.6,17:51.3,"이런 식으로 전통적인 인베딩은,",1071.3,11
17:51.4,17:52.6,"네트워크에 넣기 전에,",1072.56,11
17:52.6,17:54.2,"입력 값들을 인베딩 형태로,",1074.2,11
17:54.3,17:55.1,"표현하기 위해서,",1075.06,11
17:55.2,17:57.1,사용하는 레이어라고 볼 수 있습니다.,1077.06,11
17:57.3,17:58.0,"이때 우리가,",1078,12
17:58.2,17:59.6,"시퀀스 투 시퀀스와 같은,",1079.6,12
17:59.8,18:00.7,"RNN 기반의,",1080.72,12
18:00.8,18:02.4,"아키텍처를 사용한다고 하면,",1082.4,12
18:02.7,18:04.3,"RNN을 사용하는 것만으로도,",1084.26,12
18:04.4,18:05.4,"각각의 단어가,",1085.36,12
18:05.5,18:06.6,"RNN에 들어갈 때,",1086.64,12
18:06.8,18:08.2,"순서에 맞게 들어가기 때문에,",1088.18,12
18:08.2,18:10.6,"자동으로 각각의 히든스텔트 값은,",1090.6,12
18:10.6,18:12.4,순서에 대한 정보를 가지게 되는데요.,1092.4,12
18:12.5,18:14.2,"만약에 트랜스포머와 같이,",1094.24,12
18:14.3,18:16.3,"RNN 자체를 사용하지 않는다면,",1096.28,12
18:16.5,18:18.1,"위치에 대한 정보를 주기 위해서,",1098.12,12
18:18.3,18:18.7,"즉,",1098.7,12
18:18.7,18:20.2,"하나의 문장에 포함되어 있는,",1100.18,12
18:20.2,18:21.4,"각각의 단어 중에서,",1101.38,12
18:21.5,18:23.0,"어떤 단어가 앞에 오는 것이고,",1102.96,12
18:23.1,18:24.7,"어떠한 단어가 뒤에 오는 것인지,",1104.66,12
18:24.8,18:26.5,"그러한 정보를 알려주기 위해서는,",1106.5,12
18:26.6,18:28.3,"위치에 대한 정보를 포함하고 있는,",1108.26,12
18:28.4,18:29.9,인베딩을 사용할 필요가 있습니다.,1109.88,12
18:30.2,18:31.1,"이제 이를 위해,",1111.06,12
18:31.2,18:32.0,"트랜스포머에서는,",1112.02,12
18:32.3,18:34.2,"위치에 대한 정보를 인코딩하고 있는,",1114.18,12
18:34.4,18:35.2,"위치 인코딩,",1115.2,12
18:35.3,18:35.6,"즉,",1115.6,12
18:35.6,18:37.2,포지셔널 인코딩을 사용합니다.,1117.16,12
18:37.3,18:38.1,"즉, 이런 식으로,",1118.06,12
18:38.2,18:39.6,"인풋 인베딩 매트릭스와,",1119.58,12
18:39.7,18:40.6,"같은 크기,",1120.58,12
18:40.6,18:42.1,"즉, 같은 디멘전을 가지는,",1122.06,12
18:42.2,18:44.4,"별도의 위치에 대한 정보를 가지고 있는,",1124.44,12
18:44.5,18:46.0,"인코딩 정보를 넣어줘서,",1126.02,12
18:46.1,18:48.4,"각각 엘레멘트 와이즈로 더해줌으로써,",1128.38,12
18:48.5,18:49.4,"각각의 단어가,",1129.42,12
18:49.5,18:51.7,"어떤 순서를 가지는지에 대한 정보를,",1131.66,12
18:51.8,18:53.8,네트워크가 알 수 있도록 만드는 것입니다.,1133.76,12
18:54.0,18:55.0,"이제 그렇게 실제로,",1134.96,12
18:55.3,18:58.1,"위치에 대한 정보까지 포함하고 있는 입력 값을,",1138.12,13
18:58.2,19:00.2,실제 어텐션에 넣어줄 수 있도록 합니다.,1140.2,13
19:00.5,19:02.3,"즉, 이렇게 어텐션이 받는 값은,",1142.3,13
19:02.4,19:04.0,"입력 문장에 대한 정보에다가,",1144.04,13
19:04.0,19:05.8,"실제 위치에 대한 정보까지,",1145.76,13
19:05.8,19:07.8,같이 포함되어 있는 입력 값입니다.,1147.82,13
19:08.1,19:08.7,"이제 그래서,",1148.72,13
19:08.8,19:10.0,"그러한 입력을 받아서,",1149.98,13
19:10.1,19:11.8,"각각의 단어들을 이용해서,",1151.8,13
19:12.0,19:13.2,어텐션을 수행하고요.,1153.18,13
19:13.3,19:15.9,"이제 이렇게 인코더 파트에서 수행하는 어텐션은,",1155.94,13
19:16.0,19:17.3,"self-attention이라고 해서,",1157.3,13
19:17.5,19:19.0,"각각의 단어가 서로에게,",1159.04,13
19:19.1,19:22.0,어떤 연관성을 가지고 있는지를 구하기 위해 사용합니다.,1162,13
19:22.3,19:23.2,"예를 들어 이런 식으로,",1163.16,13
19:23.5,19:25.6,"I am a teacher라고 문장이 들어오게 되면,",1165.56,13
19:25.9,19:27.8,"이 문장을 구성하는 각각의 단어인,",1167.78,13
19:27.8,19:30.3,"I am a teacher가,",1170.3,13
19:30.4,19:32.9,"각각 서로에게 어텐션 스코어를 구해서,",1172.86,13
19:33.0,19:34.0,"각각의 단어는,",1174.02,13
19:34.0,19:37.5,"다른 어떠한 단어와 높은 연관성을 가지는지에 대한 정보를,",1177.52,13
19:37.6,19:39.0,학습하도록 만들 수 있습니다.,1178.96,13
19:39.1,19:39.8,"다시 말해,",1179.78,13
19:39.8,19:40.7,"이러한 어텐션은,",1180.74,13
19:40.8,19:42.5,"이 전반적인 입력 문장에 대한,",1182.54,13
19:42.7,19:45.6,문맥에 대한 정보를 잘 학습하도록 만드는 것입니다.,1185.56,13
19:45.8,19:47.4,"또한 여기에서 추가적으로,",1187.38,14
19:47.7,19:50.1,residual learning과 같은 테크닉이 사용되는데요.,1190.06,14
19:50.3,19:52.2,"이런 residual learning 같은 경우는,",1192.16,14
19:52.3,19:54.5,"대표적인 이미지 분류 네트워크인,",1194.46,14
19:54.5,19:57.5,"레지넷과 같은 네트워크에서 사용되고 있는 기법으로,",1197.5,14
19:57.7,19:59.0,"이렇게 어떠한 값을,",1198.96,14
19:59.0,20:02.2,"레이어를 거쳐서 반복적으로 단순하게 갱신하는 것이 아니라,",1202.22,14
20:02.4,20:04.0,"특정 레이어를 건너뛰어서,",1204,14
20:04.0,20:07.1,복사가 된 값을 그대로 넣어주는 기법을 의미합니다.,1207.06,14
20:07.2,20:09.6,"이런 식으로 특정 레이어를 건너뛰어서,",1209.62,14
20:09.7,20:11.5,"입력할 수 있도록 만드는 것을,",1211.54,14
20:11.6,20:14.0,일반적으로 residual connection이라고 부르고요.,1213.98,14
20:14.1,20:15.2,"이렇게 해줌으로써,",1215.2,14
20:15.3,20:18.1,"전체 네트워크는 기존 정보를 입력 받으면서,",1218.12,14
20:18.4,20:21.7,"추가적으로 잔여된 부분만 학습하도록 만들기 때문에,",1221.68,14
20:21.9,20:23.5,"전반적인 학습 난이도가 낮고,",1223.48,14
20:23.7,20:26.6,"그렇기 때문에 초기의 모델 수렴 속도가 높게 되고,",1226.62,14
20:26.9,20:30.3,"그로 인해 더욱 더 글로벌 옵티마를 찾을 확률이 높아지기 때문에,",1230.34,14
20:30.5,20:32.5,"전반적으로 다양한 네트워크에 대해서,",1232.52,14
20:32.7,20:34.0,"residual learning을 사용해,",1234,14
20:34.0,20:36.7,성능이 좋아지는 것을 많이 목격할 수 있고요.,1236.74,14
20:36.9,20:38.5,"트랜스포머 또한 마찬가지로,",1238.48,14
20:38.5,20:40.6,"그런 아이디어를 전적으로 채택해서,",1240.6,14
20:40.7,20:42.2,성능을 높였다고 할 수 있는 겁니다.,1242.22,14
20:42.4,20:44.5,"그래서 이렇게 attention을 수행해주고,",1244.52,14
20:44.5,20:47.3,"나온 값과 residual connection을 이용해서,",1247.3,14
20:47.4,20:49.7,"바로 더해진 값을 같이 받아서,",1249.68,14
20:49.9,20:51.7,"노멀라이제이션까지 수행한 뒤에,",1251.72,14
20:51.9,20:53.9,그 결과를 내보낼 수 있도록 만듭니다.,1253.88,14
20:54.0,20:55.9,이것이 인코더의 동작 과정이고요.,1255.92,14
20:56.1,20:58.8,"그래서 실제로 이렇게 입력 값이 들어온 이후로부터,",1258.78,15
20:59.0,21:00.1,"attention을 거치고,",1260.1,15
21:00.2,21:02.4,"residual connection 이후에 노멀라이제이션,",1262.36,15
21:02.5,21:03.8,"그 다음에 다시 feed for,",1263.82,15
21:03.8,21:04.9,"레이어를 거친 다음에,",1264.86,15
21:05.0,21:06.7,"마찬가지로 residual learning,",1266.74,15
21:07.0,21:08.7,"그리고 노멀라이제이션을 추가해서,",1268.66,15
21:08.8,21:10.9,"결과적으로 하나의 인코더 레이어에서,",1270.92,15
21:11.1,21:12.8,그 결과 값을 뽑아낼 수 있습니다.,1272.76,15
21:13.0,21:16.6,"이런 식으로 attention과 정교화 과정을 반복하는 방식으로,",1276.58,15
21:16.6,21:18.8,여러 개의 레이어를 중첩해서 사용합니다.,1278.8,15
21:19.1,21:20.4,"이때 한 가지 유의할 점은,",1280.42,15
21:20.4,21:23.4,각각의 레이어는 서로 다른 파라미터를 가집니다.,1283.38,15
21:23.5,21:26.6,"예를 들어, 이 레이어 1번과 레이어 2번에 포함되어 있는,",1286.58,15
21:26.7,21:29.8,"attention 및 feed for 레이어에 사용되는 파라미터는,",1289.82,15
21:29.9,21:30.5,서로 다릅니다.,1290.54,15
21:30.7,21:32.7,"또한 이때 이렇게 레이어를 중첩해서,",1292.74,15
21:32.8,21:33.8,"사용할 수 있다는,",1293.8,15
21:33.8,21:35.3,"점에서 유추할 수 있겠지만,",1295.32,15
21:35.5,21:37.0,"이렇게 입력되는 값과,",1297.04,15
21:37.1,21:39.0,출력되는 값의 dimension은 동일합니다.,1299,15
21:39.3,21:41.2,"이제 그래서 실제로는 다음과 같이,",1301.24,16
21:41.3,21:44.5,전체 인코더와 디코더의 아키텍처를 그려볼 수 있는데요.,1304.48,16
21:44.7,21:46.4,"이렇게 입력 값이 들어온 다음에,",1306.36,16
21:46.5,21:48.3,"여러 개의 인코더 레이어를 반복해서,",1308.26,16
21:48.5,21:50.7,"가장 마지막에 인코더에서 나오게 된,",1310.72,16
21:50.8,21:53.5,그 출력 값은 이렇게 디코더에 들어가게 됩니다.,1313.52,16
21:53.8,21:54.8,"이렇게 해주는 이유는,",1314.78,16
21:54.8,21:57.3,"우리가 앞서 sequence to sequence 모델의,",1317.28,16
21:57.3,21:59.5,"attention 메커니즘을 활용했을 때와 마찬가지로,",1319.5,16
21:59.6,22:01.8,"디코더 파트에서는 매번 출력할 때마다,",1321.8,16
22:02.0,22:03.6,"입력 소스 문장 중에서,",1323.62,16
22:03.6,22:07.1,"어떤 단어에게 가장 많은 초점을 둬야 되는지를,",1327.06,16
22:07.1,22:08.2,알려주기 위함입니다.,1328.22,16
22:08.4,22:10.7,"다시 말해 이렇게 디코더 파트도 마찬가지로,",1330.72,16
22:10.7,22:12.3,"여러 개의 레이어로 구성이 되고,",1332.28,16
22:12.5,22:14.8,"이 마지막 레이어에서 나오게 된 출력 값이,",1334.76,16
22:14.8,22:17.2,"바로 실제로 우리가 번역을 수행한 결과,",1337.22,16
22:17.4,22:18.8,그 출력 단어가 되는 거고요.,1338.76,16
22:18.9,22:21.2,"이때 각각의 레이어는 이 인코더의,",1341.24,16
22:21.3,22:23.6,"마지막 레이어에서 나오게 된 출력 값을,",1343.56,16
22:23.6,22:24.5,입력으로 받는 것입니다.,1344.52,16
22:24.8,22:28.4,이것이 바로 트랜스포머의 가장 기본적인 동작 방식이고요.,1348.36,16
22:28.5,22:30.3,"물론 이런 식으로 인코더 파트에서,",1350.34,16
22:30.4,22:32.4,"마지막 레이어의 출력 값만 받는 게 아니라,",1352.38,16
22:32.6,22:33.6,"이렇게 각각의 레이어를,",1353.6,16
22:33.6,22:33.6,"이렇게 각각의 레이어를,",1353.6,16
22:33.6,22:36.6,레이어마다 출력 값을 받는 기법도 존재하긴 하지만요.,1356.56,16
22:36.6,22:39.3,"아무튼 기본적인 트랜스포머의 아키텍션은 이런 식으로,",1359.28,16
22:39.4,22:42.6,"인코더의 마지막 레이어에서 나오게 된 출력 값을,",1362.58,16
22:42.6,22:45.6,매번 디코더의 레이어에 넣어주는 방식으로 동작합니다.,1365.64,16
22:45.9,22:47.8,"그래서 이때 디코더 또한 마찬가지로,",1367.8,16
22:47.8,22:49.6,"각각 단어 정보를 받아서,",1369.64,16
22:49.8,22:53.7,"이어서 각 단어의 상대적인 위치에 대한 정보를 알려주기 위해,",1373.7,16
22:53.9,22:56.2,인코딩 값을 추가한 뒤에 입력을 넣게 되고요.,1376.24,16
22:56.4,22:58.3,"참고로 하나의 디코더 레이어에서는,",1378.3,16
22:58.3,22:59.9,두 개의 어텐션을 사용하는데요.,1379.88,16
23:00.2,23:02.6,"첫 번째로 보이는 어텐션은 셀프 어텐션으로,",1382.62,16
23:02.7,23:03.6,"인코더 파트와,",1383.6,16
23:03.6,23:09.3,"마찬가지로 각각의 단어들이 서로가 서로에게 어떠한 가중치를 가지는지를 구하도록 만들어서,",1389.3,16
23:09.4,23:13.7,이 출력되고 있는 문장에 대한 전반적인 표현을 학습할 수 있도록 만들고요.,1393.68,16
23:13.8,23:16.7,"이렇게 이어서 디코더 레이어의 두 번째 어텐션에서는,",1396.7,16
23:16.9,23:19.5,인코더에 대한 정보를 어텐션 할 수 있도록 만듭니다.,1399.54,16
23:19.6,23:21.6,"다시 말해 각각의 출력 단어가,",1401.62,16
23:21.7,23:24.9,인코더의 출력 정보를 받아와 사용할 수 있도록 만듭니다.,1404.94,16
23:25.0,23:27.9,"이건 다시 말해 각각의 출력되고 있는 단어가,",1407.94,16
23:28.0,23:32.2,소스 문장에서의 어떤 단어와 연관성이 있는지를 구해주는 겁니다.,1412.24,16
23:32.4,23:33.6,"그래서 여기 보이는,",1413.58,16
23:33.6,23:37.1,어텐션은 일반적으로 인코더 디코더 어텐션이라고 부르고요.,1417.08,16
23:37.2,23:39.7,"그 동작 원리를 간단한 예시로 설명드리자면,",1419.72,16
23:39.8,23:43.3,"예를 들어 입력 문장이 I am a teacher라고 한다면,",1423.28,16
23:43.5,23:46.3,이렇게 출력 값은 차례대로 나는 선생님이다.,1426.32,16
23:46.4,23:48.3,이런 식으로 출력을 내뱉게 될 건데요.,1428.32,16
23:48.4,23:50.3,"이때 출력되고 있는 단어들,",1430.32,16
23:50.4,23:54.0,"예를 들어서 선생님이라고 단어를 번역한다고 하면,",1434.02,16
23:54.2,23:57.4,"그 선생님이라는 단어는 I am a teacher 중에서,",1437.42,16
23:57.5,24:01.2,어떤 단어와 가장 높은 연관성을 가지는지를 구할 수가 있는 겁니다.,1441.16,16
24:01.4,24:03.6,"그러한 정보를 매번 어텐션을 통해서,",1443.56,16
24:03.6,24:04.8,"계산하도록 만들어서,",1444.8,16
24:04.9,24:09.0,"이렇게 인코더 파트에서 나왔던 출력 결과를 전적으로 활용하도록,",1449.04,16
24:09.1,24:10.9,네트워크를 설계할 수 있는 것입니다.,1450.94,16
24:11.4,24:13.5,"즉 그래서 디코러 또한 마찬가지로,",1453.54,16
24:13.5,24:17.9,"입력으로 들어온 입력 디멘전과 이 출력 디멘전이 같도록 만들어서,",1457.86,16
24:18.0,24:21.4,각각의 디코더 레이어는 여러 번 중첩해서 사용할 수 있습니다.,1461.36,16
24:21.6,24:23.4,"즉 다시 말해 이 트랜스포머에서는,",1463.44,17
24:23.6,24:25.5,"마지막 인코더의 레이어의 출력이,",1465.52,17
24:25.6,24:28.7,모든 디코더의 레이어에 입력되는 형식으로 동작하고요.,1468.66,17
24:28.8,24:32.4,이때 전체 레이어의 개수가 4개일 때의 예시는 다음과 같습니다.,1472.38,17
24:32.7,24:33.5,"일반적으로,",1473.5,17
24:33.6,24:37.7,이 레이어의 개수는 인코더와 디코더가 동일하도록 맞추어 주는 경우가 많고요.,1477.74,17
24:37.9,24:42.3,"즉 이렇게 인코더랑 디코더 둘 다 4개의 레이어로 구성되어 있는 걸 확인할 수 있고,",1482.34,17
24:42.5,24:45.6,"이렇게 인코더 파트에서 마지막 레이어의 출력 값이,",1485.64,17
24:45.7,24:48.5,각각의 디코더 레이어에 입력되는 걸 확인할 수 있습니다.,1488.46,17
24:48.6,24:50.1,"여기서 입력이 된다는 의미는,",1490.06,17
24:50.2,24:54.0,"방금 그림에서 확인했던 여기 디코더 파트의 두 번째 어텐션에서,",1493.96,16
24:54.1,24:55.4,"각각의 출력 단어가,",1495.38,16
24:55.5,25:00.4,"입력 단어 중에서 어떤 정보와 가장 높은 연관성을 가지는지를 계산하도록 만들어주는,",1500.38,16
25:00.5,25:03.0,바로 여기 부분에서 사용된다고 보시면 되겠습니다.,1502.96,16
25:03.2,25:03.5,"또한,",1503.54,16
25:03.5,25:06.9,말씀드렸듯이 트랜스포머에서도 인코더와 디코더의 구조를 따릅니다.,1506.88,18
25:07.0,25:07.4,"즉,",1507.44,18
25:07.4,25:10.0,"RNN을 사용하지 않는다는 점이 큰 차이점이고,",1509.98,18
25:10.1,25:13.0,인코더와 디코더를 다수 사용한다는 점이 특징입니다.,1513.04,18
25:13.3,25:14.5,"여기서 재미있는 점은,",1514.46,18
25:14.5,25:17.7,"원래 기본적으로 RNN을 사용할 때는 인코더,",1517.66,18
25:17.7,25:21.2,"즉 LSTM이나 RNN 등은 고정된 크기로 사용하고,",1521.2,18
25:21.3,25:27.6,"이 입력 단어의 개수만큼 반복적으로 인코더 레이어를 거쳐서 매번 히든 스테이트를 만들었다고 하면,",1527.58,18
25:27.8,25:31.8,"트랜스포머에서는 입력 단어 자체가 하나로 쭉 연결되어서,",1531.8,18
25:31.9,25:33.1,"한 번에 입력이 되고,",1533.14,18
25:33.5,25:35.9,한 번에 그에 대한 어텐션 값을 구한다고 할 수 있습니다.,1535.86,18
25:36.0,25:36.5,"즉,",1536.46,18
25:36.5,25:37.0,"다시 말해,",1537.02,18
25:37.0,25:40.8,"RNN을 사용했을 때와 다르게 위치에 대한 정보를 한꺼번에 넣어서,",1540.76,18
25:40.9,25:45.3,"한 번에 인코더를 거칠 때마다 병렬적으로 출력 값을 구해낼 수 있기 때문에,",1545.3,18
25:45.5,25:50.5,RNN을 사용했을 때와 비교하여 일반적으로 계산 복잡도가 더 낮게 형성됩니다.,1550.46,18
25:50.6,25:55.5,"또한 실제로 학습을 수행할 때는 이러한 입력 값들 전부를 한꺼번에 넣을 수 있기 때문에,",1555.46,18
25:55.6,25:58.9,RNN을 사용하지 않고 학습을 진행할 수 있다는 점이 장점인데요.,1558.9,18
25:59.1,26:02.0,"다만 이제 실제로 모델에서 출력 값을 내보낼 때는,",1562.02,18
26:02.1,26:03.5,"마찬가지로 이 디코더 아키더가,",1563.52,18
26:03.5,26:04.8,"아키텍처를 여러 번 사용해서,",1564.82,18
26:04.9,26:09.7,이렇게 EOS가 나올 때까지 반복하도록 만들어서 출력 값을 구하도록 만들 수 있습니다.,1569.72,18
26:09.9,26:14.7,"보시면 이렇게 중간에 컨텍스트 벡터를 압축하는 과정 등이 완전히 생략이 되어 있기 때문에,",1574.74,18
26:14.8,26:20.9,네트워크 자체에서 LSTM과 같은 RNN 구조를 아예 사용할 필요가 없다는 점이 장점이라고 할 수 있습니다.,1580.92,18
26:21.1,26:25.2,이제 그래서 실제로 이 멀티헤드 어텐션이 뭔지 한 번 알아보도록 할게요.,1585.24,16
26:25.5,26:32.1,이렇게 트랜스포머에서 사용되는 각각의 어텐션은 여러 개의 헤드를 가진다고 해서 멀티헤드 어텐션이라고 부르는데요.,1592.14,16
26:32.4,26:33.5,"그 실제 구조는,",1593.48,19
26:33.5,26:34.4,바로 다음과 같습니다.,1594.4,19
26:34.7,26:38.5,바로 오른쪽에 보이는 그림이 멀티헤드 어텐션을 보여주고 있는 그림이고요.,1598.54,19
26:38.6,26:42.1,이때 이렇게 중간에 scaled.product.attention이 사용되는데요.,1602.12,19
26:42.3,26:46.3,이러한 scaled.product.attention은 바로 왼쪽 그림과 같이 구성됩니다.,1606.28,19
26:46.4,26:48.8,"이때 이러한 어텐션 메커니즘을 이해하기 위해서는,",1608.82,19
26:48.8,26:52.5,"Query와 Key, Value가 무엇인지 알 필요가 있는데요.",1612.46,19
26:52.7,26:55.2,이때 Query는 무언가를 물어보는 주체입니다.,1615.22,19
26:55.4,26:57.6,"즉, 어텐션 메커니즘을 간단히 설명하면,",1617.64,19
26:57.8,27:02.5,어떠한 단어가 다른 단어들과 어떠한 연관성을 가지는지를 구하는 것이라 할 수 있는데요.,1622.52,19
27:02.7,27:03.2,"이때,",1623.24,19
27:03.2,27:04.8,"물어보는 주체가 Query이고,",1624.82,19
27:04.9,27:06.7,그 물어보는 대상이 Key입니다.,1626.74,19
27:06.9,27:07.4,"예를 들어,",1627.42,19
27:07.5,27:10.2,"I am a teacher라는 하나의 문장이 있을 때,",1630.16,19
27:10.3,27:13.2,"I am a teacher에 포함되어 있는 각각의 단어가,",1633.16,19
27:13.3,27:16.5,"다른 단어와 얼마나 연관성을 가지는지 측정하기 위해서,",1636.5,19
27:16.7,27:18.2,"Self-Attention을 수행할 수 있는데,",1638.18,19
27:18.4,27:18.9,"이때,",1638.86,19
27:19.0,27:24.2,"I라는 단어가 I am a teacher 각각에 대해서 얼마나 연관성이 있는지 구한다고 치면,",1644.22,19
27:24.3,27:26.2,그때 I가 Query가 되는 거고요.,1646.24,19
27:26.4,27:29.2,I am a teacher 각각 단어들은 Key가 되는 것입니다.,1649.24,19
27:29.5,27:32.5,"즉, 어떠한 단어가 다른 어떠한 단어들에 관해서,",1652.5,19
27:32.5,27:35.3,"어떠한 가중치 값을 가지는지 구하고자 한다면,",1655.26,19
27:35.4,27:40.4,이런 식으로 각각의 Key에 대해서 Attention Score를 구해오는 방식으로 동작하는 것입니다.,1660.44,19
27:40.7,27:44.3,"이때, 그렇게 Score를 구한 뒤에는 실제로 Value 값들과 곱해서,",1664.34,19
27:44.5,27:47.1,결과적인 Attention Value 값을 구할 수 있는 겁니다.,1667.14,19
27:47.4,27:48.7,"내용을 확인해 보시면 이런 식으로,",1668.7,19
27:48.9,27:51.3,"물어보는 주체, 즉 Query가 들어오고,",1671.26,19
27:51.3,27:53.4,"각각의 Attention을 수행할 단어들,",1673.38,19
27:53.5,27:54.9,그 정보가 Key로 들어가는 겁니다.,1674.92,19
27:55.2,27:58.3,"그래서 행렬급을 수행한 뒤에 간단하게 스케일링을 해주고,",1678.26,19
27:58.4,27:59.9,"필요하다면 마스크를 씌워준 다음에,",1679.92,19
28:00.1,28:02.4,"이제 Softmax를 취해서 각각의 Key 중에서,",1682.44,19
28:02.5,28:05.4,"어떤 단어와 가장 높은 연관성을 가지는지를,",1685.36,19
28:05.4,28:06.7,그 비율을 구할 수 있습니다.,1686.68,19
28:06.9,28:10.0,앞에서 공부했었던 Attention Mechanism과 같다고 할 수 있죠.,1690,19
28:10.2,28:13.1,"그렇게 구해진 확률 값과 실제로 Value 값을 곱해서,",1693.12,19
28:13.3,28:17.5,가중치가 적용된 결과적인 Attention Value를 구할 수가 있는 겁니다.,1697.54,19
28:17.8,28:21.6,이제 그러한 과정이 이렇게 Scaled.Paradox Attention에서 수행되는 것이고요.,1701.6,19
28:21.8,28:23.3,"또한 여기에서 참고로,",1703.34,19
28:23.4,28:25.1,"실제로 입력 값이 들어왔을 때,",1705.08,19
28:25.2,28:27.6,그러한 입력 값들은 A 체계로 구분됩니다.,1707.56,19
28:27.7,28:29.7,"즉, 어떠한 입력 문장이 들어왔을 때,",1709.74,19
28:29.8,28:32.4,"이제 그것은 Value, Key, Query로 구분되는데,",1712.42,19
28:32.5,28:38.4,"이때 A 체계의 서로 다른 Value와 Key, Query로 구분될 수 있도록 만드는 것입니다.",1718.38,19
28:38.6,28:39.6,"이렇게 해주는 이유는,",1719.58,19
28:39.6,28:43.5,"A 체계의 서로 다른 Attention Concept을 학습하도록 만들어서,",1723.52,19
28:43.6,28:48.2,더욱더 구분된 다양한 특징들을 학습할 수 있도록 유도해준다는 장점이 있습니다.,1728.22,19
28:48.5,28:52.2,"그래서 이와 같이 입력으로 들어온 값은 세 개로 복제가 되어서,",1732.24,19
28:52.4,28:55.4,"각각 Value, Key, Query로 들어가게 되고,",1735.36,19
28:55.6,29:00.0,"이러한 Value, Key, Query 값들은 Linear Layer, 즉 행렬 곱을 수행해서,",1739.98,19
29:00.1,29:02.5,"A 체계로 구분된 각각의 Query를,",1742.48,19
29:02.5,29:03.7,"A 체계의 서로 다른 Attention Concept을 만들어내게 되고,",1743.68,19
29:03.8,29:06.6,"이때 여기에서 H는 Head의 개수이기 때문에,",1746.6,19
29:06.7,29:10.7,"각각 서로 다른 Head끼리 이렇게 Value, Key, Query의 쌍을 받아서,",1750.66,19
29:10.7,29:12.7,Attention을 수행해서 결과를 내보냅니다.,1752.74,19
29:12.9,29:15.0,"이제 그 다음에 앞서 말씀 드렸듯이,",1754.98,19
29:15.0,29:16.8,"이 Attention Mechanism의 입력 값과,",1756.82,19
29:16.8,29:19.1,"이 출력 값의 Dimension은 같아야 되기 때문에,",1759.06,19
29:19.3,29:22.2,"이렇게 각각의 Head로부터 나오게 된 Attention 값들을,",1762.2,19
29:22.3,29:25.4,"다시 이렇게 Concat을 수행해서 일자로 쭉 붙인 뒤에,",1765.38,19
29:25.6,29:28.0,"마지막으로 이 Linear Layer을 거쳐서,",1768.04,19
29:28.0,29:29.5,Output 값을 내보내게 됩니다.,1769.5,19
29:29.7,29:30.6,"이때 결과적으로,",1770.64,19
29:30.6,29:33.8,"이 입력 값과 출력 값의 Dimension이 같도록 만들어서,",1773.84,19
29:33.9,29:36.1,"이러한 Multi-Head Attention Layer를 사용한 뒤에도,",1776.12,19
29:36.2,29:38.3,Dimension이 줄어들지 않도록 만듭니다.,1778.26,19
29:38.3,29:41.7,"바로 이런 식으로 각각의 Attention Mechanism이 사용되는 것이고요,",1781.68,19
29:41.8,29:43.2,"이러한 Multi-Head Attention은,",1783.22,19
29:43.2,29:46.4,이 전체 아키텍처에서 다 동일한 함수로써 동작합니다.,1786.42,16
29:46.7,29:47.8,"이때 다른 점이라고 한다면,",1787.84,16
29:48.0,29:49.3,"이렇게 사용되는 위치마다,",1789.3,16
29:49.6,29:53.0,"Query랑 Key랑 Value를 어떻게 사용할지가 달라질 수 있는 건데,",1792.98,16
29:53.2,29:54.2,"그런 점을 제외하고,",1794.18,16
29:54.3,29:57.5,기본적인 각각의 Attention Layer의 동작 방식은 같습니다.,1797.48,16
29:57.8,29:58.7,"그래서 예를 들어,",1798.66,16
29:58.7,30:00.6,"이렇게 Encoder, Decoder, Attention에서,",1800.62,16
30:00.6,30:03.3,"Decoder의 출력 단어가 Query가 되는 것이고,",1803.28,16
30:03.4,30:05.5,"각각의 출력 단어를 만들기 위해서,",1805.46,16
30:05.6,30:09.9,"Encoder 파트에서의 어떤 단어를 참고하면 좋은지를 구하기 위해서,",1809.92,16
30:10.1,30:14.2,이 Key랑 Value의 값으로는 Encoder의 출력 값을 쓰겠다는 겁니다.,1814.18,16
30:14.4,30:15.0,"다시 말해,",1815.02,16
30:15.1,30:16.7,"각각의 단어를 출력하기 위해서,",1816.7,16
30:16.9,30:18.8,"어떤 정보를 참고해야 해? 라고,",1818.84,16
30:18.9,30:20.9,"이렇게 Encoder한테 물어보는 것이기 때문에,",1820.92,16
30:21.1,30:23.6,"이 Decoder 파트에 있는 단어가 Query가 되고,",1823.56,16
30:23.8,30:27.7,Encoder 파트에 있는 각각의 값들이 Key와 Value가 된다고 할 수 있습니다.,1827.74,16
30:27.9,30:30.6,"그래서 Multi-Head Attention Layer를 더욱더 자세히,",1830.6,20
30:30.6,30:32.8,자세하게 수식으로 표현하면 다음과 같은데요.,1832.82,20
30:32.8,30:36.3,"자, 이렇게 하나의 Attention은 Query와 Key와 Value를 받고요.",1836.26,20
30:36.4,30:38.3,"이때, Query랑 Key랑 곱해서,",1838.26,20
30:38.4,30:39.4,"각 Query에 대해서,",1839.4,20
30:39.5,30:42.1,각각의 Key에 대한 에너지 값을 구할 수 있겠죠.,1842.1,20
30:42.3,30:43.6,"이제 그런 에너지 값에 대해서,",1843.62,20
30:43.8,30:45.5,"확률 값으로 표현하도록 만들어서,",1845.46,20
30:45.6,30:49.5,실제로 어떤 Key에 대해서 높은 가중치를 가지는지 계산할 수가 있고요.,1849.46,20
30:49.7,30:51.6,"이때 이렇게 Scale Factor로서,",1851.62,20
30:51.7,30:53.1,Root Decay를 사용합니다.,1853.14,20
30:53.4,30:56.0,이때 Decay는 각각의 Key Dimension이 되겠고요.,1856.02,20
30:56.2,30:58.3,"이렇게 특정한 Scale로 나눠주는 이유는,",1858.28,20
30:58.4,31:00.2,"이 Softmax 함수 자체가 가지는,",1860.2,20
31:00.2,31:01.1,"특성을 생각해 보시면,",1861.14,20
31:01.4,31:02.8,"0 근처의 위치에서는,",1862.8,20
31:02.9,31:05.2,"Gradient가 높게 형성되는 것에 반해,",1865.22,20
31:05.3,31:08.5,"값이 들쭉날쭉 조금씩 왼쪽 오른쪽으로 이동하게 되면,",1868.52,20
31:08.8,31:10.3,"교육의 값이 많이 줄어들기 때문에,",1870.34,20
31:10.6,31:13.1,"Gradient Vanishing 문제를 피하기 위한 방법으로,",1873.1,20
31:13.2,31:16.3,이러한 Scale Factor를 넣어줄 수 있다고 논문에선 말하고 있습니다.,1876.34,20
31:16.6,31:17.5,"결과적으로 이렇게,",1877.52,20
31:17.7,31:19.9,"각각의 Query가 각각의 Key에 대해서,",1879.86,20
31:20.1,31:21.9,"어떠한 가중치를 가지는지,",1881.86,20
31:21.9,31:23.2,"Score 값을 구한 뒤에,",1883.24,20
31:23.4,31:25.5,"이제 걔를 실제로 Value 값과 곱해서,",1885.48,20
31:25.7,31:27.8,Attention Value를 만들어낼 수가 있는 것입니다.,1887.84,20
31:28.2,31:29.3,"이때 말씀드렸듯이,",1889.26,20
31:29.3,31:31.2,"입력으로 들어오는 각각의 값에 대해서,",1891.22,20
31:31.4,31:34.3,"서로 다른 Linear Layer를 거치도록 만들어서,",1894.26,20
31:34.4,31:39.0,A측에 서로 다른 각각 Query Key Value 값을 만들 수 있도록 하는 것입니다.,1898.96,20
31:39.4,31:39.9,"이제 이런 식으로,",1899.88,20
31:40.1,31:42.2,"A측에 서로 다른 컨셉을,",1902.22,20
31:42.2,31:45.1,"네트워크가 구분해서 학습하도록 만들므로써,",1905.12,20
31:45.3,31:48.5,Attention을 수행하기 위한 다양한 Feature들을 학습하도록 만듭니다.,1908.52,20
31:48.6,31:51.7,"실제로 나중에 우리가 Attention Score를 시각화해 볼 때는,",1911.72,20
31:51.9,31:53.4,"이 H의 개수만큼,",1913.36,20
31:53.4,31:55.5,Attention Score의 그림이 나오게 됩니다.,1915.46,20
31:55.8,31:56.8,"이제 결과적으로 이렇게,",1916.76,20
31:57.0,31:59.1,"각 Head에 대한 출력 값들을 구할 수 있고,",1919.08,20
31:59.1,32:01.1,"이제 이것을 일자로 쭉 붙인 다음에,",1921.14,20
32:01.3,32:02.1,"마지막으로,",1922.1,20
32:02.1,32:03.5,"아웃풋 매트릭스랑 곱해서,",1923.54,20
32:03.7,32:07.3,결과적인 이 Multi-Headed Attention의 값을 구해낼 수가 있는 것입니다.,1927.32,20
32:07.5,32:09.7,"이제 이런 식으로 매번 입력 값이 들어왔을 때,",1929.7,20
32:09.8,32:11.1,"기본적으로는 이런 식으로,",1931.06,20
32:11.2,32:13.9,"Value와 Key와 Query의 값으로 각각 들어가게 되고,",1933.94,20
32:14.1,32:15.0,"이렇게 나올 때는,",1935.04,20
32:15.2,32:18.5,"입력으로 들어왔던 값과 동일한 Dimension을 가지기 때문에,",1938.46,20
32:18.7,32:20.9,"이러한 Multi-Headed Attention Layer가 포함된,",1940.94,20
32:21.0,32:25.0,하나의 Encoder 혹은 Decoder Layer는 중첩해서 사용될 수 있는 것입니다.,1944.98,20
32:25.2,32:28.5,이제 한번 자세하게 이 Transformer의 동작 원리를 알아보겠습니다.,1948.5,21
32:29.1,32:30.2,"지금은 그냥 간단하게,",1950.16,21
32:30.2,32:32.0,하나의 단어만 있다고 가정을 해볼게요.,1952,21
32:32.2,32:32.7,"이때,",1952.68,21
32:32.8,32:33.8,"Attention을 위해서,",1953.76,21
32:34.0,32:37.7,각각의 Head마다 Query와 Key Value 값을 만들 필요가 있습니다.,1957.68,21
32:38.0,32:38.9,"이제 그래서 이렇게,",1958.86,21
32:39.0,32:42.8,"하나의 단어가 Embedding 차원으로 표현되고 있는 상태에서,",1962.76,21
32:43.0,32:45.3,"이제 여기에서 Linear Layer를 거쳐서,",1965.34,21
32:45.4,32:48.1,각각 Query랑 Key랑 Value 값을 만들 수 있습니다.,1968.14,21
32:48.5,32:51.2,"이때 Embedding 차원을 Dim Model이라고 부를 수 있고요,",1971.22,21
32:51.5,32:55.7,"원본 논문에서는 Embedding 차원을 512 차원으로 사용한다고 언급을 했고요,",1975.74,21
32:55.9,32:58.0,"이때 만약에 Head의 개수가 8개라고 하면,",1978.04,21
32:58.2,32:59.0,"512를,",1979.04,21
32:59.0,33:03.7,"8로 나눈 64만큼 각각의 Query, Key Value의 차원이 구성되는 것입니다.",1983.72,21
33:04.0,33:07.2,"여기 보이는 그림은 그냥 간단하게 Embedding 차원이 4차원이고,",1987.2,21
33:07.3,33:09.0,Head가 2개라고 가정한 상황입니다.,1989.02,21
33:09.2,33:10.0,"즉 이제 이럴 때는,",1990,21
33:10.1,33:12.8,4 곱하기 2짜리 매트릭스가 만들어지겠죠.,1992.8,21
33:13.0,33:16.6,"왜냐면은 이 4차원의 데이터를 2차원의 데이터로 맵핑해야 되기 때문에,",1996.6,21
33:16.7,33:19.5,이렇게 4 곱하기 2 가중치 매트릭스가 사용되는 것입니다.,1999.46,21
33:19.6,33:23.6,"그래서 이런 식으로 Love라는 단어가 4차원으로 표현되어 있다고 하면,",2003.62,21
33:23.9,33:27.6,"이제 이것은 Query, Key, Value 각각 2차원으로 구성되어 있는,",2007.62,21
33:27.7,33:29.0,"데이터로 표현될 수 있는,",2009.02,21
33:29.0,33:29.4,것입니다.,2009.44,21
33:29.7,33:32.7,"이제 이런 식으로 Key랑 Query랑 Value를 다 구했다고 치면,",2012.74,22
33:32.8,33:36.5,"바로 다음의 공식으로 이용해서 실제로 Attention Value를 구할 수가 있는데요,",2016.54,22
33:36.8,33:39.2,"이때 이 Query는 각각의 다른 단어들,",2019.18,22
33:39.2,33:41.4,"이 Key와 행렬급을 수행해서,",2021.38,22
33:41.5,33:44.2,이렇게 하나의 Attention Energy 값을 구할 수가 있는 겁니다.,2024.24,22
33:44.4,33:44.9,"예를 들어,",2024.92,22
33:45.1,33:47.7,"I love you라고 하나의 문장이 들어왔다고 하면,",2027.7,22
33:48.0,33:53.2,"이 I라는 단어는 I에 해당하는 Key, Love에 해당하는 Key, You에 해당하는 Key 값과,",2033.2,22
33:53.2,33:54.4,"각각 곱해져서,",2034.4,22
33:54.5,33:57.1,"하나의 Attention Energy 값을 구할 수가 있는 거고요,",2037.12,22
33:57.3,33:58.7,"이제 아까 전에 말씀드렸듯이,",2038.74,22
33:59.0,34:01.0,"Softmax에 들어가는 값의 크기를,",2040.98,22
34:01.0,34:02.5,"노멀라이제이션 해주기 위해서,",2042.52,22
34:02.7,34:04.7,각각 스케일링 팩터로 나누어 줍니다.,2044.7,22
34:05.0,34:06.6,"이제 이후에 Softmax를 취해서,",2046.62,22
34:06.8,34:08.6,"실제로 각각의 Key 값에 대해서,",2048.64,22
34:09.0,34:12.1,어떠한 가중치를 가지는지를 구해낼 수가 있는 것입니다.,2052.08,22
34:12.5,34:13.4,"여기에 보이는 그림에서는,",2053.38,22
34:13.6,34:19.1,"이 I라는 단어는 I라는 단어와 72%만큼의 높은 연관성을 가지고,",2059.1,22
34:19.4,34:21.4,"이 Love라는 단어와는 15%,",2061.4,22
34:21.4,34:23.3,"그 다음에 You라는 단어와는 13%,",2063.34,22
34:23.3,34:26.7,"이렇게 각각의 가중치를 가진다고 표현할 수가 있는 거고요,",2066.72,22
34:26.9,34:28.5,"이렇게 각각의 가중치 값에다가,",2068.54,22
34:28.5,34:30.6,"이 Value 값들을 각각 곱한 뒤에,",2070.62,22
34:30.7,34:32.0,"전부 더해줘서,",2071.98,22
34:32.0,34:35.0,결과적인 Attention Value 값을 만들어낼 수가 있는 것입니다.,2074.96,22
34:35.1,34:38.1,"즉, 마찬가지로 Weighted Sum을 구할 수가 있다는 거고요,",2078.06,22
34:38.2,34:41.2,바로 이러한 과정을 통해서 실제로 Attention이 수행되는 것입니다.,2081.22,22
34:41.4,34:44.8,"그래서 한번 실제로 전체 문장이 한꺼번에 입력되는,",2084.78,23
34:44.9,34:47.7,이런 행렬과 같은 상황에서 예시를 다시 한번 확인해 보겠습니다.,2087.72,23
34:48.1,34:50.4,"실제로는 이런 식으로 행렬 곡셈 연산을 이용해서,",2090.4,23
34:50.5,34:52.1,"한꺼번에 연산이 가능하고요,",2092.06,23
34:52.2,34:54.0,"I Love You라는 하나의 문장이 있고,",2094.02,23
34:54.3,34:54.6,"그 다음에,",2094.64,23
34:54.8,34:56.8,"Embedding 차원이 4차원이라고 했을 때,",2096.78,23
34:56.8,34:58.5,"바로 이렇게 3x4짜리 메타로드,",2098.52,23
34:58.5,34:59.5,"이 메타로드가 구성되는데요,",2099.48,23
34:59.6,35:01.6,"이때 마찬가지로 하나의 헤드에 있는,",2101.62,23
35:01.7,35:05.2,이 Query Key Value를 구하기 위한 가중치 값이 이렇게 있다고 해볼게요.,2105.24,23
35:05.4,35:07.1,"현재 헤드에서는 바로 이런 식으로,",2107.08,23
35:07.2,35:09.7,"I Love You에 대한 각각의 Query 값,",2109.68,23
35:09.7,35:10.3,"Key 값,",2110.28,23
35:10.3,35:11.9,Value 값이 만들어지는 것입니다.,2111.92,23
35:12.3,35:15.4,"마찬가지로 이렇게 Query와 Key와 Value의 값이 구해졌기 때문에,",2115.4,24
35:15.5,35:17.6,"Attention Value를 구할 수 있게 되는 건데요,",2117.58,24
35:17.6,35:19.3,"이렇게 I와 Love와 You,",2119.3,24
35:19.4,35:23.0,"Query 값들을 한꺼번에 이렇게 각 Key 값과 곱해줘서,",2123,24
35:23.7,35:26.4,Attention Energy를 이렇게 3x3으로 만들어낼 수 있습니다.,2126.4,24
35:26.6,35:28.1,"이때 Attention Energy 값은,",2128.14,24
35:28.1,35:31.2,"말 그대로 각각의 단어가 각각의 Key 값에 대해서,",2131.22,24
35:31.4,35:35.5,얼마나 높은 그 연관성을 표현하는 수치를 부여했는지를 구할 수가 있는 겁니다.,2135.5,24
35:35.7,35:38.2,"즉 이런 식으로 Attention Energy 값은,",2138.24,24
35:38.2,35:40.9,"I Love You 각각에 대해서 구해지는 방식으로,",2140.9,24
35:41.0,35:44.7,이렇게 행과 열은 모두 단어의 개수와 동일한 크기를 가집니다.,2144.74,24
35:44.9,35:49.2,"각각의 단어가 서로에게 어떠한 연관성을 가지는지 구할 수가 있는 것이고요,",2149.22,24
35:49.4,35:50.9,"이제 여기에 Softness를 취해서,",2150.92,24
35:51.1,35:55.7,"각각의 행마다 각 Key에 대한 값들을 확률 값으로 구해낼 수 있도록 만드는 거고요,",2155.72,24
35:55.9,35:57.7,"이제 그러한 가중치 값들과,",2157.68,24
35:57.7,35:59.0,"Value 값을 곱해주어서,",2159.02,24
35:59.1,36:01.9,실제 Attention Value Matrix를 구할 수 있습니다.,2161.94,24
36:02.2,36:04.5,"보시면 이제 이렇게 Attention Value 값 자체는,",2164.54,24
36:04.6,36:08.8,입력되었던 Query와 Key와 Value와 모두 동일한 차원을 가집니다.,2168.8,24
36:09.0,36:10.8,"또한 한 가지 알아두시면 좋은 점은,",2170.84,25
36:10.9,36:12.7,"Mask 행렬을 사용할 수 있다는 점인데요,",2172.66,25
36:12.9,36:14.8,"이 Mask 행렬, 즉 Mask Matrix는,",2174.84,25
36:14.9,36:17.9,특정한 단어를 무시할 수 있도록 하기 위해 사용할 수 있습니다.,2177.94,25
36:18.2,36:20.0,"이렇게 Attention Energy 값이 있을 때에,",2179.98,25
36:20.1,36:23.5,"Attention Energy와 같은 차원의 Mask Matrix를 만들어서,",2183.52,25
36:23.8,36:27.1,"이제 얘를 Element-wise로, 즉 각각의 원소 단위로 곱해주어서,",2187.1,25
36:27.1,36:30.1,어떠한 단어는 참고하지 않도록 만들 수가 있는 것입니다.,2190.12,25
36:30.3,36:31.0,"예를 들어서 이렇게,",2190.98,25
36:31.1,36:34.9,"이 i라는 단어는 이 Love와 U에 해당하는 Key 값은 무시하도록,",2194.94,25
36:35.0,36:38.4,"즉 이 Love와 U는 그냥 Attention 하지 않도록 무시하고자 한다면,",2198.4,25
36:38.5,36:42.6,"이렇게 Attention Energy 값을 전부 다 Minus 무한이라고 할 수 있는,",2202.56,25
36:42.7,36:45.2,"가능한 최대로 작은 값을 넣어주게 되면,",2205.2,25
36:45.4,36:48.8,"실제로 Softmax를 취해서 Attention Score 값이 구해졌을 때,",2208.76,25
36:48.8,36:51.8,"고려하지 않도록 처리가 된 그런 단어들에 대해서는,",2211.76,25
36:51.8,36:54.3,모두 0%의 가중치 값을 가지게 됩니다.,2214.34,25
36:54.5,36:56.8,"즉 Mask Matrix를 그냥 씌워줌으로써,",2216.84,25
36:56.9,36:57.1,"등장하는,",2217.08,25
36:57.1,37:00.7,특정한 단어는 무시해서 Attention을 수행하지 않도록 만들 수가 있는 것입니다.,2220.74,25
37:00.9,37:03.0,"이와 같이 Mask Matrix를 이용해서,",2223.04,25
37:03.1,37:06.0,"이러한 Attention Energy 값에 Mask를 적용함으로써,",2226,25
37:06.1,37:10.3,특정 단어는 그냥 무시해서 Attention을 수행하지 않도록 만들 수가 있는 것입니다.,2230.34,25
37:10.6,37:13.9,"그래서 결과적으로 이렇게 각각의 Head마다 입력으로 들어온,",2233.94,26
37:13.9,37:17.3,"Query와 Key와 Value와 같은 차원의 Vector를 만들어 내기 때문에,",2237.32,26
37:17.5,37:22.0,"이렇게 각 Head마다 Query와 Key와 Value의 값들을 각각 넣어서,",2241.96,26
37:22.1,37:26.7,"Attention을 수행한 값들을 이렇게 다 Head1부터 HeadH까지라고 했을 때,",2246.68,26
37:26.7,37:29.3,"이러한 정보들을 다 일자로 쭉 연결하게 되면,",2249.26,26
37:29.4,37:34.4,다시 맨 처음에 입력이 되었던 이런 입력 Dimension과 같은 Dimension을 가지게 되는데요.,2254.44,26
37:34.7,37:39.3,"다시 말해 이런 식으로 Multi-Head Attention은 각각의 Head에 대해서 Attention을 수행한 뒤에,",2259.28,27
37:39.4,37:41.6,"그러한 결과를 다시 쭉 이어붙이기 때문에,",2261.62,27
37:41.7,37:47.3,결과적으로 만들어진 Matrix의 이 열의 개수는 원래 입력의 Embedding 차원과 동일한 값을 가집니다.,2267.32,27
37:47.5,37:50.6,"그렇기 때문에 이제 마지막에 이 W가중치 값으로,",2270.56,27
37:50.6,37:54.7,"Dmodel 곱하기 Dmodel 차원을 가지는 Matrix를 곱해 줌으로써,",2274.68,27
37:54.8,37:56.7,"결과적인 Multi-Head Attention의 값을,",2276.66,27
37:56.7,37:57.2,구할 수 있고요.,2277.18,27
37:57.4,38:01.2,"이제 이렇게 하더라도 결과 값은 입력 Dimension과 정확히 동일하기 때문에,",2281.2,27
38:01.3,38:05.8,이러한 Multi-Head Attention을 수행한 뒤에도 차원이 동일하게 유지가 된다는 점이 특징입니다.,2285.76,27
38:05.9,38:10.5,또한 앞서 간단하게 말씀드렸듯이 이 Transformer에는 세 가지 종류의 Attention이 사용되는데요.,2290.46,28
38:10.7,38:13.7,"Transformer에 쓰이는 Attention은 항상 Multi-Head Attention으로,",2293.66,28
38:13.7,38:15.8,"Head가 여러 개인 Attention이라 볼 수 있는데,",2295.78,28
38:16.0,38:18.2,"이제 그러한 Attention이 사용되는 위치에 따라서,",2298.24,28
38:18.3,38:21.7,"Encoder Self-Attention, 그리고 Masked Decoder Self-Attention,",2301.7,28
38:22.1,38:24.5,Encoder Decoder Attention 이 세 가지 종류가 존재합니다.,2304.46,28
38:24.7,38:26.6,기본적으로 Encoder에 Self-Attention을 사용하는 것입니다.,2306.64,28
38:26.7,38:30.6,"저는 말씀드렸듯이 각각의 단어가 서로에게 어떠한 연관성을 가지는지를,",2310.62,28
38:30.7,38:32.5,"Attention을 통해서 구하도록 만들고,",2312.52,28
38:32.6,38:37.0,전체 문장에 대한 Representation을 Learning할 수 있도록 만든다는 점이 특징이고요.,2316.96,28
38:37.1,38:39.6,"다만 이제 Decoder 파트에서 Self-Attention을 수행할 때는,",2319.64,28
38:39.8,38:44.3,"이렇게 각각의 출력 단어가 다른 모든 출력 단어를 전부 참고하도록 만들진 않고,",2324.28,28
38:44.5,38:47.5,앞쪽에 등장했던 단어들만 참고할 수 있도록 만듭니다.,2327.52,28
38:47.6,38:48.7,"예를 들어 출력 문장이,",2328.74,28
38:48.9,38:51.2,"나는 축구를 했다 라고 하면은,",2331.24,28
38:51.2,38:53.9,"우리가 축구를 이라는 단어를 출력할 때 있어서,",2333.86,28
38:54.1,38:56.2,"했다 라고 이렇게 뒤쪽에 나오는 단어가,",2336.22,28
38:56.2,38:58.5,"무엇인지 참고할 수 있도록 만들어버리면,",2338.46,28
38:58.6,39:01.1,"그것은 이제 일종의 치팅처럼 동작을 하기 때문에,",2341.06,28
39:01.1,39:03.7,모델이 정상적으로 학습이 되기가 어렵습니다.,2343.74,28
39:03.9,39:06.5,"그렇기 때문에 이 Decoder 파트에서 Attention을 수행할 때는,",2346.52,28
39:06.7,39:10.9,"이렇게 각각의 단어에 대해서 이 앞쪽 단어들만 참고할 수 있도록 만들므로써,",2350.88,28
39:10.9,39:14.8,치팅을 하지 않고 정상적으로 모델이 학습될 수 있도록 만드는 것입니다.,2354.78,28
39:15.0,39:16.9,"마지막으로 Encoder Decoder Attention은,",2356.9,28
39:16.9,39:18.2,"Query가 Decoder에 있고,",2358.22,28
39:18.4,39:21.3,각각의 Key와 Value는 Encoder에 있는 상황을 의미하는 것입니다.,2361.28,28
39:21.5,39:22.0,"예를 들어,",2362,28
39:22.2,39:23.6,"난 널 좋아해 라고,",2363.58,28
39:23.7,39:26.0,"I like you 라고 문장이 들어왔을 때,",2366.04,28
39:26.0,39:28.7,"출력 문장이 난 널 좋아해 라고 나온다고 하면,",2368.74,28
39:28.9,39:32.0,"각각의 출력 단어들이 이러한 입력 단어들 중에서,",2372.04,28
39:32.2,39:36.2,어떤 정보에 더욱 더 많은 가중치를 두는지를 구할 수 있어야 되는데요.,2376.22,28
39:36.4,39:39.2,"이제 그러한 과정에서 이 Decoder 파트에 있는 Query 값이,",2379.16,28
39:39.2,39:42.5,"이렇게 Encoder 파트에 있는 Key와 Value 값을 참조한다고 해서,",2382.54,28
39:42.7,39:44.8,Encoder Decoder Attention이라고 부르는 것입니다.,2384.84,28
39:45.0,39:47.7,또한 이어서 Self-Attention에 대해서 알아볼 건데요.,2387.74,29
39:47.8,39:49.6,"실제로 이러한 Self-Attention은,",2389.62,29
39:49.6,39:52.7,말씀드렸듯이 Encoder와 Decoder 모두에서 사용되고요.,2392.74,29
39:52.9,39:56.0,"시각화 과정을 통해서 Attention Score로 나온 값을 그려보는,",2396.02,29
39:56.0,39:56.5,어떤 단어를 구할 수 있는지 알아볼 수 있습니다.,2396.48,29
39:56.6,39:58.6,"매번 입력 문장에서 각 단어가,",2398.6,29
39:58.7,40:01.7,다른 어떤 단어와 연관성이 높은지를 구할 수가 있는 건데요.,2401.72,29
40:01.7,40:04.2,"예를 들어 이런 식으로 하나의 입력 문장이 들어왔을 때,",2404.22,29
40:04.3,40:06.9,"각각의 단어들은 다른 모든 단어에 대해서,",2406.9,29
40:07.1,40:09.2,Attention Score 값을 구할 수가 있는 겁니다.,2409.18,29
40:09.4,40:10.1,"예를 들어 이렇게,",2410.08,29
40:10.4,40:12.6,A boy who is looking at the tree is surprised.,2412.58,29
40:12.9,40:14.7,"이런 식으로 문장이 있다고 했을 때,",2414.66,29
40:14.8,40:17.5,"각각의 단어들은 다른 단어 모두에 대해서,",2417.46,29
40:17.7,40:21.7,얼마나 가중치를 부여할지를 Attention을 통해서 계산할 수가 있는 건데요.,2421.7,29
40:21.7,40:24.0,"예를 들어 이렇게 It이란 단어를 출력한다고 하면,",2424.04,29
40:24.0,40:25.7,"이러한 It이 의미하는 단어는,",2425.72,29
40:25.7,40:28.3,앞쪽에 있는 Tree와 이렇게 동일한 It이 되겠죠.,2428.32,29
40:28.5,40:32.4,"그렇기 때문에 실제로 Attention Score를 시각적으로 출력하도록 만들면,",2432.44,29
40:32.5,40:34.4,"이런 식으로 Tree와 It과 관련해서,",2434.38,29
40:34.5,40:38.0,더 높은 Score를 가지는 방식으로 학습이 될 가능성이 높습니다.,2437.98,29
40:38.1,40:42.1,"그래서 이런 식으로 각각의 단어들이 서로 어떠한 연관성을 가지는지를,",2442.14,29
40:42.2,40:45.0,Self-Attention 과정을 통해서 시각화해 볼 수가 있습니다.,2445.04,29
40:45.4,40:47.8,"자, 이제 결과적으로 우리가 앞에서 확인했던 이,",2447.78,19
40:47.9,40:51.7,Transformer의 전체 아키텍처에 포함되어 있는 내용들을 하나씩 확인해 보았는데요.,2451.7,19
40:51.9,40:54.0,"이렇게 Encoder 파트에선 입력 값이 들어오고,",2454.02,19
40:54.0,40:58.4,위치에 대한 정보를 반영해 준 입력을 실제로 첫 번째 레이어에 넣어주게 되고요.,2458.42,19
40:58.5,41:02.5,"이제 이렇게 Encoder 레이어는 N번 만큼 반복이 되어서 중첩해 사용이 되고,",2462.54,19
41:02.7,41:05.8,"이제 그렇게 나온 마지막 레이어의 Encoder의 출력 값이,",2465.78,19
41:05.8,41:08.2,각각의 Decoder 레이어에 들어간다고 보시면 됩니다.,2468.2,19
41:08.5,41:12.0,"이제 그래서 마찬가지로 Decoder 레이어도 N번만큼 중첩이 되어서,",2472,19
41:12.2,41:16.5,"가장 마지막에 나온 그 출력 값에 Linear 레이어와 Softmax를 취해서,",2476.48,19
41:16.6,41:18.9,각각의 출력 단어를 만들어 낼 수가 있는 것입니다.,2478.92,19
41:19.1,41:21.3,"다만 이제 우리가 한 가지 얘기 안 한 게 있다고 하면,",2481.34,19
41:21.5,41:23.6,"바로 위치 정보를 어떤 식으로 넣을지에 대한,",2483.64,19
41:24.0,41:24.6,Encoding 함수입니다.,2484.62,19
41:24.8,41:28.7,"원본 논문에서는 하나의 문장에 포함되어 있는 각각의 단어들에 대한,",2488.7,30
41:28.8,41:32.0,"상대적인 위치에 대한 정보를 모델에게 알려주기 위해서,",2491.96,30
41:32.1,41:34.2,바로 주기 함수를 활용한 공식을 사용하는데요.,2494.24,30
41:34.4,41:36.0,실제 공식은 바로 다음과 같습니다.,2496.02,30
41:36.5,41:39.1,이때 p는 포지셔널 인코딩의 약자고요.,2499.06,30
41:39.2,41:41.8,이때 이 for는 각각의 단어 번호가 되겠고요.,2501.78,30
41:41.9,41:46.1,이때 이 i는 각각의 단어에 대한 Embedded 값의 위치 하나하나를 의미합니다.,2506.06,30
41:46.3,41:49.9,"이제 그래서 이런 식으로 Sine 함수와 같은 주기 함수 값을,",2509.86,30
41:49.9,41:51.6,인코딩을 위해서 사용하는데요.,2511.64,30
41:51.9,41:53.3,"이렇게 파라미터로 들어와 있는,",2513.34,30
41:53.3,41:56.0,"만과 같은 값이나 이런 Sine과 Cosine 함수는,",2516.04,30
41:56.1,41:58.4,"이렇게 기본적인 Sine 함수와 Cosine 함수 말고,",2518.38,30
41:58.5,42:00.4,다른 주기 함수를 사용할 수도 있는 거고요.,2520.44,30
42:00.6,42:03.9,"아무튼 우리 네트워크가 각각의 입력 문장에 포함되어 있는,",2523.92,30
42:04.0,42:07.3,"각 단어들의 상대적인 위치에 대한 정보를 알 수 있도록,",2527.3,30
42:07.4,42:10.0,"이러한 주기성을 학습할 수 있도록 만들기만 한다면,",2530.02,30
42:10.2,42:11.8,어떤 함수가 들어와도 사용할 수 있습니다.,2531.82,30
42:12.0,42:15.7,"그래서 원본 논문에서도 이렇게 Sine 함수와 Cosine 함수를 이용해서,",2535.74,30
42:15.9,42:18.0,"정해진 그런 함수 값을 사용할 수도 있지만,",2538.04,30
42:18.3,42:21.3,"우리가 위치에 대한 Embedded 값을 따로 학습하도록 만들어서,",2541.3,30
42:21.4,42:23.3,네트워크에 넣을 수 있다고 말하고 있고요.,2543.32,30
42:23.3,42:24.8,"실제로 그렇게 넣었을 때도,",2544.8,30
42:24.9,42:27.2,"이렇게 Sine 함수와 Cosine 함수를 이용했을 때와,",2547.2,30
42:27.3,42:29.9,실제 성능상의 차이는 거의 없었다고 말하고 있습니다.,2549.92,30
42:30.2,42:33.4,"그래서 실제로 트랜스포머 이후에 나온 다양한 아키텍처에서는,",2553.42,30
42:33.6,42:35.1,"이러한 주기 함수를 사용하지 않고,",2555.14,30
42:35.4,42:39.4,그냥 학습이 가능한 형태로 별도의 Embedding 레이어를 사용하기도 합니다.,2559.36,30
42:39.7,42:42.6,"더욱 더 자세하게 실제로 이러한 위치 인코딩이,",2562.6,31
42:42.6,42:45.1,어떤 식으로 들어갈 수 있는지를 확인해 보시면요.,2565.14,31
42:45.2,42:48.6,예를 들어 이렇게 입력 문장이 WeRD1이라고 한번 해볼게요.,2568.56,31
42:48.8,42:52.7,이때 각각의 단어들은 딥 마더 만큼의 Embedding 차원을 가지게 됩니다.,2572.68,31
42:52.9,42:53.3,"지금 도움드리지만,",2573.32,31
42:53.3,42:55.1,이 그림에서는 이 Embedding 차원이 8이 되겠죠.,2575.12,31
42:55.3,42:58.9,"이 Sine과 Cosine 함수에 들어가는 이 포스 값과 I 값은,",2578.86,31
42:58.9,43:03.2,이러한 입력 행렬 값에서의 각각의 인덱스 값과 동일하게 들어가는 것입니다.,2583.18,31
43:03.4,43:05.1,"예를 들어 여기는 0,3이 되는데요.",2585.08,31
43:05.3,43:07.9,첫 번째 단어의 네 번째 Embedding이기 때문이죠.,2587.92,31
43:08.1,43:11.1,"그래서 이제 각각의 값들이 이러한 함수에 들어가게 돼서,",2591.1,31
43:11.2,43:14.2,"바로 입력 값과 정확히 동일한 디멘전을 가지는,",2594.18,31
43:14.2,43:15.9,위치 인코딩을 만들어낼 수 있습니다.,2595.88,31
43:16.1,43:19.2,"그래서 이제 이 값을 Element-wise로 다 더해줘서,",2599.2,31
43:19.3,43:21.2,"원소 by 원소로 다 더해준 뒤에,",2601.24,31
43:21.3,43:22.4,"그 값을 실제로,",2602.38,31
43:22.4,43:26.7,각 인코더와 디코더 레이어의 입력 값으로 사용을 한다고 보시면 되겠습니다.,2606.72,31
43:27.0,43:30.2,"여기 보이는 코드는 그냥 간단하게 N과 디멘전에 대해서,",2610.2,32
43:30.5,43:34.7,"실제로 어떤 식으로 각 단어의 위치에 대한 인코딩 정보가 들어가는지를,",2614.74,32
43:34.8,43:36.1,그림으로 표현한 것인데요.,2616.06,32
43:36.3,43:40.1,바로 이렇게 간단하게 맷플롤 라이브러리를 이용해서 그림을 그려볼 수 있습니다.,2620.1,32
43:40.4,43:45.2,"자, 마찬가지로 전체 실습 코드는 제 GitHub 저장소에 올려놓았으니까요.",2625.16,32
43:45.2,43:46.2,확인하실 수 있습니다.,2626.16,32
43:46.5,43:47.8,"이렇게 아래쪽에 내려와 보시면,",2627.78,32
43:48.1,43:49.6,Attention is all you need.,2629.62,32
43:49.8,43:51.2,Code Practice 보이시죠?,2631.24,32
43:51.3,43:52.3,"여기 들어오셔서,",2632.34,32
43:52.3,43:54.4,전체 코드를 확인해 보실 수 있습니다.,2634.44,32
43:54.6,43:57.3,"전체 코드는 여러분들의 개인 개발 환경이 아닌,",2637.26,32
43:57.3,43:59.6,"무료 딥러닝 개발 환경인 CoreApp에서,",2639.56,32
43:59.6,44:01.8,바로 실행해 볼 수 있도록 준비를 해놓았습니다.,2641.84,32
44:02.1,44:04.3,"그래서 여러분들은 구글 아이디만 있으시면,",2644.26,32
44:04.4,44:05.6,"바로 여기 링크 들어오셔서,",2645.6,32
44:05.8,44:08.2,CoreApp에서 즉시 실행해 보실 수 있습니다.,2648.24,32
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
,,,,
