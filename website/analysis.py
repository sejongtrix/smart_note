import os
import json
import whisper
import torch
from pdf2image import convert_from_path
from PIL import Image
import PyPDF2
import pandas as pd
import requests
import fitz


from io import BytesIO
from torchvision import transforms
import torch.nn.functional as F

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Whisper ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ (ì§€ì—° ë¡œë“œ)
whisper_model = None

def pdf2images(pdf_path):
    #pdf_path = r"C:\Users\good1\Documents\ì¹´ì¹´ì˜¤í†¡ ë°›ì€ íŒŒì¼\ì œëª©ì„-ì…ë ¥í•´ì£¼ì„¸ìš”_-2.pdf"
    def extract_images_from_pdf(pdf_path):
        # PDF íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œê±°)
        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
        
        # ì €ì¥í•  ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        image_list = []

        # PDF ì—´ê¸°
        doc = fitz.open(pdf_path)

        for page_num in range(len(doc)):
            page = doc[page_num]
            image_infos = page.get_images(full=True)

            for img_index, img in enumerate(image_infos):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                image_ext = base_image["ext"]
                
                # PIL ì´ë¯¸ì§€ ê°ì²´ ìƒì„±
                image = Image.open(BytesIO(image_bytes))

                # ì´ë¯¸ì§€ ì´ë¦„ ë§Œë“¤ê¸°
                image_name = f"{pdf_name}_{page_num+1}_{img_index+1}.{image_ext}"
                
                # ë¦¬ìŠ¤íŠ¸ì— íŠœí”Œë¡œ ì €ì¥: (ì´ë¯¸ì§€ ì´ë¦„, PIL ì´ë¯¸ì§€ ê°ì²´)
                image_list.append((image_name, image))

        doc.close()
        return image_list


    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ê²Œ ì¡°ì •
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ê¸°ì¤€
                            std=[0.229, 0.224, 0.225]),
    ])

    def load_model(model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):
        model = torch.load(model_path, map_location=device, weights_only=False)
        model.eval()
        return model

    def predict_and_rename_images(image_list, model, transform, device='cuda' if torch.cuda.is_available() else 'cpu'):
        renamed_images = []
        for image_name, image in image_list:
            # RGBA â†’ RGBë¡œ ë³€í™˜
            image = image.convert("RGB")

            # ì „ì²˜ë¦¬
            input_tensor = transform(image).unsqueeze(0).to(device)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€

            # ì¶”ë¡ 
            #with torch.no_grad():
            #    outputs = model(input_tensor)
            #    predicted_class = torch.argmax(outputs, dim=1).item()
            with torch.no_grad():
                outputs = model(input_tensor)
                probs = F.softmax(outputs, dim=1)
                predicted_class = torch.argmax(probs, dim=1).item()
                # print("ì˜ˆì¸¡ í™•ë¥ :", probs.cpu().numpy())
            # ìƒˆë¡œìš´ ì´ë¦„ ì§€ì •
            new_image_name = image_name.rsplit('.', 1)[0] + f'_{predicted_class}.' + image_name.rsplit('.', 1)[1]

            renamed_images.append((new_image_name, image))
        return renamed_images


    images = extract_images_from_pdf(pdf_path)
    model = load_model("model_full.pth")

    # 3. ì¶”ë¡  ë° ì´ë¦„ ë³€ê²½
    renamed_images= predict_and_rename_images(images, model, transform)
    output_dir = os.path.join("static", "output_images")
    os.makedirs(output_dir, exist_ok=True)
    useful_images = []
    for filename, image in renamed_images:
        save_path = os.path.join(output_dir, filename)
        useful_images.append(filename)
        image.save(save_path)
    print(useful_images)
    del model
    torch.cuda.empty_cache()
    return useful_images
def load_whisper_model():
    """Whisper ëª¨ë¸ì„ ì§€ì—° ë¡œë“œí•©ë‹ˆë‹¤."""
    global whisper_model
    if whisper_model is None:
        print("Whisper large ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
        whisper_model = whisper.load_model("medium", device=DEVICE)
        print("Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    return whisper_model

PROCESSED_FOLDER = 'processed'
if not os.path.exists(PROCESSED_FOLDER):
    os.makedirs(PROCESSED_FOLDER)

def transcribe_video(video_path, filename,lang):
    """ìœ„ìŠ¤í¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ íŒŒì¼ì˜ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    print("ë°›ì€ lang ê°’",lang)
    try:
        print(f"{video_path}ì˜ ìŒì„± ì¸ì‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        model = load_whisper_model()
        
        # ë” ì•ˆì •ì ì¸ ì„¤ì •ìœ¼ë¡œ ìŒì„± ì¸ì‹ ìˆ˜í–‰
        if lang == 0:
            result = model.transcribe(
                video_path, 
                verbose=True, 
                fp16=False,  # fp16ì„ ë¹„í™œì„±í™”í•˜ì—¬ ì•ˆì •ì„± í–¥ìƒ
                task='transcribe',  # ë²ˆì—­ì´ ì•„ë‹Œ ì „ì‚¬ ì‘ì—…
                temperature=0.2, )
        if lang == 1:
            result = model.transcribe(
                video_path, 
                verbose=True, 
                fp16=False,  # fp16ì„ ë¹„í™œì„±í™”í•˜ì—¬ ì•ˆì •ì„± í–¥ìƒ
                task='translate',  # ë²ˆì—­ì´ ì•„ë‹Œ ì „ì‚¬ ì‘ì—…
                temperature=0.2,
            )
        
        # ê²°ê³¼ ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦
        df = pd.read_csv("slide_similarity_log.csv")
        page_segments = []
        prev_page = None
        for i, row in df.iterrows():
            if row['page'] != prev_page:
                page_segments.append({"time": row['seconds'], "page": int(row['page'])})
                prev_page = row['page']
        segments = []
        for segment in result.get('segments', []):
            segment_start = segment.get('start',0.0)
            current_page = 0
            for pseg in page_segments:
                if segment_start >= pseg['time']:
                    current_page = pseg['page']
                else:
                    break
            segments.append({
                'start': segment.get('start', 0.0),
                'end': segment.get('end', 0.0),
                'text': segment.get('text', '').strip(),
                'page': current_page
            })
        
        transcript_data = {
            'filename': filename,
            'transcript': result.get('text', '').strip(),
            'segments': segments,
            'language': result.get('language', 'unknown')
        }
        transcript_file = os.path.join(PROCESSED_FOLDER, f"{os.path.splitext(filename)[0]}_transcript.json")
        with open(transcript_file, 'w', encoding='utf-8') as f:
            json.dump(transcript_data, f, ensure_ascii=False, indent=2)
        
        print("ìŒì„± ì¸ì‹ ì™„ë£Œ ë° íŒŒì¼ ì €ì¥ ì„±ê³µ.")
        del model
        torch.cuda.empty_cache()
        return transcript_data,segments

    except Exception as e:
        print(f'ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}')
        return {'error': str(e)}

def summarize_ai(raw_df,pdf_path,lang):
    df = pd.DataFrame(raw_df)
    grouped = df.groupby('page', as_index=False).agg({
    'text': ' '.join  
    })

    example_df = grouped

    doc = fitz.open(pdf_path)

    # í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì €ì¥
    data = []
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        text = page.get_text()
        data.append({
            'page': page_num,
            'text': text.strip()
        })

    # DataFrameìœ¼ë¡œ ì €ì¥
    page_df = pd.DataFrame(data)


    # Ollama ì„œë²„ URL
    OLLAMA_URL = 'http://localhost:11434/api/generate'

    # ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    results = []

    for idx, row in example_df.iterrows():
        text1 = row['text'] 

        page_num = row['page']  # page ë²ˆí˜¸
        # page_dfì—ì„œ í•´ë‹¹í•˜ëŠ” pageì˜ text ê°€ì ¸ì˜¤ê¸°
        matched_rows = page_df[page_df['page'] == page_num]

        if matched_rows.empty:
            print(f"â— page {page_num}ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        text2 = matched_rows.iloc[0]['text']

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if lang == 0:
            prompt = f"""
            ë¬¸ì¥1ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì¥2ë¥¼ ë³´ì™„í•´ì¤˜. ë‹¨, ë¬¸ì¥1ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ê³  ë¬¸ì¥2 ì•ˆì˜ í‘œí˜„ì´ë‚˜ ë§¥ë½ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ í’ë¶€í•˜ê²Œ í•´ì¤˜.

            ê²°ê³¼ëŠ” ë³´ì™„ëœ ë¬¸ì¥2ë§Œ ì¶œë ¥í•´. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë‚˜ ì„¤ëª…ì€ ìƒëµí•˜ê³ , ë„ˆì˜ ì‚¬ê³ ê³¼ì •ë„ ë“œëŸ¬ë‚´ì§€ ë§ˆ.

            ë¬¸ì¥1: {text1}
            ë¬¸ì¥2: {text2}
            """
        if lang == 1:
            prompt = f"""
            The content of sentence 1 is based on the content of sentence 2. Do not add any content that is not in sentence 1, and make the expression or context of sentence 2 more specific or rich.

            Only output the completed sentence 2. Do not include system prompts or explanations, and do not reveal your thought process.
            Sentence1: {text1}
            Sentence2: {text2}

            (((Answer in english.)))
            """

        # ìš”ì²­ payload
        payload = {
            "model": "gemma3:12b",
            "prompt": prompt,
            "stream": False
        }

        # ìš”ì²­ ë³´ë‚´ê¸°
        response = requests.post(OLLAMA_URL, json=payload)
        # ì‘ë‹µ ì²˜ë¦¬
        if response.status_code == 200:
            result = response.json()
            print(f"(index {idx}):")
            print(result['response'])
            results.append({
                'index': idx,
                'page': page_num,
                'text1': text1,
                'text2': text2,
                'gemma_response': result['response']
            })
        else:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ (index {idx}): {response.status_code}")
            print("text1 : ",text1)
            print("text2 :", text2)
            print(response.text)
            raise Exception("ê°•ì œë¡œ ì—ëŸ¬ ë°œìƒ!")
            print(response.text)

    print("aiìš”ì•½ì™„ë£Œ")
    output_df = pd.DataFrame(results)
    #output_df.to_csv('gemma_opinions.csv', index=False)
    output_df.to_json('gemma_opinions.json', orient='records', force_ascii=False, indent=2)
    return output_df

def process_pdf(pdf_path, filename,video_path):
    """PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # ì—¬ê¸°ì„œ pdf ì²˜ë¦¬ ë¡œì§ ë‹¤ êµ¬í˜„ í•´ë²„ë¦¬ìê³  
    try:
        print(f"{pdf_path}ì˜ PDF ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        # 1. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_content = ""
        with open(pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text_content += page.extract_text() + "\n\n"

        # 2. PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        images = convert_from_path(pdf_path)
        image_paths = []
        for i, image in enumerate(images):
            image_filename = f"{os.path.splitext(filename)[0]}_page_{i+1}.png"
            image_filepath = os.path.join(PROCESSED_FOLDER, image_filename)
            image.save(image_filepath, 'PNG')
            image_paths.append(image_filename)

        pdf_data = {
            'filename': filename,
            'text_content': text_content,
            'image_paths': image_paths,
            'total_pages': len(images)
        }

        pdf_file = os.path.join(PROCESSED_FOLDER, f"{os.path.splitext(filename)[0]}_processed.json")
        with open(pdf_file, 'w', encoding='utf-8') as f:
            json.dump(pdf_data, f, ensure_ascii=False, indent=2)

        print("PDF ì²˜ë¦¬ ë° íŒŒì¼ ì €ì¥ ì„±ê³µ.")
        # 3. pdfì™€ ë™ì˜ìƒ íŒŒì¼ë¡œ 
        #real_first_page = matching_pdf_and_video(images, video_path)
        matching_pdf_and_video(images,video_path)
        return pdf_data #,real_first_page

    except Exception as e:
        print(f'PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}')
        return {'error': str(e)}
    
def matching_pdf_and_video(images, video_path):
    """PDF ë°ì´í„°ì™€ ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ ë§¤ì¹­í•©ë‹ˆë‹¤."""
    import torch
    import torchvision.models as models
    import torchvision.transforms as transforms
    from PIL import Image
    from torch.nn.functional import cosine_similarity
    import io
    import cv2
    import numpy as np
    import csv
    import math

    print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = models.resnet50(pretrained=True)
    model = torch.nn.Sequential(*list(model.children())[:-1]) 
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")


    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
    ])

    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    output_path = "slide_similarity_log.csv"
    csv_file = open(output_path, mode="w", newline="", encoding="utf-8")
    csv_writer = csv.writer(csv_file)
    csv_writer.writerow(["seconds", "page", "similarity"])
    print(f"ğŸ“ CSV ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™” ì™„ë£Œ: {output_path}")

    # MSE ìœ ì‚¬ë„ ì¸¡ì • í•¨ìˆ˜
    def mse(imageA, imageB):
        err = np.sum((imageA.astype("float") - imageB.astype("float")) ** 2)
        err /= float(imageA.shape[0] * imageA.shape[1])
        return err

    # í”„ë ˆì„ â†’ feature ì¶”ì¶œ
    def extract_feature(image):
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image).convert("RGB")
        else:
            image = image.convert("RGB")
        tensor = transform(image).unsqueeze(0)
        with torch.no_grad():
            feature = model(tensor).squeeze()
        return feature

    # helper: page -> grayscale resized array (for MSE comparisons / last_slide)
    def page_to_gray_resized(page, size=(320, 240)):
        # accepts PIL.Image or numpy array
        if isinstance(page, np.ndarray):
            arr = page
        else:
            arr = np.array(page.convert("RGB"))
        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)
        gray = cv2.resize(gray, size)
        return gray

    # helper: ensure image object for feature extraction (PIL.Image)
    def ensure_pil(page):
        if isinstance(page, np.ndarray):
            return Image.fromarray(page).convert("RGB")
        else:
            return page.convert("RGB")

    # ì˜ìƒ íŒŒì¼ ë¡œë”©
    print("ğŸï¸ ì˜ìƒ ë¡œë”© ì¤‘...")
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0
    frame_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    print(f"âœ… ì˜ìƒ ë¡œë”© ì™„ë£Œ - FPS: {fps:.2f}, Frame count: {frame_total}")
    frame_interval = max(1, int(fps * 1))

    def is_last_page(page_index, doc_length):
        return page_index == doc_length - 1

    # ìƒíƒœ ë³€ìˆ˜ë“¤
    last_slide = None
    #last_best_page = 0
    prev_sim = None
    frame_count = 0

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    mse_threshold = 500
    sim_drop_threshold = 0.01
    max_search_range = 20

    # ---- ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„: ì´ˆê¸° ì²« ìŠ¬ë¼ì´ë“œ í›„ë³´ ê²°ì • ----
    # ë¹„ë””ì˜¤ ì´ˆë°˜ í”„ë ˆì„ ëª‡ ê°œë¥¼ ì¶”ì¶œí•´ì„œ, ë¬¸ì„œì˜ ì²˜ìŒ up_to_n_pages ì¤‘ì—ì„œ í‰ê·  ìœ ì‚¬ë„ ìµœëŒ€ì¸ í˜ì´ì§€ë¥¼ ì²« ìŠ¬ë¼ì´ë“œë¡œ ì„ íƒ
    def choose_initial_slide(num_pages_to_check=5, sample_frame_count=3, sample_span_seconds=1.0):
        nonlocal last_best_page, last_slide

        if len(images) == 0:
            return

        # ì‹¤ì œë¡œ ì²´í¬í•  í˜ì´ì§€ ìˆ˜
        up_to = min(num_pages_to_check, len(images))
        candidate_indices = list(range(up_to))
        print(f"ğŸ”° ì´ˆê¸° ìŠ¬ë¼ì´ë“œ í›„ë³´ ì¸ë±ìŠ¤: {candidate_indices}")

        # ìƒ˜í”Œ í”„ë ˆì„ ì‹œê°„ ê°„ê²© (seconds)
        span = sample_span_seconds
        # sample_frame_count í”„ë ˆì„ì„ ë½‘ë˜, ì˜ìƒ ì „ì²´ì— ê±¸ì³ ì ì ˆ ê°„ê²©ìœ¼ë¡œ ë½‘ìŒ (ì´ˆë°˜ë¶€ë§Œ ì‚¬ìš©í•˜ë ¤ë©´ start at 0..)
        # ì—¬ê¸°ì„œëŠ” ì˜ìƒ ì‹œì‘ ~ (span * sample_frame_count) ì˜ì—­ ë‚´ì—ì„œ frame_interval ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œ
        sampled_features = []
        sampled_times = []

        # ì•ˆì „: ì˜ìƒ í”„ë ˆì„ì´ ì ìœ¼ë©´ ê°€ëŠ¥í•œ í”„ë ˆì„ë“¤ë§Œ ì‚¬ìš©
        for s_idx in range(sample_frame_count):
            # ì‹œê°„ ìœ„ì¹˜ (s_idx * span)
            t = s_idx * span
            frame_pos = int(min(frame_total - 1, max(0, round(t * fps))))
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_pos)
            ret, f = cap.read()
            if not ret:
                continue
            rgb = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)
            feat = extract_feature(rgb)
            sampled_features.append(feat)
            sampled_times.append(frame_pos / fps)

        if not sampled_features:
            print("âš ï¸ ì´ˆê¸° ìƒ˜í”Œ í”„ë ˆì„ì„ ì½ì–´ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸(0)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            last_best_page = 0
            last_slide = page_to_gray_resized(images[0])
            return

        # ê° í›„ë³´ í˜ì´ì§€ì— ëŒ€í•´ sampled_featuresì™€ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        avg_sims = []
        for idx in candidate_indices:
            page = images[idx]
            pil_page = ensure_pil(page)
            feat_page = extract_feature(pil_page)
            sims = []
            for sf in sampled_features:
                sim = cosine_similarity(sf.unsqueeze(0), feat_page.unsqueeze(0)).item()
                sims.append(sim)
            avg = float(np.mean(sims))
            avg_sims.append((idx, avg))
            print(f"    í›„ë³´ í˜ì´ì§€ {idx} í‰ê·  ìœ ì‚¬ë„: {avg:.4f}")

        # ìµœê³  í‰ê·  ìœ ì‚¬ë„ í˜ì´ì§€ ì„ íƒ
        best_idx, best_avg = max(avg_sims, key=lambda x: x[1])
        last_best_page = best_idx
        last_slide = page_to_gray_resized(images[best_idx])
        print(f"ğŸ ì´ˆê¸° ì„ íƒ ì™„ë£Œ â†’ í˜ì´ì§€ {best_idx} (í‰ê·  ìœ ì‚¬ë„ {best_avg:.4f})")
        # ë¦¬í„´í•´ì„œ ë””ë²„ê·¸ì— ì‚¬ìš© ê°€ëŠ¥
        return best_idx, best_avg, sampled_times

    print("ğŸš€ ë¶„ì„ ì‹œì‘ (ì´ˆê¸° ìŠ¬ë¼ì´ë“œ ì„ íƒ ì¤‘...)")
    # ì„ íƒ ìˆ˜í–‰ (ì—¬ê¸°ì„œ í•„ìš”í•˜ë©´ íŒŒë¼ë¯¸í„°ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŒ)
    last_best_page,trash1,trash2 = choose_initial_slide(num_pages_to_check=5, sample_frame_count=3, sample_span_seconds=0.5)
    real_first_page = last_best_page
    # ì´ˆê¸° ìƒ˜í”Œë§ ë•Œë¬¸ì— ë¹„ë””ì˜¤ í¬ì§€ì…˜ì´ ì›€ì§ì˜€ìœ¼ë¯€ë¡œ ë£¨í”„ ì‹œì‘ ì „ í”„ë ˆì„ í¬ì§€ì…˜ì„ 0ìœ¼ë¡œ ëŒë¦¼
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    frame_count = 0

    # ë©”ì¸ ë£¨í”„
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ğŸ¬ ì˜ìƒ ë")
            break

        frame_time = frame_count / fps

        if frame_count % frame_interval == 0:
            #print(f"\nğŸ§­ í”„ë ˆì„ {frame_count} (ì‹œê°„: {frame_time:.2f}ì´ˆ) ë¶„ì„ ì¤‘...")
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            gray = cv2.resize(gray, (320, 240))

            if last_slide is None:
                print("ğŸ†• (ë£¨í”„ ì¤‘) ì²« ìŠ¬ë¼ì´ë“œë¡œ ì„¤ì •")
                last_slide = gray
            else:
                diff = mse(gray, last_slide)
                #print(f"ğŸ” MSE ì°¨ì´: {diff:.2f}")

                if diff > mse_threshold:
                    print("ğŸ“ˆ ìŠ¬ë¼ì´ë“œ ë³€ê²½ ê°ì§€ë¨ â†’ íƒìƒ‰ ì‹œì‘")
                    frame_sim_results = []

                    for offset in range(-int(2 * fps), int(2 * fps) + 3, frame_interval):
                        pos = frame_count + offset
                        if pos < 0 or pos >= cap.get(cv2.CAP_PROP_FRAME_COUNT):
                            continue

                        cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
                        ret2, temp_frame = cap.read()
                        if not ret2:
                            continue

                        frame_rgb = cv2.cvtColor(temp_frame, cv2.COLOR_BGR2RGB)
                        feat1 = extract_feature(frame_rgb)

                        # ê¸°ë³¸ í›„ë³´ í˜ì´ì§€ ì„¤ì •
                        candidate_range = [-2, -1, 0, 1, 2]
                        candidates = [last_best_page + i for i in candidate_range if 0 <= last_best_page + i < len(images)]

                        best_page = last_best_page
                        max_sim = -1

                        print(f"ğŸ” 1ì°¨ íƒìƒ‰: í›„ë³´ í˜ì´ì§€ {candidates}")
                        for i in candidates:
                            page = images[i]
                            feat2 = extract_feature(ensure_pil(page))
                            sim = cosine_similarity(feat1.unsqueeze(0), feat2.unsqueeze(0)).item()

                            print(f"    í˜ì´ì§€ {i} ìœ ì‚¬ë„: {sim:.4f}")
                            if sim > max_sim:
                                max_sim = sim
                                best_page = i
                                best_feat2 = feat2

                        # ìœ ì‚¬ë„ ê¸‰ë½ ì‹œ ì¬íƒìƒ‰
                        if prev_sim is not None and (prev_sim - max_sim) >= sim_drop_threshold:
                            print(f"âš ï¸ ìœ ì‚¬ë„ í•˜ë½ ê°ì§€ (ì´ì „: {prev_sim:.4f} â†’ í˜„ì¬: {max_sim:.4f}) â†’ 2ì°¨ íƒìƒ‰")
                            expanded_range = list(range(-(max_search_range//2), max_search_range//2 + 1))
                            expanded_candidates = [last_best_page + i for i in expanded_range if 0 <= last_best_page + i < len(images)]

                            print(f"ğŸ” 2ì°¨ íƒìƒ‰: í›„ë³´ í˜ì´ì§€ {expanded_candidates}")
                            for i in expanded_candidates:
                                page = images[i]
                                feat2 = extract_feature(ensure_pil(page))
                                sim = cosine_similarity(feat1.unsqueeze(0), feat2.unsqueeze(0)).item()

                                print(f"    [2ì°¨] í˜ì´ì§€ {i} ìœ ì‚¬ë„: {sim:.4f}")
                                if sim > max_sim:
                                    max_sim = sim
                                    best_page = i
                                    best_feat2 = feat2

                        result_time = pos / fps
                        minutes = int(result_time // 60)
                        seconds = int(result_time % 60)
                        print(f"âœ… [ê²°ê³¼] {minutes}ë¶„ {seconds}ì´ˆ: í˜ì´ì§€ {best_page} / ìœ ì‚¬ë„ {max_sim:.4f}")
                        csv_writer.writerow([round(result_time, 2), best_page, round(max_sim, 4)])
                        frame_sim_results.append((max_sim, best_page, result_time, gray))

                    if frame_sim_results:
                        best_result = max(frame_sim_results, key=lambda x: x[0])
                        prev_sim = best_result[0]
                        last_best_page = best_result[1]
                        last_slide = best_result[3]
                        print(f"ğŸ“Œ í˜„ì¬ ìƒíƒœ ê°±ì‹  â†’ í˜ì´ì§€ {last_best_page}, ìœ ì‚¬ë„ {prev_sim:.4f}")
                        
                        if is_last_page(last_best_page, len(images)):
                            print(f"ğŸš© ë§ˆì§€ë§‰ í˜ì´ì§€({last_best_page}) ë„ë‹¬, ì¢…ë£Œí•©ë‹ˆë‹¤.")
                            break

        frame_count += 1

    cap.release()
    csv_file.close()
    del model
    torch.cuda.empty_cache()
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ. ë¡œê·¸ ì €ì¥ë¨.")
    return real_first_page